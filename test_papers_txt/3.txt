Adversarial Examples are not Bugs, they are Features

Andrew Ilyas
MIT
ailyas@mit.eduShibani Santurkar
MIT
shibani@mit.eduDimitris Tsipras
MIT
tsipras@mit.edu
Logan Engstrom
MIT
engstrom@mit.eduBrandon Tran
MIT
btran115@mit.eduAleksander M Ë› adry
MIT
madry@mit.edu
Abstract
Adversarial examples have attracted signiï¬cant attention in machine learning, but
the reasons for their existence and pervasiveness remain unclear. We demonstrate
that adversarial examples can be directly attributed to the presence of non-robust
features : features (derived from patterns in the data distribution) that are highly
predictive, yet brittle and (thus) incomprehensible to humans. After capturing
these features within a theoretical framework, we establish their widespread ex-
istence in standard datasets. Finally, we present a simple setting where we can
rigorously tie the phenomena we observe in practice to a misalignment between
the (human-speciï¬ed) notion of robustness and the inherent geometry of the data.
1 Introduction
The pervasive brittleness of deep neural networks [Sze+14; Eng+19b; HD19; Ath+18] has attracted
signiï¬cant attention in recent years. Particularly worrisome is the phenomenon of adversarial ex-
amples [Big+13; Sze+14], imperceptibly perturbed natural inputs that induce erroneous predictions
in state-of-the-art classiï¬ers. Previous work has proposed a variety of explanations for this phe-
nomenon, ranging from theoretical models [Sch+18; BPR18] to arguments based on concentration
of measure in high-dimensions [Gil+18; MDM18; Sha+19a]. These theories, however, are often
unable to fully capture behaviors we observe in practice (we discuss this further in Section 5).
More broadly, previous work in the ï¬eld tends to view adversarial examples as aberrations arising
either from the high dimensional nature of the input space or statistical ï¬‚uctuations in the training
data [GSS15; Gil+18]. From this point of view, it is natural to treat adversarial robustness as a goal
that can be disentangled and pursued independently from maximizing accuracy [Mad+18; SHS19;
Sug+19], either through improved standard regularization methods [TG16] or pre/post-processing
of network inputs/outputs [Ues+18; CW17a; He+17].
In this work, we propose a new perspective on the phenomenon of adversarial examples. In con-
trast to the previous models, we cast adversarial vulnerability as a fundamental consequence of the
dominant supervised learning paradigm. Speciï¬cally, we claim that:
Adversarial vulnerability is a direct result of sensitivity to well-generalizing features in the data.
Recall that we usually train classiï¬ers to solely maximize (distributional) accuracy. Consequently,
classiï¬ers tend to use anyavailable signal to do so, even those that look incomprehensible to hu-
mans. After all, the presence of â€œa tailâ€ or â€œearsâ€ is no more natural to a classiï¬er than any other
equally predictive feature. In fact, we ï¬nd that standard ML datasets doadmit highly predictive
33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.yet imperceptible features. We posit that our models learn to rely on these â€œnon-robustâ€ features,
leading to adversarial perturbations that exploit this dependence.2
Our hypothesis also suggests an explanation for adversarial transferability : the phenomenon that
perturbations computed for one model often transfer to other, independently trained models. Since
any two models are likely to learn similar non-robust features, perturbations that manipulate such
features will apply to both. Finally, this perspective establishes adversarial vulnerability as a human-
centric phenomenon, since, from the standard supervised learning point of view, non-robust features
can be as important as robust ones. It also suggests that approaches aiming to enhance the inter-
pretability of a given model by enforcing â€œpriorsâ€ for its explanation [MV15; OMS17; Smi+17]
actually hide features that are â€œmeaningfulâ€ andpredictive to standard models. As such, produc-
inghuman -meaningful explanations that remain faithful to underlying models cannot be pursued
independently from the training of the models themselves.
To corroborate our theory, we show that it is possible to disentangle robust from non-robust features
in standard image classiï¬cation datasets. Speciï¬cally, given a training dataset, we construct:
1.A â€œrobustiï¬edâ€ version for robust classiï¬cation (Figure 1a)3.We are able to effectively
remove non-robust features from a dataset. Concretely, we create a training set (semanti-
cally similar to the original) on which standard training yields good robust accuracy on the
original, unmodiï¬ed test set. This ï¬nding establishes that adversarial vulnerability is not
necessarily tied to the standard training framework, but is also a property of the dataset.
2.A â€œnon-robustâ€ version for standard classiï¬cation (Figure 1b)2.We are also able to
construct a training dataset for which the inputs are nearly identical to the originals, but
all appear incorrectly labeled. In fact, the inputs in the new training set are associated
to their labels only through small adversarial perturbations (and hence utilize only non-
robust features). Despite the lack of any predictive human-visible information, training on
this dataset yields good accuracy on the original, unmodiï¬ed test set. This demonstrates
that adversarial perturbations can arise from ï¬‚ipping features in the data that are useful for
classiï¬cation of correct inputs (hence not being purely aberrations).
Finally, we present a concrete classiï¬cation task where the connection between adversarial examples
and non-robust features can be studied rigorously. This task consists of separating Gaussian distri-
butions, and is loosely based on the model presented in Tsipras et al. [Tsi+19], while expanding
upon it in a few ways. First, adversarial vulnerability in our setting can be precisely quantiï¬ed as a
difference between the intrinsic data geometry and that of the adversaryâ€™s perturbation set. Second,
robust training yields a classiï¬er which utilizes a geometry corresponding to a combination of these
two. Lastly, the gradients of standard models can be signiï¬cantly misaligned with the inter-class
direction, capturing a phenomenon that has been observed in practice [Tsi+19].
2 The Robust Features Model
We begin by developing a framework, loosely based on the setting of Tsipras et al. [Tsi+19], that
enables us to rigorously refer to â€œrobustâ€ and â€œnon-robustâ€ features. In particular, we present a set of
deï¬nitions which allow us to formally describe our setup, theoretical results, and empirical evidence.
Setup. We study binary classiï¬cation, where input-label pairs (x;y)2Xf 1gare sampled
from a distribution D; the goal is to learn a classiï¬er C:X!f 1gpredictingygivenx.
We deï¬ne a feature to be a function mapping from the input space Xto real numbers, with the
set of all features thus being F=ff:X !Rg. For convenience, we assume that the features
inFare shifted/scaled to be mean-zero and unit-variance (i.e., so that E(x;y)D[f(x)] = 0 and
E(x;y)D[f(x)2] = 1 ), making the following deï¬nitions scale-invariant. Note that this deï¬nition
captures what we abstractly think of as features (e.g., a function capturing how â€œfurryâ€ an image is).
2It is worth emphasizing that while our ï¬ndings demonstrate that adversarial vulnerability does arise from
non-robust features, they do not preclude the possibility of adversarial vulnerability also arising from other
phenomena [Nak19a]. Nevertheless, the mere existence of useful non-robust features sufï¬ces to establish that
without explicitly preventing models from utilizing these features, adversarial vulnerability will persist.
3The corresponding datasets for CIFAR-10 are publicly available at http://git.io/adv-datasets .
2Robust dataset
Traingood standard accuracy good robust accuracygood standard accuracy bad robust accuracyUnmodiï¬ed test setTraining imagefrog
frogfrogNon-robust datasetTrain(a)
Evaluate on original test set
Training imageRobust Features: dog Non-Robust Features: dogdog
Relabel as catRobust Features: dog Non-Robust Features: catcat
catmax P(cat) Adversarial example towards â€œcatâ€ 
Traingood accuracy (b)
Figure 1: A conceptual diagram of the experiments of Section 3: (a) we disentangle features into
robust and non-robust (Section 3.1), (b) we construct a dataset which appears mislabeled to humans
(via adversarial examples) but results in good accuracy on the original test set (Section 3.2).
Useful, robust, and non-robust features. We now deï¬ne the key concepts required for formulat-
ing our framework. To this end, we categorize features in the following manner:
-useful features : For a given distribution D, we call a feature f -useful ( >0) if it is
correlated with the true label in expectation, that is if
E(x;y)D[yf(x)]: (1)
We then deï¬ne D(f)as the largest for which feature fis-useful under distribution D.
(Note that if a feature fis negatively correlated with the label, then  fis useful instead.)
Crucially, a linear classiï¬er trained on -useful features can attain non-trivial performance.

-robustly useful features : Suppose we have a -useful feature f(D(f)>0). We
refer tofas a robust feature (formally a 
-robustly useful feature for 
 > 0) if, under
adversarial perturbation (for some speciï¬ed set of valid perturbations ),fremains
-
useful. Formally, if we have that
E(x;y)D
inf
2(x)yf(x+)

: (2)
Useful, non-robust features : Auseful, non-robust feature is a feature which is -useful for
somebounded away from zero, but is not a 
-robust feature for any 
0. These features
help with classiï¬cation in the standard setting, but may hinder accuracy in the adversarial
setting, as the correlation with the label can be ï¬‚ipped.
Classiï¬cation. In our framework, a classiï¬er C= (F;w;b )is comprised of a set of features
FF, a weight vector w, and a scalar bias b. For an input x, the classiï¬er predicts the label yas
C(x) =sgn0
@b+X
f2Fwff(x)1
A:
For convenience, we denote the set of features learned by a classiï¬er CasFC.
Standard Training. Training a classiï¬er is performed by minimizing a loss function L(x;y)
over input-label pairs (x;y)from the training set (via empirical risk minimization (ERM)) that de-
creases with the correlation between the weighted combination of the features and the label. When
minimizing classiï¬cation loss, no distinction exists between robust and non-robust features: the
only distinguishing factor of a feature is its -usefulness. Furthermore, the classiï¬er will utilize any
-useful feature in Fto decrease the loss of the classiï¬er.
Robust training. In the presence of an adversary , any useful but non-robust features can be made
anti-correlated with the true label, leading to adversarial vulnerability. Therefore, ERM is no longer
sufï¬cient to train classiï¬ers that are robust, and we need to explicitly account for the effect of the
3adversary on the classiï¬er. To do so, we use an adversarial loss function that can discern between
robust and non-robust features [Mad+18]:
E(x;y)D
max
2(x)L(x+;y)
; (3)
for an appropriately deï¬ned set of perturbations . Since the adversary can exploit non-robust
features to degrade classiï¬cation accuracy, minimizing this adversarial loss [GSS15; Mad+18] can
be viewed as explicitly preventing the classiï¬er from relying on non-robust features.
Remark. We want to note that even though this framework enables us to describe and predict
the outcome of our experiments, it does not capture the notion of non-robust features exactly as
we intuitively might think of them. For instance, in principle, our theoretical framework would
allow for useful non-robust features to arise as combinations of useful robust features and useless
non-robust features [Goh19b]. These types of constructions, however, are actually precluded by our
experimental results (for instance, the classiï¬ers trained in Section 3 would not generalize). This
shows that our experimental ï¬ndings capture a stronger, more ï¬ne-grained statement than our formal
deï¬nitions are able to express. We view bridging this gap as an interesting direction for future work.
3 Finding Robust (and Non-Robust) Features
The central premise of our proposed framework is that there exist both robust and non-robust features
that constitute useful signals for standard classiï¬cation. We now provide evidence in support of this
hypothesis by disentangling these two sets of features (see conceptual description in Figure 1).
On one hand, we will construct a â€œrobustiï¬edâ€ dataset, consisting of samples that primarily contain
robust features. Using such a dataset, we are able to train robust classiï¬ers (with respect to the
standard test set) using standard (i.e., non-robust) training. This demonstrates that robustness can
arise by removing certain features from the dataset (as, overall, the new dataset contains less infor-
mation about the original training set). Moreover, it provides evidence that adversarial vulnerability
is caused by non-robust features and is not inherently tied to the standard training framework.
On the other hand, we will construct datasets where the input-label association is based purely on
non-robust features (and thus the resulting dataset appears completely mislabeled to humans). We
show that this dataset sufï¬ces to train a classiï¬er with good performance on the standard test set. This
indicates that natural models use non-robust features to make predictions, even in the presence of
robust features. These features alone are sufï¬cient for non-trivial generalization to natural images ,
indicating that they are indeed predictive, rather than artifacts of ï¬nite-sample overï¬tting.
3.1 Disentangling robust and non-robust features
Recall that the features a classiï¬er learns to rely on are based purely on how useful these features
are for (standard) generalization. Thus, under our conceptual framework, if we can ensure that only
robust features are useful, standard training should result in a robust classiï¬er. Unfortunately, we
cannot directly manipulate the features of very complex, high-dimensional datasets. Instead, we will
leverage a robust model and modify our dataset to contain only the features relevant to that model.
Conceptually, given a robust (i.e., adversarially trained [Mad+18]) model C, we aim to construct a
distributionbDRfor which features used by Care as useful as they were on the original distribution
Dwhile ensuring that the rest of the features are not useful. In terms of our formal framework:
E(x;y)bDR[f(x)y] =E(x;y)D[f(x)y]iff2FC
0 otherwise;(4)
whereFCagain represents the set of features utilized by C.
We will construct a training set for bDRvia a one-to-one mapping x7!xrfrom the original training
set forD. In the case of a deep neural network, FCcorresponds to exactly the set of activations in the
penultimate layer (since these correspond to inputs to a linear classiï¬er). To ensure that features used
by the model are equally useful under both training sets, we (approximately) enforce all features in
FCto have similar values for both xandxrthrough the following optimization:
min
xrkg(xr) g(x)k2; (5)
4â€œairplaneâ€™â€™â€œshipâ€™â€™â€œdogâ€™â€™â€œfrogâ€™â€™â€œtruckâ€™â€™D/hatwideDNR/hatwideDR(a)
Std Training 
 using 
Adv Training 
 using 
Std Training 
 using R
Std Training 
 using NR
020406080100Test Accuracy on  (%)
Std accuracy Adv accuracy (=0.25)
 (b)
Figure 2: (a): Random samples from our variants of the CIFAR-10 [Kri09] training set: the original
training set; the robust training set bDR, restricted to features used by a robust model; and the non-
robust training set bDNR, restricted to features relevant to a standard model (labels appear incorrect
to humans). (b): Standard and robust accuracy on the CIFAR-10 test set ( D) for models trained
with: (i) standard training (on D) ; (ii) standard training on bDNR; (iii) adversarial training (on
D); and (iv) standard training on bDR. Models trained on bDRandbDNRreï¬‚ect the original models
used to create them: notably, standard training on bDRyields nontrivial robust accuracy. Results for
Restricted-ImageNet [Tsi+19] are in D.8 Figure 12.
wherexis the original input and gis the mapping from xto the representation layer. We optimize
this objective using (normalized) gradient descent (see details in Appendix C).
Since we donâ€™t have access to features outside FC, there is no way to ensure that the expectation
in (4) is zero for all f62FC. To approximate this condition, we choose the starting point of gradient
descent for the optimization in (5) to be an input x0which is drawn from Dindependently of the
label ofx(we also explore sampling x0from noise in Appendix D.1). This choice ensures that
any feature present in that input will not be useful since they are not correlated with the label in
expectation over x0. The underlying assumption here is that, when performing the optimization
in (5), features that are not being directly optimized (i.e., features outside FC) are not affected. We
provide pseudocode for the construction in Figure 5 (Appendix C).
Given the new training set for bDR(a few random samples are visualized in Figure 2a), we train a
classiï¬er using standard (non-robust) training. We then test this classiï¬er on the original test set (i.e.
D). The results (Figure 2b) indicate that the classiï¬er learned using the new dataset attains good
accuracy in both standard and adversarial settings (see additional evaluation in Appendix D.2.)4.
As a control, we repeat this methodology using a standard (non-robust) model for Cin our con-
struction of the dataset. Sample images from the resulting â€œnon-robust datasetâ€ bDNRare shown
in Figure 2aâ€”they tend to resemble more the source image of the optimization x0than the target
imagex. We ï¬nd that training on this dataset leads to good standard accuracy, yet yields almost
no robustness (Figure 2b). We also verify that this procedure is not simply a matter of encoding
the weights of the original modelâ€”we get the same results for both bDRandbDNRif we train with
different architectures than that of the original models.
Overall, our ï¬ndings corroborate the hypothesis that adversarial examples can arise from (non-
robust) features of the data itself. By ï¬ltering out non-robust features from the dataset (e.g. by
restricting the set of available features to those used by a robust model), one can train a signiï¬cantly
more robust model using standard training .
3.2 Non-robust features sufï¬ce for standard classiï¬cation
The results of the previous section show that by restricting the dataset to only contain features that
are used by a robust model, standard training results in classiï¬ers that are signiï¬cantly more robust.
4In an attempt to explain the gap in accuracy between the model trained on bDRand the original robust
classiï¬er C, we test distributional shift, by reporting results on the â€œrobustiï¬edâ€ test set in Appendix D.3.
5This suggests that when training on the standard dataset, non-robust features take on a large role in
the resulting learned classiï¬er. Here we will show that this is not merely incidental. In particular, we
demonstrate that non-robust features alone sufï¬ce for standard generalizationâ€” i.e., a model trained
solely on non-robust features can generalize to the standard test set.
To show this, we construct a dataset where the only features that are useful for classiï¬cation are
non-robust features (or in terms of our formal model from Section 2, all features fthat are-useful
are non-robust). To accomplish this, we modify each input-label pair (x;y)as follows. We select a
target classteither (a) uniformly at random (hence features become uncorrelated with the labels) or
(b) deterministically according to the source class (e.g. permuting the labels). Then, we add a small
adversarial perturbation to xto cause it to be classiï¬ed as tby a standard model:
xadv= arg min
kx0 xk"LC(x0;t); (6)
whereLCis the loss under a standard (non-robust) classiï¬er Cand"is a small constant. The result-
ing inputs are indistinguishable from the originals (Appendix D Figure 9)â€”to a human observer,
it thus appears that the label tassigned to the modiï¬ed input is simply incorrect. The resulting
input-label pairs (xadv;t)make up the new training set (pseudocode in Appendix C Figure 6).
Now, sincekxadv xkis small, by deï¬nition the robust features of xadvare still correlated with
classy(and nott) in expectation over the dataset. After all, humans still recognize the original class.
On the other hand, since every xadvis strongly classiï¬ed as tby a standard classiï¬er, it must be that
some of the non-robust features are now strongly correlated with t(in expectation).
In the case where tis chosen at random, the robust features are originally uncorrelated with the label
t(in expectation), and after the perturbation can be only slightly correlated (hence being signiï¬cantly
less useful for classiï¬cation than before)5. Formally, we aim to construct a dataset bDrand where
E(x;y)bDrand[yf(x)]>0iffnon-robustly useful under D;
'0otherwise.(7)
In contrast, when tis chosen deterministically based on y, the robust features actually point away
from the assigned label t. In particular, all of the inputs labeled with class texhibit non-robust
features correlated with t, but robust features correlated with the original class y. Thus, robust
features on the original training set provide signiï¬cant predictive power on the training set, but will
actually hurt generalization on the standard test set. Formally, our goal is to construct bDdetsuch that
E(x;y)bDdet[yf(x)]8
<
:>0iffnon-robustly useful under D;
<0iffrobustly useful under D
2Rotherwise (fnot useful underD):(8)
We ï¬nd that standard training on these datasets actually generalizes to the original test set, as shown
in Table 1). This indicates that non-robust features are indeed useful for classiï¬cation in the standard
setting. Remarkably, even training on bDdet(where all the robust features are correlated with the
wrong class), results in a well-generalizing classiï¬er. This indicates that non-robust features can be
picked up by models during standard training, even in the presence of predictive robust features6
3.3 Transferability can arise from non-robust features
One of the most intriguing properties of adversarial examples is that they transfer across models with
different architectures and independently sampled training sets [Sze+14; PMG16; CRP19]. Here, we
show that this phenomenon can in fact be viewed as a natural consequence of the existence of non-
robust features. Recall that, according to our main thesis, adversarial examples can arise as a result
of perturbing well-generalizing, yet brittle features. Given that such features are inherent to the data
distribution, different classiï¬ers trained on independent samples from that distribution are likely
to utilize similar non-robust features. Consequently, perturbations constructed by exploiting non-
robust features learned by one classiï¬er will transfer to other classiï¬ers utilizing similar features.
5Goh [Goh19a] provides an approach to quantifying this â€œrobust feature leakageâ€ and ï¬nds that one can
obtain a (small) amount of test accuracy by leveraging robust feature leakage on bDrand.
6Additional results and analysis are in App. D.5, D.6, and D.7.
6In order to illustrate and corroborate this hypothesis, we train ï¬ve different architectures on the
dataset generated in Section 3.2 (adversarial examples with deterministic labels) for a standard
ResNet-50 [He+16]. Our hypothesis would suggest that architectures which learn better from this
training set (in terms of performance on the standard test set) are more likely to learn similar non-
robust features to the original classiï¬er. Indeed, we ï¬nd that the test accuracy of each architecture is
predictive of how often adversarial examples transfer from the original model to standard classiï¬ers
with that architecture (Figure 3). In a similar vein, Nakkiran [Nak19a] constructs a set of adversarial
perturbations that is explicitly non-transferable and ï¬nds that these perturbations cannot be used to
learn a good classiï¬er. These ï¬ndings thus corroborate our hypothesis that adversarial transferability
arises when models learn similar brittle features of the underlying dataset.
25 30 35 40 45 50
Test accuracy (%; trained on Dy+1)60708090100Transfer success rate (%)
VGG-16Inception-v3ResNet-18 DenseNetResNet-50
Figure 3: Transfer rate of adversarial exam-
ples from a ResNet-50 to different architec-
tures alongside test set performance of these
architecture when trained on the dataset gen-
erated in Section 3.2. Architectures more
susceptible to transfer attacks also performed
better on the standard test set supporting
our hypothesis that adversarial transferability
arises from using similar non-robust features .Source DatasetDataset
CIFAR-10 ImageNet R
D 95.3% 96.6%
bDrand 63.3% 87.9%
bDdet 43.7% 64.4%
Table 1: Test accuracy (on D) of classiï¬ers
trained on theD,bDrand, andbDdettrain-
ing sets created using a standard (non-robust)
model. For both bDrand andbDdet, only non-
robust features correspond to useful features
on both the train set and D. These datasets
are constructed using adversarial perturba-
tions ofxtowards a class t(random for
bDrand and deterministic for bDdet); the result-
ing images are relabeled as t.
4 A Theoretical Framework for Studying (Non)-Robust Features
The experiments from the previous section demonstrate that the conceptual framework of robust and
non-robust features is strongly predictive of the empirical behavior of state-of-the-art models on real-
world datasets. In order to further strengthen our understanding of the phenomenon, we instantiate
the framework in a concrete setting that allows us to theoretically study various properties of the
corresponding model. Our model is similar to that of Tsipras et al. [Tsi+19] in the sense that it
contains a dichotomy between robust and non-robust features, extending upon it in a few ways: a)
the adversarial vulnerability can be explicitly expressed as a difference between the inherent data
metric and the `2metric, b) robust learning corresponds exactly to learning a combination of these
two metrics, c) the gradients of robust models align better with the adversaryâ€™s metric.
Setup. We study a simple problem of maximum likelihood classiï¬cation between two Gaussian
distributions. In particular, given samples (x;y)sampled fromDaccording to
yu.a.r. f  1;+1g; xN(y;); (9)
our goal is to learn parameters  = (;)such that
 = arg min
;E(x;y)D[`(x;y;)]; (10)
where`(x;;)represents the Gaussian negative log-likelihood (NLL) function. Intuitively, we
ï¬nd the parameters ;which maximize the likelihood of the sampled data under the given model.
Classiï¬cation can be accomplished via likelihood test: given an unlabeled sample x, we predict yas
y= arg max
y`(x;y;) =sign 
x> 1
:
7In turn, the robust analogue of this problem arises from replacing `(x;y;)with the NLL under
adversarial perturbation. The resulting robust parameters rcan be written as
r= arg min
;E(x;y)D
max
kk2"`(x+;y;)
; (11)
A detailed analysis appears in Appendix Eâ€”here we present a high-level overview of the results.
(1) Vulnerability from metric misalignment (non-robust features). Note that in this model, one
can rigorously refer to an inner product (and thus a metric) induced by the features. In particular,
one can view the learned parameters of a Gaussian  = (;)as deï¬ning an inner product over the
input space given by hx;yi= (x )> 1(y ). This in turn induces the Mahalanobis distance,
which represents how a change in the input affects the features of the classiï¬er. This metric is not
necessarily aligned with the metric in which the adversary is constrained, the `2-norm. Actually, we
show that adversarial vulnerability arises exactly as a misalignment of these two metrics.
Theorem 1 (Adversarial vulnerability from misalignment) .Consider an adversary whose pertur-
bation is determined by the â€œLagrangian penaltyâ€ form of (11), i.e.
max
`(x+;y;) Ckk2;
whereC1
min()is a constant trading off NLL minimization and the adversarial constraint (the
bound onCensures the problem is concave). Then, the adversarial loss Ladvincurred by (;)is
Ladv() L() = tr
I+ (C I) 12
 d;
and, for a ï¬xed tr () =kthe above is minimized by =k
dI.
In fact, note that such a misalignment corresponds precisely to the existence of non-robust features â€”
â€œsmallâ€ changes in the adversaryâ€™s metric along certain directions can cause large changes under the
notion of distance established by the parameters (illustrated in Figure 4).
(2) Robust Learning. The (non-robust) maximum likelihood estimate is  = , and thus the
vulnerability for the standard MLE depends entirely on the data distribution. The following theorem
characterizes the behaviour of the learned parameters in the robust problem (we study a slight relax-
ation of (11) that becomes exact exponentially fast as d!1 , see Appendix E.3.3). In fact, we can
prove (Section E.3.4) that performing (sub)gradient descent on the inner maximization (known as
adversarial training [GSS15; Mad+18]) yields exactly r. We ï¬nd that as the perturbation budget "
increases, the metric induced by the classiï¬er mixes`2and the metric induced by the data features.
Theorem 2 (Robustly Learned Parameters) .Just as in the non-robust case, r=, i.e. the true
mean is learned. For the robust covariance r, there exists an "0>0, such that for any "2[0;"0),
r=1
2+1
I+r
1
+1
42; where 
1 +"1=2
"1=2+"3=2
O1 +"1=2
"1=2
:
The effect of robust optimization under an `2-constrained adversary is visualized in Figure 4. As 
grows, the learned covariance becomes more aligned with identity. For instance, we can see that the
classiï¬er learns to be less sensitive in certain directions, despite their usefulness for classiï¬cation.
(3) Gradient Interpretability. Tsipras et al. [Tsi+19] observe that gradients of robust models
tend to look more semantically meaningful. It turns out that under our model, this behaviour arises
as a natural consequence of Theorem 2. In particular, we show that the resulting robustly learned
parameters cause the gradient of the linear classiï¬er and the vector connecting the means of the two
distributions to better align (in a worst-case sense) under the `2inner product.
Theorem 3 (Gradient alignment) .Letf(x)andfr(x)be monotonic classiï¬ers based on the linear
separator induced by standard and `2-robust maximum likelihood classiï¬cation, respectively. The
maximum angle formed between the gradient of the classiï¬er (wrt input) and the vector connecting
the classes can be smaller for the robust model:
min
h;rxfr(x)i
kkkrxfr(x)k>min
h;rxf(x)i
kkkrxf(x)k:
Figure 4 illustrates this phenomenon in the two-dimensional case, where `2-robustness causes the
gradient direction to become increasingly aligned with the vector between the means ( ).
820
 15
 10
 5
 0 5 10 15 20
Feature x110.0
7.5
5.0
2.5
0.02.55.07.510.0Feature x2Maximum likelihood estimate
2 unit ball
1-induced metric unit ball
Samples from (0,)
20
 15
 10
 5
 0 5 10 15 20
Feature x110.0
7.5
5.0
2.5
0.02.55.07.510.0Feature x2True Parameters (=0)
Samples from (,)
Samples from (,)
20
 15
 10
 5
 0 5 10 15 20
Feature x110.0
7.5
5.0
2.5
0.02.55.07.510.0Feature x2Robust parameters, =1.0
20
 15
 10
 5
 0 5 10 15 20
Feature x110.0
7.5
5.0
2.5
0.02.55.07.510.0Feature x2Robust parameters, =10.0
Figure 4: An empirical demonstration of the effect illustrated by Theorem 2â€”as the adversarial
perturbation budget "is increased, the learned mean remains constant, but the learned covariance
â€œblendsâ€ with the identity matrix, effectively adding uncertainty onto the non-robust feature.
Discussion. Our analysis suggests that rather than offering quantitative classiï¬cation beneï¬ts, a
natural way to view the role of robust optimization is as enforcing a prior over the features learned
by the classiï¬er. In particular, training with an `2-bounded adversary prevents the classiï¬er from
relying heavily on features which induce a metric dissimilar to the `2metric. The strength of the
adversary then allows for a trade-off between the enforced prior, and the data-dependent features.
Robustness and accuracy. Note that in the setting described so far, robustness canbe at odds
with accuracy since robust training prevents us from learning the most accurate classiï¬er (a similar
conclusion is drawn in [Tsi+19]). However, we note that there are very similar settings where non-
robust features manifest themselves in the same way, yet a classiï¬er with perfect robustness and
accuracy is still attainable. Concretely, consider the distributions pictured in Figure 14 in Appendix
D.10. It is straightforward to show that while there are many perfectly accurate classiï¬ers, any
standard loss function will learn an accurate yet non-robust classiï¬er. Only when robust training is
employed does the classiï¬er learn a perfectly accurate and perfectly robust decision boundary.
5 Related Work
Several models for explaining adversarial examples have been proposed in prior work, utilizing
ideas ranging from ï¬nite-sample overï¬tting to high-dimensional statistical phenomena [Gil+18;
FFF18; For+19; TG16; Sha+19a; MDM18; Sha+19b; GSS15; BPR18]. The key differentiating
aspect of our model is that adversarial perturbations arise as well-generalizing, yet brittle, features ,
rather than statistical anomalies. In particular, adversarial vulnerability does not stem from using
a speciï¬c model class or a speciï¬c training method, since standard training on the â€œrobustiï¬edâ€
data distribution of Section 3.1 leads to robust models. At the same time, as shown in Section 3.2,
these non-robust features are sufï¬cient to learn a good standard classiï¬er. We discuss the connection
between our model and others in detail in Appendix A and additional related work in Appendix B.
6 Conclusion
In this work, we cast the phenomenon of adversarial examples as a natural consequence of the
presence of highly predictive but non-robust features in standard ML datasets. We provide support
for this hypothesis by explicitly disentangling robust and non-robust features in standard datasets,
as well as showing that non-robust features alone are sufï¬cient for good generalization. Finally,
we study these phenomena in more detail in a theoretical setting where we can rigorously study
adversarial vulnerability, robust training, and gradient alignment.
Our ï¬ndings prompt us to view adversarial examples as a fundamentally human phenomenon. In
particular, we should not be surprised that classiï¬ers exploit highly predictive features that happen
to be non-robust under a human-selected notion of similarity, given such features exist in real-world
datasets. In the same manner, from the perspective of interpretability, as long as models rely on these
non-robust features, we cannot expect to have model explanations that are both human-meaningful
and faithful to the models themselves. Overall, attaining models that are robust and interpretable
will require explicitly encoding human priors into the training process.
9Acknowledgements
We thank Preetum Nakkiran for suggesting the experiment of Appendix D.9 (i.e. replicating Figure 3
with targeted attacks). We also are grateful to the authors of Engstrom et al. [Eng+19a] (Chris Olah,
Dan Hendrycks, Justin Gilmer, Reiichiro Nakano, Preetum Nakkiran, Gabriel Goh, Eric Wallace)â€”
for their insights and efforts replicating, extending, and discussing our experimental results.
Work supported in part by the NSF grants CCF-1553428, CCF-1563880, CNS-1413920, CNS-
1815221, IIS-1447786, IIS-1607189, the Microsoft Corporation, the Intel Corporation, the MIT-
IBM Watson AI Lab research grant, and an Analog Devices Fellowship.
References
[ACW18] Anish Athalye, Nicholas Carlini, and David A. Wagner. â€œObfuscated Gradients Give a
False Sense of Security: Circumventing Defenses to Adversarial Examplesâ€. In: Inter-
national Conference on Machine Learning (ICML) . 2018.
[Ath+18] Anish Athalye et al. â€œSynthesizing Robust Adversarial Examplesâ€. In: International
Conference on Machine Learning (ICML) . 2018.
[BCN06] Cristian Bucilu Ë‡a, Rich Caruana, and Alexandru Niculescu-Mizil. â€œModel compres-
sionâ€. In: International Conference on Knowledge Discovery and Data Mining (KDD) .
2006.
[Big+13] Battista Biggio et al. â€œEvasion attacks against machine learning at test timeâ€. In:
Joint European conference on machine learning and knowledge discovery in databases
(ECML-KDD) . 2013.
[BPR18] SÃ©bastien Bubeck, Eric Price, and Ilya Razenshteyn. â€œAdversarial examples from com-
putational constraintsâ€. In: arXiv preprint arXiv:1805.10204 . 2018.
[Car+19] Nicholas Carlini et al. â€œOn Evaluating Adversarial Robustnessâ€. In: ArXiv preprint
arXiv:1902.06705 . 2019.
[CRK19] Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. â€œCertiï¬ed adversarial robustness
via randomized smoothingâ€. In: arXiv preprint arXiv:1902.02918 . 2019.
[CRP19] Zachary Charles, Harrison Rosenberg, and Dimitris Papailiopoulos. â€œA Geometric Per-
spective on the Transferability of Adversarial Directionsâ€. In: International Conference
on Artiï¬cial Intelligence and Statistics (AISTATS) . 2019.
[CW17a] Nicholas Carlini and David Wagner. â€œAdversarial Examples Are Not Easily Detected:
Bypassing Ten Detection Methodsâ€. In: Workshop on Artiï¬cial Intelligence and Secu-
rity (AISec) . 2017.
[CW17b] Nicholas Carlini and David Wagner. â€œTowards evaluating the robustness of neural net-
worksâ€. In: Symposium on Security and Privacy (SP) . 2017.
[Dan67] John M. Danskin. The Theory of Max-Min and its Application to Weapons Allocation
Problems . 1967.
[Das+19] Constantinos Daskalakis et al. â€œEfï¬cient Statistics, in High Dimensions, from Trun-
cated Samplesâ€. In: Foundations of Computer Science (FOCS) . 2019.
[Din+19] Gavin Weiguang Ding et al. â€œOn the Sensitivity of Adversarial Robustness to Input
Data Distributionsâ€. In: International Conference on Learning Representations . 2019.
[Eng+19a] Logan Engstrom et al. â€œA Discussion of â€™Adversarial Examples Are Not Bugs, They
Are Featuresâ€™â€. In: Distill (2019). https://distill.pub/2019/advex-bugs-discussion. DOI:
10.23915/distill.00019 .
[Eng+19b] Logan Engstrom et al. â€œA Rotation and a Translation Sufï¬ce: Fooling CNNs with
Simple Transformationsâ€. In: International Conference on Machine Learning (ICML) .
2019.
[FFF18] Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. â€œAdversarial vulnerability for any
classiï¬erâ€. In: Advances in Neural Information Processing Systems (NeuRIPS) . 2018.
[FMF16] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. â€œRobustness
of classiï¬ers: from adversarial to random noiseâ€. In: Advances in Neural Information
Processing Systems . 2016.
10[For+19] Nic Ford et al. â€œAdversarial Examples Are a Natural Consequence of Test Error in
Noiseâ€. In: arXiv preprint arXiv:1901.10513 . 2019.
[Fur+18] Tommaso Furlanello et al. â€œBorn-Again Neural Networksâ€. In: International Confer-
ence on Machine Learning (ICML) . 2018.
[Gei+19] Robert Geirhos et al. â€œImageNet-trained CNNs are biased towards texture; increasing
shape bias improves accuracy and robustness.â€ In: International Conference on Learn-
ing Representations . 2019.
[Gil+18] Justin Gilmer et al. â€œAdversarial spheresâ€. In: Workshop of International Conference
on Learning Representations (ICLR) . 2018.
[Goh19a] Gabriel Goh. â€œA Discussion of â€™Adversarial Examples Are Not Bugs, They Are
Featuresâ€™: Robust Feature Leakageâ€. In: Distill (2019). https://distill.pub/2019/advex-
bugs-discussion/response-2. DOI:10.23915/distill.00019.2 .
[Goh19b] Gabriel Goh. â€œA Discussion of â€™Adversarial Examples Are Not Bugs, They
Are Featuresâ€™: Two Examples of Useful, Non-Robust Featuresâ€. In: Distill
(2019). https://distill.pub/2019/advex-bugs-discussion/response-3. DOI:10 . 23915 /
distill.00019.3 .
[GSS15] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. â€œExplaining and Harness-
ing Adversarial Examplesâ€. In: International Conference on Learning Representations
(ICLR) . 2015.
[HD19] Dan Hendrycks and Thomas G. Dietterich. â€œBenchmarking Neural Network Robust-
ness to Common Corruptions and Surface Variationsâ€. In: International Conference on
Learning Representations (ICLR) . 2019.
[He+16] Kaiming He et al. â€œDeep Residual Learning for Image Recognitionâ€. In: Conference
on Computer Vision and Pattern Recognition (CVPR) . 2016.
[He+17] Warren He et al. â€œAdversarial example defense: Ensembles of weak defenses are not
strongâ€. In: USENIX Workshop on Offensive Technologies (WOOT) . 2017.
[HVD14] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. â€œDistilling the Knowledge in a Neu-
ral Networkâ€. In: Neural Information Processing Systems (NeurIPS) Deep Learning
Workshop . 2014.
[JLT18] Saumya Jetley, Nicholas Lord, and Philip Torr. â€œWith friends like these, who needs ad-
versaries?â€ In: Advances in Neural Information Processing Systems (NeurIPS) . 2018.
[Kri09] Alex Krizhevsky. â€œLearning Multiple Layers of Features from Tiny Imagesâ€. In: Tech-
nical report . 2009.
[KSJ19] Beomsu Kim, Junghoon Seo, and Taegyun Jeon. â€œBridging Adversarial Robustness and
Gradient Interpretabilityâ€. In: International Conference on Learning Representations
Workshop on Safe Machine Learning (ICLR SafeML) . 2019.
[Lec+19] Mathias Lecuyer et al. â€œCertiï¬ed robustness to adversarial examples with differential
privacyâ€. In: Symposium on Security and Privacy (SP) . 2019.
[Liu+17] Yanpei Liu et al. â€œDelving into Transferable Adversarial Examples and Black-box At-
tacksâ€. In: International Conference on Learning Representations (ICLR) . 2017.
[LM00] Beatrice Laurent and Pascal Massart. â€œAdaptive estimation of a quadratic functional
by model selectionâ€. In: Annals of Statistics . 2000.
[Mad+18] Aleksander Madry et al. â€œTowards deep learning models resistant to adversarial at-
tacksâ€. In: International Conference on Learning Representations (ICLR) . 2018.
[MDM18] Saeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. â€œThe curse of
concentration in robust learning: Evasion and poisoning attacks from concentration of
measureâ€. In: AAAI Conference on Artiï¬cial Intelligence (AAAI) . 2018.
[Moo+17] Seyed-Mohsen Moosavi-Dezfooli et al. â€œUniversal adversarial perturbationsâ€. In: con-
ference on computer vision and pattern recognition (CVPR) . 2017.
[MV15] Aravindh Mahendran and Andrea Vedaldi. â€œUnderstanding deep image representations
by inverting themâ€. In: computer vision and pattern recognition (CVPR) . 2015.
[Nak19a] Preetum Nakkiran. â€œA Discussion of â€™Adversarial Examples Are Not Bugs,
They Are Featuresâ€™: Adversarial Examples are Just Bugs, Tooâ€. In: Distill
(2019). https://distill.pub/2019/advex-bugs-discussion/response-5. DOI:10 . 23915 /
distill.00019.5 .
11[Nak19b] Preetum Nakkiran. â€œAdversarial robustness may be at odds with simplicityâ€. In: arXiv
preprint arXiv:1901.00532 . 2019.
[OMS17] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. â€œFeature Visualizationâ€.
In:Distill . 2017.
[Pap+17] Nicolas Papernot et al. â€œPractical black-box attacks against machine learningâ€. In: Asia
Conference on Computer and Communications Security . 2017.
[PMG16] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. â€œTransferability in Machine
Learning: from Phenomena to Black-box Attacks using Adversarial Samplesâ€. In:
ArXiv preprint arXiv:1605.07277 . 2016.
[Rec+19] Benjamin Recht et al. â€œDo CIFAR-10 Classiï¬ers Generalize to CIFAR-10?â€ In: Inter-
national Conference on Machine Learning (ICML) . 2019.
[RSL18] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. â€œCertiï¬ed defenses against
adversarial examplesâ€. In: International Conference on Learning Representations
(ICLR) . 2018.
[Rus+15] Olga Russakovsky et al. â€œImageNet Large Scale Visual Recognition Challengeâ€. In:
International Journal of Computer Vision (IJCV) . 2015.
[Sch+18] Ludwig Schmidt et al. â€œAdversarially Robust Generalization Requires More Dataâ€. In:
Advances in Neural Information Processing Systems (NeurIPS) . 2018.
[Sha+19a] Ali Shafahi et al. â€œAre adversarial examples inevitable?â€ In: International Conference
on Learning Representations (ICLR) . 2019.
[Sha+19b] Adi Shamir et al. â€œA Simple Explanation for the Existence of Adversarial Examples
with Small Hamming Distanceâ€. In: arXiv preprint arXiv:1901.10861 . 2019.
[SHS19] David Stutz, Matthias Hein, and Bernt Schiele. â€œDisentangling Adversarial Robustness
and Generalizationâ€. In: Computer Vision and Pattern Recognition (CVPR) . 2019.
[Smi+17] D. Smilkov et al. â€œSmoothGrad: removing noise by adding noiseâ€. In: ICML workshop
on visualization for deep learning . 2017.
[Sug+19] Arun Sai Suggala et al. â€œRevisiting Adversarial Riskâ€. In: Conference on Artiï¬cial
Intelligence and Statistics (AISTATS) . 2019.
[Sze+14] Christian Szegedy et al. â€œIntriguing properties of neural networksâ€. In: International
Conference on Learning Representations (ICLR) . 2014.
[TG16] Thomas Tanay and Lewis Grifï¬n. â€œA Boundary Tilting Perspective on the Phenomenon
of Adversarial Examplesâ€. In: ArXiv preprint arXiv:1608.07690 . 2016.
[Tra+17] Florian Tramer et al. â€œThe Space of Transferable Adversarial Examplesâ€. In: ArXiv
preprint arXiv:1704.03453 . 2017.
[Tsi+19] Dimitris Tsipras et al. â€œRobustness May Be at Odds with Accuracyâ€. In: International
Conference on Learning Representations (ICLR) . 2019.
[Ues+18] Jonathan Uesato et al. â€œAdversarial Risk and the Dangers of Evaluating Against Weak
Attacksâ€. In: International Conference on Machine Learning (ICML) . 2018.
[Wan+18] Tongzhou Wang et al. â€œDataset Distillationâ€. In: ArXiv preprint arXiv:1811.10959 .
2018.
[WK18] Eric Wong and J Zico Kolter. â€œProvable defenses against adversarial examples via the
convex outer adversarial polytopeâ€. In: International Conference on Machine Learning
(ICML) . 2018.
[Xia+19] Kai Y . Xiao et al. â€œTraining for Faster Adversarial Robustness Veriï¬cation via Inducing
ReLU Stabilityâ€. In: International Conference on Learning Representations (ICLR) .
2019.
[Zou+18] Haosheng Zou et al. â€œGeometric Universality of Adversarial Examples in Deep Learn-
ingâ€. In: Geometry in Machine Learning ICML Workshop (GIML) . 2018.
12