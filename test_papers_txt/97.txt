Grassmann Discriminant Analysis:
a Unifying View on Subspace-Based Learning
Jihun Hamm jhham@seas.upenn.edu
Daniel D. Lee ddlee@seas.upenn.edu
GRASP Laboratory, University of Pennsylvania, Philadelphia, PA 19104 USA
Abstract
In this paper we propose a discriminant
learning framework for problems in which
data consist of linear subspaces instead of
vectors. By treating subspaces as basic el-
ements, we can make learning algorithms
adapt naturally to the problems with lin-
ear invariant structures. We propose a uni-
fying view on the subspace-based learning
method by formulating the problems on the
Grassmann manifold, which is the set of
ﬁxed-dimensional linear subspaces of a Eu-
clidean space. Previous methods on the prob-
lem typically adopt an inconsistent strategy:
feature extraction is performed in the Eu-
clidean space while non-Euclidean distances
are used. In our approach, we treat each sub-
space as a point in the Grassmann space, and
perform feature extraction and classiﬁcation
in the same space. We show feasibility of
the approach by using the Grassmann kernel
functions such as the Projection kernel and
the Binet-Cauchy kernel. Experiments with
real image databases show that the proposed
method performs well compared with state-
of-the-art algorithms.
1. Introduction
We often encounter learning problems in which the ba-
sic elements of the data are sets of vectors instead of
vectors. Suppose we want to recognize a person from
multiple pictures of the individual, taken from diﬀer-
ent angles, under diﬀerent illumination or at diﬀerent
places. When comparing such sets of image vectors, we
are free to deﬁne the similarity between sets based on
Appearing in Proceedings of the 25thInternational Confer-
ence on Machine Learning , Helsinki, Finland, 2008. Copy-
right 2008 by the author(s)/owner(s).the similarity between image vectors (Shakhnarovich
et al., 2002; Kondor & Jebara, 2003; Zhou & Chel-
lappa, 2006).
In this paper, we speciﬁcally focus on those data that
can be modeled as a collection of linear subspaces. In
the example above, let’s assume that the set of images
of a single person is well approximated by a low di-
mensional subspace (Turk & Pentland, 1991), and the
whole data is the collection of such subspaces. The
beneﬁts of using subspaces are two-fold: 1) compar-
ing two subspaces is cheaper than comparing two sets
directly when those sets are very large, and 2) it is
more robust to missing data since the subspace can
‘ﬁll-in’ the missing pictures. However the advantages
come with the challenge of representing and handling
the subspaces appropriately.
We approach the subspace-based learning problems by
formulating the problems on the Grassmann manifold,
the set of ﬁxed-dimensional linear subspaces of a Eu-
clidean space. With this unifying framework we can
make analytic comparisons of the various distances of
subspaces. In particular, we single out those distances
that are induced from the Grassmann kernels , which
are positive deﬁnite kernel functions on the Grassmann
space. The Grassmann kernels allow us to use the
usual kernel-based algorithms on this unconventional
space and to avoid ad hoc approaches to the problem.
We demonstrate the proposed framework by using the
Projection metric and the Binet-Cauchy metric and by
applying kernel Linear Discriminant Analysis to clas-
siﬁcation problems with real image databases.
1.1. Contributions of the Paper
Although the Projection metric and the Binet-Cauchy
metric were previously used (Chang et al., 2006; Wolf
& Shashua, 2003), their potential for subspace-based
learning has not been fully explored. In this work, we
provide an analytic exposition of the two metrics as
examples of the Grassmann kernels, and contrast theGrassmann Discriminant Analysis
YiYj
θ2G(m,D)
u1
v1
θ1,...,θmspan(Yi)
span(Yj)RD
Figure 1. Principal angles and Grassmann distances. Let span( Yi) and span( Yj) be two subspaces in the Euclidean space
RDon the left. The distance between two subspaces span( Yi) and span( Yj) can be measured by the principal angles
θ= [θ1, ... , θ m]/primeusing the usual innerproduct of vectors. In the Grassmann manifold viewpoint, the subspaces span( Yi)
and span( Yj) are considered as two points on the manifold G(m, D ), whose Riemannian distance is related to the principal
angles by d(Yi, Yj) =/bardblθ/bardbl2. Various distances can be deﬁned based on the principal angles.
two metrics with other metrics used in the literature.
Several subspace-based classiﬁcation methods have
been previously proposed (Yamaguchi et al., 1998;
Sakano, 2000; Fukui & Yamaguchi, 2003; Kim et al.,
2007). However, these methods adopt an inconsistent
strategy: feature extraction is performed in the Eu-
clidean space when non-Euclidean distances are used.
This inconsistency can result in complications and
weak guarantees. In our approach, the feature ex-
traction and the distance measurement are integrated
around the Grassmann kernel, resulting in a simpler
and better-understood formulation.
The rest of the paper is organized as follows. In Sec. 2
and 3 we introduce the Grassmann manifolds and de-
rive various distances on the space. In Sec. 4 we
present a kernel view of the problem and emphasize the
advantages of using positive deﬁnite metrics. In Sec. 5
we propose the Grassmann Discriminant Analysis and
compare it with other subspace-based discrimination
methods. In Sec. 6 we test the proposed algorithm for
face recognition and object categorization tasks. We
conclude in Sec. 7 with a discussion.
2. Grassmann Manifold and Principal
Angles
In this section we brieﬂy review the Grassmann man-
ifold and the principal angles.
Deﬁnition 1 The Grassmann manifold G(m, D )is
the set of m-dimensional linear subspaces of the RD.
TheG(m, D ) is a m(D−m)-dimensional compact Rie-
mannian manifold.1An element of G(m, D ) can be
1G(m, D ) can be derived as a quotient space of orthog-
onal groups G(m, D ) =O(D)/O(m)× O(D−m), whererepresented by an orthonormal matrix Yof size Dby
msuch that Y/primeY=Im, where Imis the mbymiden-
tity matrix. For example, Ycan be the mbasis vectors
of a set of pictures in RD. However, the matrix rep-
resentation of a point in G(m, D ) is not unique: two
matrices Y1andY2are considered the same if and only
if span( Y1) = span( Y2), where span( Y) denotes the
subspace spanned by the column vectors of Y. Equiva-
lently, span( Y1) = span( Y2) if and only if Y1R1=Y2R2
for some R1, R2∈ O(m). With this understanding, we
will often use the notation Ywhen we actually mean
its equivalence class span( Y), and use Y1=Y2when
we mean span( Y1) = span( Y2), for simplicity.
Formally, the Riemannian distance between two sub-
spaces is the length of the shortest geodesic connecting
the two points on the Grassmann manifold. However,
there is a more intuitive and computationally eﬃcient
way of deﬁning the distances using the principal angles
(Golub & Loan, 1996).
Deﬁnition 2 LetY1and Y2be two orthonormal
matrices of size Dbym. The principal an-
gles 0≤θ1≤ ··· ≤ θm≤π/2between two subspaces
span( Y1)andspan( Y2), are deﬁned recursively by
cosθk= maxuk∈span( Y1)maxvk∈span( Y2)uk/primevk,subject to
uk/primeuk= 1,vk/primevk= 1,
uk/primeui= 0,vk/primevi= 0,(i= 1, ..., k−1).
In other words, the ﬁrst principal angle θ1is the small-
est angle between all pairs of unit vectors in the ﬁrst
and the second subspaces. The rest of the principal
O(m) is the group of mbymorthonormal matrices. We
refer the readers to (Wong, 1967; Absil et al., 2004) for
details on the Riemannian geometry of the space.Grassmann Discriminant Analysis
angles are similarly deﬁned. It is known (Wong, 1967;
Edelman et al., 1999) that the principal angles are re-
lated to the geodesic distance by d2
G(Y1, Y2) =/summationtext
iθ2
i
(refer to Fig. 1.)
The principal angles can be computed from the Singu-
lar Value Decomposition (SVD) of Y/prime
1Y2,
Y/prime
1Y2=U(cos Θ) V/prime, (1)
where U= [u1...um],V= [v1...vm], and cos Θ
is the diagonal matrix cos Θ = diag(cos θ1...cosθm).
The cosines of the principal angles cos θ1, ... , cosθm
are also known as canonical correlations .
Although the deﬁnition can be extended to the cases
where Y1andY2have diﬀerent number of columns,
we will assume Y1andY2have the same size Dbym
throughout this paper. Also, we will occasionally use
Ginstead of G(m, D ) for simplicity.
3. Distances for Subspaces
In this paper we use the term distance as any assign-
ment of nonnegative values for each pair of points in
a space X. A valid metric is, however, a distance that
satisﬁes the additional axioms:
Deﬁnition 3 A real-valued function d:X × X → R
is called a metric if
1.d(x1, x2)≥0,
2.d(x1, x2) = 0 if and only if x1=x2,
3.d(x1, x2) =d(x2, x1),
4.d(x1, x2) +d(x2, x3)≤d(x1, x3),
for all x1, x2, x3∈ X.
A distance (or a metric) between subspaces d(Y1, Y2)
has to be invariant under diﬀerent representations
d(Y1, Y2) =d(Y1R1, Y2R2),∀R1, R2∈ O(m).
In this section we introduce various distances for sub-
spaces derivable from the principal angles.
3.1. Projection Metric and Binet-Cauchy
Metric
We ﬁrst underline two main distances of this paper.
1.Projection metric
dP(Y1, Y2) =/parenleftBiggm/summationdisplay
i=1sin2θi/parenrightBigg1/2
=/parenleftBigg
m−m/summationdisplay
i=1cos2θi/parenrightBigg1/2
.
(2)The Projection metric is the 2-norm of the sine
of principal angles (Edelman et al., 1999; Wang
et al., 2006).
2.Binet-Cauchy metric
dBC(Y1, Y2) =/parenleftBigg
1−/productdisplay
icos2θi/parenrightBigg1/2
.(3)
The Binet-Cauchy metric is deﬁned with the
product of canonical correlations (Wolf &
Shashua, 2003; Vishwanathan & Smola, 2004).
As the names hint, these two distances are in fact valid
metrics satisfying Def. 3. The proofs are deferred until
Sec. 4.
3.2. Other Distances in the Literature
We describe a few other distances used in the liter-
ature. The principal angles are the keys that relate
these distances.
1.Max Correlation
dMax(Y1, Y2) =/parenleftbig
1−cos2θ1/parenrightbig1/2= sin θ1.(4)
The max correlation is a distance based on only
the largest canonical correlation cos θ1(or the
smallest principal angle θ1). This max correla-
tion was used in previous works (Yamaguchi et al.,
1998; Sakano, 2000; Fukui & Yamaguchi, 2003).
2.Min Correlation
dMin(Y1, Y2) =/parenleftbig
1−cos2θm/parenrightbig1/2= sin θm.(5)
The min correlation is deﬁned similarly to the
max correlation. However, the min correlation
is more closely related to the Projection metric:
we can rewrite the Projection metric as dP=
2−1/2/bardblY1Y/prime
1−Y2Y/prime
2/bardblFand the min correlation
asdMin=/bardblY1Y/prime
1−Y2Y/prime
2/bardbl2.
3.Procrustes metric
dCF(Y1, Y2) = 2/parenleftBiggm/summationdisplay
i=1sin2(θi/2)/parenrightBigg1/2
.(6)
The Procrustes metric is the minimum distance
between diﬀerent representations of two subspaces
span( Y1) and span( Y2): (Chikuse, 2003)
dCF= min
R1,R2∈O(m)/bardblY1R1−Y2R2/bardblF=/bardblY1U−Y2V/bardblF,
where UandVare from (1). By deﬁnition,
the distance is invariant of the choice of theGrassmann Discriminant Analysis
bases of span( Y1) and span( Y2). The Procrustes
metric is also called chordal distance (Edelman
et al., 1999). We can similarly deﬁne the mini-
mum distance using other matrix norms such as
dC2(Y1, Y2) =/bardblY1U−Y2V/bardbl2= 2 sin( θm/2).
3.3. Which Distance to Use?
The choice of the best distance for a classiﬁcation task
depends on a few factors. The ﬁrst factor is the dis-
tribution of data. Since the distances are deﬁned with
particular combinations of the principal angles, the
best distance depends highly on the probability dis-
tribution of the principal angles of the given data.
For example, dMaxuses the smallest principal angle θ1
only, and may be robust when the data are noisy. On
the other hand, when all subspaces are sharply concen-
trated on one point, dMaxwill be close to zero for most
of the data. In this case, dMinmay be more discrimi-
native. The Projection metric dP, which uses all the
principal angles, will show intermediate characteristics
between the two distances. Similar arguments can be
made for the Procrustes metrics dCFanddC2, which
use all angles and the largest angle only, respectively.
The second criterion for choosing the distance, is the
degree of structure in the distance. Without any struc-
ture a distance can be used only with a simple K-
Nearest Neighbor (K-NN) algorithm for classiﬁcation.
When a distance have an extra structure such as tri-
angle inequality, for example, we can speed up the
nearest neighbor searches by estimating lower and up-
per limits of unknown distances (Farag´ o et al., 1993).
From this point of view, the max correlation is not a
metric and may not be used with more sophisticated
algorithms. On the other hand, the Min Correlation
and the Procrustes metrics are valid metrics2.
The most structured metrics are those which are in-
duced from a positive deﬁnite kernel. Among the met-
rics mentioned so far, only the Projection metric and
the Binet-Cauchy metric belong to this class. The
proof and the consequences of positive deﬁniteness are
the main topics of the next section.
4. Kernel Functions for Subspaces
We have deﬁned a valid metric on Grassmann mani-
folds. The next question is whether we can deﬁne a
kernel function compatible with the metric. For this
purpose let’s recall a few deﬁnitions. Let Xbe any
2The metric properties follow from the properties of
matrix 2-norm and F-norm. To check the conditions in
Def. 3 for Procrustes we use the equality min R1,R2/bardblY1R1−
Y2R2/bardbl2,F= min R3/bardblY1−Y2R3/bardbl2,FforR1, R2, R3∈ O(m).set, and k:X × X → Rbe a symmetric real-valued
function k(xi, xj) =k(xj, xi) for all xi, xj∈ X.
Deﬁnition 4 A real symmetric function is a (resp.
conditionally) positive deﬁnite kernel function, if/summationtext
i,jcicjk(xi, xj)≥0, for all x1, ..., x n(xi∈ X)and
c1, ..., c n(ci∈R)for any n∈N. (resp. for all
c1, ..., c n(ci∈R)such that/summationtextn
i=1ci= 0.)
In this paper we are interested in the kernel functions
on the Grassmann space.
Deﬁnition 5 A Grassmann kernel function is a pos-
itive deﬁnite kernel function on G.
In the following we show that the Projection metric
and the Binet-Cauchy are induced from the Grass-
mann kernels.
4.1. Projection Metric
The Projection metric can be understood by associ-
ating a point span( Y)∈ Gwith its projection matrix
Y Y/primeby an embedding:
ΨP:G(m, D )→RD×D,span( Y)/mapsto→Y Y/prime.(7)
The image Ψ P(G(m, D )) is the set of rank- mor-
thogonal projection matrices. This map is in fact
an isometric embedding (Chikuse, 2003) and the
projection metric is simply a Euclidean distance in
RD×D. The corresponding innerproduct of the space
is tr [( Y1Y/prime
1)(Y2Y/prime
2)] =/bardblY/prime
1Y2/bardbl2
F, and therefore
Proposition 1 The Projection kernel
kP(Y1, Y2) =/bardblY/prime
1Y2/bardbl2
F (8)
is a Grassmann kernel.
Proof The kernel is well-deﬁned because kP(Y1, Y2) =
kP(Y1R1, Y2R2) for any R1, R2∈ O(m). The positive
deﬁniteness follows from the properties of the Frobe-
nius norm. For all Y1, ..., Y n(Yi∈ G) and c1, ..., c n(ci∈
R) for any n∈N, we have
/summationdisplay
ijcicj/bardblY/prime
iYj/bardbl2
F=/summationdisplay
ijcicjtr(YiY/prime
iYjY/prime
j)
= tr(/summationdisplay
iciYiY/prime
i)2=/bardbl/summationdisplay
iciYiY/prime
i/bardbl2
F≥0.
We can generate a family of kernels from the Projec-
tion kernel. For example, the square-root /bardblY/prime
iYj/bardblFis
also a positive deﬁnite kernel.Grassmann Discriminant Analysis
4.2. Binet-Cauchy Metric
The Binet-Cauchy metric can also be understood from
an embedding. Let sbe a subset of {1, ..., D }with
melements s={r1, ..., r m}, and Y(s)be the m×m
matrix whose rows are the r1, ... , r m-th rows of Y. If
s1, s2, ..., s nare all such choices of the subset sordered
lexicographically, then the Binet-Cauchy embedding is
deﬁned as
ΨBC:G(m, D )→Rn, Y /mapsto→/parenleftBig
detY(s1), ...,detY(sn)/parenrightBig
,
(9)
where n=DCmis the number of choosing mrows out
ofDrows. The natural innerproduct in this case is/summationtextn
r=1detY(si)
1detY(si)
2.
Proposition 2 The Binet-Cauchy kernel
kBC(Y1, Y2) = (det Y/prime
1Y2)2= det Y/prime
1Y2Y/prime
2Y1(10)
is a Grassmann kernel.
Proof First, the kernel is well-deﬁned because
kBC(Y1, Y2) = kBC(Y1R1, Y2R2) for any R1, R2∈
O(m). To show that kBCis positive deﬁnite it suﬃces
to show that k(Y1, Y2) = det Y/prime
1Y2is positive deﬁnite.
From the Binet-Cauchy identity, we have
detY/prime
1Y2=/summationdisplay
sdetY(s)
1detY(s)
2.
Therefore, for all Y1, ..., Y n(Yi∈ G) and c1, ..., c n(ci∈
R) for any n∈N, we have
/summationdisplay
ijcicjdetY/prime
iYj=/summationdisplay
ijcicj/summationdisplay
sdetY(s)
idetY(s)
j
=/summationdisplay
s/parenleftBigg/summationdisplay
icidetY(s)
i/parenrightBigg2
≥0.
We can also generate another family of kernels
from the Binet-Cauchy kernel. Note that although
detY/prime
1Y2is a Grassmann kernel we prefer using
kBC(Y1, Y2) = det( Y/prime
1Y2)2, since it is directly related
to principal angles det( Y/prime
1Y2)2=/producttextcos2θi, whereas
detY/prime
1Y2/negationslash=/producttextcosθiin general.3Another variant
arcsin kBC(Y1, Y2) is also a positive deﬁnite kernel4
and its induced metric d= (arccos(det Y/prime
1Y2))1/2is
a conditionally positive deﬁnite metric.
4.3. Indeﬁnite Kernels from Other Metrics
Since the Projection metric and the Binet-Cauchy
metric are derived from positive deﬁnite kernels, all
3detY/prime
1Y2can be negative whereasQcosθi, the product
of singular values, is nonnegative by deﬁnition.
4Theorem 4.18 and 4.19 (Sch¨ olkopf & Smola, 2001).the kernel-based algorithms for Hilbert spaces are at
our disposal. In contrast, other metrics in the previ-
ous sections are not associated with any Grassmann
kernel. To show this we can use the following result
(Schoenberg, 1938; Hein et al., 2005):
Proposition 3 A metric dis induced from a positive
deﬁnite kernel if and only if
ˆk(x1, x2) =−d2(x1, x2)/2, x 1, x2∈ X (11)
is conditionally positive deﬁnite.
The proposition allows us to show a metric’s non-
positive deﬁniteness by constructing an indeﬁnite ker-
nel matrix from (11) as a counterexample.
There have been eﬀorts to use indeﬁnite kernels for
learning (Ong et al., 2004; Haasdonk, 2005), and sev-
eral heuristics have been proposed to make an in-
deﬁnite kernel matrix to a positive deﬁnite matrix
(Pekalska et al., 2002). However, we do not advocate
the use of the heuristics since they change the geome-
try of the original data.
5. Grassmann Discriminant Analysis
In this section we give an example of the Discriminant
Analysis on Grassmann space by using kernel LDA
with the Grassmann kernels.
5.1. Linear Discriminant Analysis
The Linear Discriminant Analysis (LDA) (Fukunaga,
1990), followed by a K-NN classiﬁer, has been success-
fully used for classiﬁcation.
Let{x1, ...,xN}be the data vectors and {y1, ..., y N}
be the class labels yi∈ {1, ..., C}. Without loss of
generality we assume the data are ordered according
to the class labels: 1 = y1≤y2≤...≤yN=C. Each
class chasNcnumber of samples.
Letµc= 1/Nc/summationtext
{i|yi=c}xibe the mean of class c, and
µ= 1/N/summationtext
ixibe the overall mean. LDA searches
for the discriminant direction wwhich maximizes the
Rayleigh quotient L(w) =w/primeSbw/w/primeSwwwhere Sb
andSware the between-class and within-class covari-
ance matrices respectively:
Sb=1
NC/summationdisplay
c=1Nc(µc−µ)(µc−µ)/prime
Sw=1
NC/summationdisplay
c=1/summationdisplay
{i|yi=c}(xi−µc)(xi−µc)/prime
The optimal wis obtained from the largest eigenvec-
tor of S−1
wSb. Since S−1
wSbhas rank C−1, there areGrassmann Discriminant Analysis
C−1-number of local optima W={w1, ...,wC−1}.
By projecting data onto the space spanned by W, we
achieve dimensionality reduction and feature extrac-
tion of data onto the most discriminant subspace.
5.2. Kernel LDA with Grassmann Kernels
Kernel LDA can be formulated by using the kernel
trick as follows. Let φ:G → H be the feature map,
and Φ = [ φ1...φN] be the feature matrix of the train-
ing points. Assuming wis a linear combination of the
those feature vectors, w= Φα, we can rewrite the
Rayleigh quotient in terms of αas
L(α) =α/primeΦ/primeSBΦα
α/primeΦ/primeSWΦα=α/primeK(V−1N1/prime
N/N)Kα
α/prime(K(IN−V)K+σ2IN)α,
(12)
where Kis the kernel matrix, 1Nis a uniform vector
[1...1]/primeof length N,Vis a block-diagonal matrix
whose c-th block is the uniform matrix 1Nc1/prime
Nc/Nc,
andσ2INis a regularizer for making the computation
stable. Similarly to LDA, the set of optimal α’s are
computed from the eigenvectors.
The procedures for using kernel LDA with the Grass-
mann kernels are summarized below:
Assume the Dbymorthonormal bases {Yi}are
already computed from the SVD of sets in the data.
Training:
1. Compute the matrix [Ktrain]ij=kP(Yi, Yj)or
kBC(Yi, Yj)for all Yi, Yjin the training set.
2. Solve maxαL(α)by eigen-decomposition.
3. Compute the (C−1)-dimensional coeﬃcients
Ftrain=α/primeKtrain.
Testing:
1. Compute the matrix [Ktest]ij=kP(Yi, Yj)or
kBC(Yi, Yj)for all Yiin training set and Yjin
the test set.
2. Compute the (C−1)-dim coeﬃcients Ftest=
α/primeKtest.
3. Perform 1-NN classiﬁcation from the Eu-
clidean distance between Ftrain andFtest.
Another way of applying LDA to subspaces is to use
the Projection embedding Ψ P(7) or the Binet-Cauchy
embedding Ψ BC(9) directly. A subspace is repre-
sented by a DbyDmatrix in the former, or by a
vector of length n=DCmin the latter. However, us-
ing these embeddings to compute SborSwis a wasteof computation and storage resources when Dis large.
5.3. Other Subspace-Based Algorithms
5.3.1. Mutual Subspace Method (MSM)
The original MSM (Yamaguchi et al., 1998) performs
simple 1-NN classiﬁcation with dMaxwith no feature
extraction. The method can be extended to any dis-
tance described in the paper. There are attempts to
use kernels for MSM (Sakano, 2000). However, the
kernel is used only to represent data in the original
space, and the algorithm is still a 1-NN classiﬁcation.
5.3.2. Constrained MSM
Constrained MSM (Fukui & Yamaguchi, 2003) is a
technique that applies dimensionality reduction to
bases of the subspaces in the original space. Let
G=/summationtext
iYiY/prime
ibe the sum of the projection matrices
and{v1, ...,vD}be the eigenvectors corresponding to
the eigenvalues {λ1≤...≤λD}of G. The authors
claim that the ﬁrst few eigenvectors v1, ...,vdofGare
more discriminative than the later eigenvectors, and
they suggest projecting the basis vectors of each sub-
space Y1onto the span( v1, ...,vl), followed by normal-
ization and orthonormalization. However these proce-
dure lack justiﬁcations, as well as a clear criterion for
choosing the dimension d, on which the result crucially
depends from our experience.
5.3.3. Discriminant Analysis of Canonical
Correlations (DCC)
DCC (Kim et al., 2007) can be understood as a non-
parametric version of linear discrimination analysis us-
ing the Procrustes metric (6). The algorithm ﬁnds the
discriminating direction wwhich maximize the ratio
L(w) =w/primeSBw/w/primeSww, where SbandSware the
nonparametric between-class and within-class ‘covari-
ance’ matrices:
Sb=/summationdisplay
i/summationdisplay
j∈Bi(YiU−YjV)(YiU−YjV)/prime
Sw=/summationdisplay
i/summationdisplay
j∈Wi(YiU−YjV)(YiU−YjV)/prime,
where UandVare from (1). Recall that tr( YiU−
YjV)(YiU−YjV)/prime=/bardblYiU−YjV/bardbl2
Fis the squared
Procrustes metric. However, unlike our method, Sb
andSwdo not admit a geometric interpretation as
true covariance matrices, and cannot be kernelized ei-
ther. A main disadvantage of the DCC is that the
algorithm iterates the two stages of 1) maximizing the
ratio L(w) and of 2) computing SbandSw, which
results in computational overheads and more parame-Grassmann Discriminant Analysis
ters to be determined. This reﬂects the complication
of treating the problem in a Euclidean space with a
non-Euclidean distance.
6. Experiments
In this section we test the Grassmann Discriminant
Analysis for 1) a face recognition task and 2) an object
categorization task with real image databases.
6.1. Algorithms
We use the following six methods for feature extraction
together with an 1-NN classiﬁer.
1) GDA1 (with Projection kernel), 2) GDA2 (with
Binet-Cauchy kernel), 3) Min dist , 4) MSM, 5) cMSM,
and 6) DCC.
For GDA1 and GDA2, the optimal values of σ
are found by scanning through a range of val-
ues. The results do not seem to vary much as
long as σis small enough. The Min dist is
a simple pairwise distance which is not subspace-
based. If YiandYjare two sets of basis vectors:
Yi={yi1, ...,yim i}andYj={yj1, ...,yjm j}, then
dMindist (Yi, Yj) = min k,l/bardblyik−yjl/bardbl2.For cMSM and
DCC, the optimal dimension lis found by exhaus-
tive searching. For DCC, we have used two nearest-
neighbors for BiandWiin Sec. 5.3.3. Since the Sw
andSbare likely to be rank deﬁcient, we ﬁrst reduced
the dimension of the data to N−Cusing PCA as
recommended. Each optimization is iterated 5 times.
6.2. Testing Illumination-Invariance with Yale
Face Database
The Yale face database and the Extended Yale face
database (Georghiades et al., 2001) together consist of
pictures of 38 subjects with 9 diﬀerent poses and 45 dif-
ferent lighting conditions. Face regions were cropped
from the original pictures, resized to 24 ×21 pixels
(D= 504), and normalized to have the same variance.
For each subject and each pose, we model the illumi-
nation variations by a subspace of the size m= 1, ...,5,
spanned by the 1 to 5 largest eigenvectors from SVD.
We evaluate the recognition rate of subjects with nine-
fold cross validation, holding out one pose of all sub-
jects from the training set and using it for test.
The recognition rates are shown in Fig. 2. The GDA1
outperforms the other methods consistently. The
GDA2 also performs well for small m, but performs
worse as mbecomes large. The rates of the others
also seem to decrease as mincreases. An interpreta-
tion of the observation is that the ﬁrst few eigenvec-tors from the data already have enough information
and the smaller eigenvectors are spurious for discrim-
inating the subjects.
6.3. Testing Pose-Invariance with ETH-80
Database
The ETH-80 (Leibe & Schiele, 2003) database con-
sists of pictures of 8 object categories (‘apple’, ‘pear’,
‘tomato’, ‘cow’, ‘dog’, ‘horse’, ‘cup’, ‘car’). Each cat-
egory has 10 objects that belong to the category, and
each object is recorded under 41 diﬀerent poses. Im-
ages were resized to 32 ×32 pixels ( D= 1024) and
normalized to have the same variance. For each cate-
gory and each object, we model the pose variations by
a subspace of the size m= 1, ...,5, spanned by the 1
to 5 largest eigenvectors from SVD. We evaluate the
classiﬁcation rate of the categories with ten-fold cross
validation, holding out one object instance of each cat-
egory from the training set and using it for test.
The recognition rates are also summarized in Fig. 2.
The GDA1 also outperforms the other methods most
of the time, but the cMSM performs better than GDA2
asmincreases. The rates seem to peak around m=
4 and then decrease as mincreases. This results is
consistent with the observation that the eigenvalues
from this database decrease more gradually than the
eigenvalues from the Yale face database.
7. Conclusion
In this paper we have proposed a Grassmann frame-
work for problem in which data consist of subspaces.
By using the Projection metric and the Binet-Cauchy
metric, which are derived from the Grassmann ker-
nels, we were able to apply kernel methods such as
kernel LDA to subspace data. In addition to having
theoretically sound grounds, the proposed method also
outperformed state-of-the-art methods in two experi-
ments with real data. As a future work, we are pur-
suing a better understanding of probabilistic distribu-
tions on the Grassmann manifold.
References
Absil, P., Mahony, R., & Sepulchre, R. (2004). Riemannian
geometry of Grassmann manifolds with a view on algo-
rithmic computation. Acta Appl. Math. ,80, 199–220.
Chang, J.-M., Beveridge, J. R., Draper, B. A., Kirby, M.,
Kley, H., & Peterson, C. (2006). Illumination face spaces
are idiosyncratic. IPCV (pp. 390–396).
Chikuse, Y. (2003). Statistics on special manifolds, lecture
notes in statistics, vol. 174 . New York: Springer.
Edelman, A., Arias, T. A., & Smith, S. T. (1999). TheGrassmann Discriminant Analysis
Figure 2. Recognition rates of subjects from Yale face database (Left), and classiﬁcation rates of categories in ETH-80
database (Right). The bars represent the rates of six algorithms (GDA1, GDA2, Min Dist, MSM, cMSM, DCC) evaluated
form= 1, ....,5 where mis the number of basis vectors for subspaces. The GDA1 achieves the best rates consistently,
and the GDA2 also performs competitively for small m.
geometry of algorithms with orthogonality constraints.
SIAM J. Matrix Anal. Appl. ,20, 303–353.
Farag´ o, A., Linder, T., & Lugosi, G. (1993). Fast nearest-
neighbor search in dissimilarity spaces. IEEE Trans.
Pattern Anal. Mach. Intell. ,15, 957–962.
Fukui, K., & Yamaguchi, O. (2003). Face recognition using
multi-viewpoint patterns for robot vision. Int. Symp. of
Robotics Res. (pp. 192–201).
Fukunaga, K. (1990). Introduction to statistical pattern
recognition (2nd ed.) . San Diego, CA, USA: Academic
Press Professional, Inc.
Georghiades, A. S., Belhumeur, P. N., & Kriegman, D. J.
(2001). From few to many: Illumination cone models for
face recognition under variable lighting and pose. IEEE
Trans. Pattern Anal. Mach. Intell. ,23, 643–660.
Golub, G. H., & Loan, C. F. V. (1996). Matrix compu-
tations (3rd ed.) . Baltimore, MD, USA: Johns Hopkins
University Press.
Haasdonk, B. (2005). Feature space interpretation of svms
with indeﬁnite kernels. IEEE Trans. Pattern Anal.
Mach. Intell. ,27, 482–492.
Hein, M., Bousquet, O., & Sch¨ olkopf, B. (2005). Maximal
margin classiﬁcation for metric spaces. J. Comput. Syst.
Sci.,71, 333–359.
Kim, T.-K., Kittler, J., & Cipolla, R. (2007). Discrimi-
native learning and recognition of image set classes us-
ing canonical correlations. IEEE Trans. Pattern Anal.
Mach. Intell. ,29, 1005–1018.
Kondor, R. I., & Jebara, T. (2003). A kernel between sets
of vectors. Proc. of the 20th Int. Conf. on Mach. Learn.
(pp. 361–368).
Leibe, B., & Schiele, B. (2003). Analyzing appearance and
contour based methods for object categorization. CVPR ,
02, 409.
Ong, C. S., Mary, X., Canu, S., & Smola, A. J. (2004).
Learning with non-positive kernels. Proc. of 21st Int.
Conf. on Mach. Learn. (p. 81). New York, NY, USA:
ACM.Pekalska, E., Paclik, P., & Duin, R. P. W. (2002). A gener-
alized kernel approach to dissimilarity-based classiﬁca-
tion. J. Mach. Learn. Res. ,2, 175–211.
Sakano, H.; Mukawa, N. (2000). Kernel mutual subspace
method for robust facial image recognition. Proc. of Int.
Conf. on Knowledge-Based Intell. Eng. Sys. and App.
Tech. (pp. 245–248).
Schoenberg, I. J. (1938). Metric spaces and positive deﬁnite
functions. Trans. Amer. Math. Soc. ,44, 522–536.
Sch¨ olkopf, B., & Smola, A. J. (2001). Learning with ker-
nels: Support vector machines, regularization, optimiza-
tion, and beyond . Cambridge, MA, USA: MIT Press.
Shakhnarovich, G., John W. Fisher, I., & Darrell, T.
(2002). Face recognition from long-term observations.
Proc. of the 7th Euro. Conf. on Computer Vision (pp.
851–868). London, UK.
Turk, M., & Pentland, A. P. (1991). Eigenfaces for recog-
nition. J. Cog. Neurosc. ,3, 71–86.
Vishwanathan, S., & Smola, A. J. (2004). Binet-cauchy
kernels. Proc. of Neural Info. Proc. Sys. .
Wang, L., Wang, X., & Feng, J. (2006). Subspace distance
analysis with application to adaptive bayesian algorithm
for face recognition. Pattern Recogn. ,39, 456–464.
Wolf, L., & Shashua, A. (2003). Learning over sets using
kernel principal angles. J. Mach. Learn. Res. ,4, 913–
931.
Wong, Y.-C. (1967). Diﬀerential geometry of Grassmann
manifolds. Proc. of the Nat. Acad. of Sci., Vol. 57 , 589–
594.
Yamaguchi, O., Fukui, K., & Maeda, K. (1998). Face
recognition using temporal image sequence. Proc. of the
3rd. Int. Conf. on Face & Gesture Recognition (p. 318).
Washington, DC, USA: IEEE Computer Society.
Zhou, S. K., & Chellappa, R. (2006). From sample similar-
ity to ensemble similarity: Probabilistic distance mea-
sures in reproducing kernel hilbert space. IEEE Trans.
Pattern Anal. Mach. Intell. ,28, 917–929.