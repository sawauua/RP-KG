Curriculum Learning

Yoshua Bengio1Yoshua.Bengio@umontreal.ca
J er^ ome Louradour1;2jeromelouradour@gmail.com
Ronan Collobert3ronan@collobert.com
Jason Weston3jasonw@nec-labs.com
(1) U. Montreal, P.O. Box 6128, Montreal, Canada (2) A2iA SA, 40bis Fabert, Paris, France
(3) NEC Laboratories America, 4 Independence Way, Princeton, NJ, USA
Abstract
Humans and animals learn much better when
the examples are not randomly presented but
organized in a meaningful order which illus-
trates gradually more concepts, and gradu-
ally more complex ones. Here, we formal-
ize such training strategies in the context
of machine learning, and call them \curricu-
lum learning". In the context of recent re-
search studying the diculty of training in
the presence of non-convex training criteria
(for deep deterministic and stochastic neu-
ral networks), we explore curriculum learn-
ing in various set-ups. The experiments show
that signicant improvements in generaliza-
tion can be achieved. We hypothesize that
curriculum learning has both an e
ect on the
speed of convergence of the training process
to a minimum and, in the case of non-convex
criteria, on the quality of the local minima
obtained: curriculum learning can be seen
as a particular form of continuation method
(a general strategy for global optimization of
non-convex functions).
1. Introduction
Humans need about two decades to be trained as
fully functional adults of our society. That training
is highly organized, based on an education system and
a curriculum which introduces di
erent concepts at
di
erent times, exploiting previously learned concepts
to ease the learning of new abstractions. By choos-
ing which examples to present and in which order to
present them to the learning system, one can guide
Appearing in Proceedings of the 26thInternational Confer-
ence on Machine Learning , Montreal, Canada, 2009. Copy-
right 2009 by the author(s)/owner(s).training and remarkably increase the speed at which
learning can occur. This idea is routinely exploited in
animal training where it is called shaping (Skinner,
1958; Peterson, 2004; Krueger & Dayan, 2009).
Previous research (Elman, 1993; Rohde & Plaut, 1999;
Krueger & Dayan, 2009) at the intersection of cogni-
tive science and machine learning has raised the follow-
ing question: can machine learning algorithms benet
from a similar training strategy? The idea of training a
learning machine with a curriculum can be traced back
at least to Elman (1993). The basic idea is to start
small , learn easier aspects of the task or easier sub-
tasks, and then gradually increase the diculty level.
The experimental results, based on learning a simple
grammar with a recurrent network (Elman, 1993), sug-
gested that successful learning of grammatical struc-
ture depends, not on innate knowledge of grammar,
but on starting with a limited architecture that is at
rst quite restricted in complexity, but then expands
its resources gradually as it learns. Such conclusions
are important for developmental psychology, because
they illustrate the adaptive value of starting, as hu-
man infants do, with a simpler initial state, and then
building on that to develop more and more sophis-
ticated representations of structure. Elman (1993)
makes the statement that this strategy could make
it possible for humans to learn what might otherwise
prove to be unlearnable. However, these conclusions
have been seriously questioned in Rohde and Plaut
(1999). The question of guiding learning of a recurrent
neural network for learning a simple language and in-
creasing its capacity along the way was recently revis-
ited from the cognitive perspective (Krueger & Dayan,
2009), providing evidence for faster convergence using
a shaping procedure. Similar ideas were also explored
in robotics (Sanger, 1994), by gradually making the
learning task more dicult.
We want to clarify when and why a curriculum orCurriculum Learning
\starting small" strategy can benet machine learning
algorithms. We contribute to this question by show-
ing several cases - involving vision and language tasks -
in which very simple multi-stage curriculum strategies
give rise to improved generalization and faster con-
vergence. We also contribute to this question with the
introduction of a hypothesis which may help to explain
some of the advantages of a curriculum strategy. This
hypothesis is essentially that a well chosen curriculum
strategy can act as a continuation method (Allgower &
Georg, 1980), i.e., can help to nd better local minima
of a non-convex training criterion. In addition, the ex-
periments reported here suggest that (like other strate-
gies recently proposed to train deep deterministic or
stochastic neural networks) the curriculum strategies
appear on the surface to operate like a regularizer, i.e.,
their benecial e
ect is most pronounced on the test
set. Furthermore, experiments on convex criteria also
show that a curriculum strategy can speed the conver-
gence of training towards the global minimum.
2. On the dicult optimization
problem of training deep neural
networks
To test the hypothesis that a curriculum strategy could
help to nd better local minima of a highly non-convex
criterion, we turn our attention to training of deep ar-
chitectures, which have been shown to involve good
solutions in local minima that are almost impossible
to nd by random initialization (Erhan et al., 2009).
Deep learning methods attempt to learn feature hi-
erarchies. Features at higher levels are formed by
the composition of lower level features. Automati-
cally learning multiple levels of abstraction may al-
low a system to induce complex functions mapping
the input to the output directly from data, without
depending heavily on human-crafted features. A the-
oretical motivation for deep architectures comes from
complexity theory: some functions can be represented
compactly with an architecture of depth k, but re-
quire an exponential size architecture when the depth
is restricted to be less than k(H astad & Goldmann,
1991; Bengio, 2009). However, training deep archi-
tectures involves a potentially intractable non-convex
optimization problem (Bengio, 2009), which compli-
cates their analysis. There were no good algorithms
for training fully-connected deep architectures before
2006, when Hinton et al. (2006) introduced a learn-
ing algorithm that greedily trains one layer at a time.
It exploits an unsupervised generative learning algo-
rithm for each layer: a Restricted Boltzmann Machine
(RBM) (Freund & Haussler, 1994). It is conceivable
that by training each layer one after the other, onerst learns the simpler concepts (represented in the
rst layer), then slightly more abstract concepts (rep-
resented in the second layer), etc. Shortly after, strate-
gies for building deep architectures from related vari-
ants were proposed (Ranzato et al., 2007; Bengio et al.,
2007). These works showed the advantage of deep ar-
chitectures over shallow ones and of the unsupervised
pre-training strategy in a variety of settings. Deep ar-
chitectures have been applied with success not only in
classication tasks (Ranzato et al., 2007; Bengio et al.,
2007; Larochelle et al., 2007; Ranzato et al., 2008; Vin-
cent et al., 2008), but also in regression (Salakhutdi-
nov & Hinton, 2008), dimensionality reduction (Hin-
ton & Salakhutdinov, 2006; Salakhutdinov & Hinton,
2007), natural language processing (Collobert & We-
ston, 2008; Weston et al., 2008), and collaborative l-
tering (Salakhutdinov et al., 2007).
Nonetheless, training deep architectures is a dicult
problem. Erhan et al. (2009) and Larochelle et al.
(2007) studied this question experimentally to clarify
why deeper networks can sometimes generalize much
better and why some strategies such as unsupervised
pre-training can make this possible. Erhan et al.
(2009) found that unsupervised pre-training makes it
possible to start the supervised optimization in a re-
gion of parameter space corresponding to solutions
that were not much better in terms of nal training er-
ror but substantially better in terms of test error. This
suggested a dual e
ect of unsupervised pre-training,
both in terms of helping optimization (starting in bet-
ter basins of attraction of the descent procedure in
parameter space) and as a kind of regularizer.
The experiments presented here suggest that pre-
training with a curriculum strategy might act similarly
to unsupervised pre-training, acting both as a way to
nd better local minima and as a regularizer. They
also suggest that they help to reach faster convergence
to a minimum of the training criterion.
3. A curriculum as a continuation
method
Continuation methods (Allgower & Georg, 1980) are
optimization strategies for dealing with minimizing
non-convex criteria. Although these global optimiza-
tion methods provide no guarantee that the global
optimum will be obtained, they have been particu-
larly useful in computational chemistry to nd approx-
imate solutions of dicult optimization problems in-
volving the congurations of molecules (Coleman &
Wu, 1994; Wu, 1997). The basic idea is to rst opti-
mize a smoothed objective and gradually consider less
smoothing, with the intuition that a smooth versionCurriculum Learning
of the problem reveals the global picture. One denes
a single-parameter family of cost functions C() such
thatC0can be optimized easily (maybe convex in ),
whileC1is the criterion that we actually wish to mini-
mize. One rst minimizes C0() and then gradually in-
creaseswhile keeping at a local minimum of C().
TypicallyC0is a highly smoothed version of C1, so
thatgradually moves into the basin of attraction of
a dominant (if not global) minimum of C1. Applying
a continuation method to the problem of minimizing
a training criterion involves a sequence of training cri-
teria, starting from one that is easier to optimize, and
ending with the training criterion of interest.
At an abstract level, a curriculum can also be seen
as a sequence of training criteria. Each training crite-
rion in the sequence is associated with a di
erent set of
weights on the training examples, or more generally, on
a reweighting of the training distribution. Initially, the
weights favor \easier" examples, or examples illustrat-
ing the simplest concepts, that can be learned most
easily. The next training criterion involves a slight
change in the weighting of examples that increases the
probability of sampling slightly more dicult exam-
ples. At the end of the sequence, the reweighting of
the examples is uniform and we train on the target
training set or the target training distribution.
One way to formalize this idea is the following. Let z
be a random variable representing an example for the
learner (possibly an ( x;y) pair for supervised learn-
ing). LetP(z) be the target training distribution from
which the learner should ultimately learn a function of
interest. Let 0W(z)1 be the weight applied to
examplezat stepin the curriculum sequence, with
01, andW1(z) = 1. The corresponding train-
ing distribution at step is
Q(z)/W(z)P(z)8z (1)
such thatR
Q(z)dz= 1. Then we have
Q1(z) =P(z)8z: (2)
Consider a monotonically increasing sequence of val-
ues, starting from = 0 and ending at = 1.
Denition We call the corresponding sequence of dis-
tributionsQ(following eqns 1 and 2) a curriculum
if the entropy of these distributions increases
H(Q)<H(Q+)8>0 (3)
andW(z) ismonotonically increasing in , i.e.,
W+(z)W(z)8z;8>0: (4)
To illustrate this denition, consider the simple set-
ting where Qis concentrated on a nite set of ex-
amples, and increasing means adding new examplesto that set: the support of Qincreases with , and
the sequence of training distributions corresponds to
a sequence of embedded training sets, starting with a
small set of easy examples and ending with the target
training set. We want the entropy to increase so as
to increase the diversity of training examples, and we
want the weights of particular examples to increase as
they get \added" into the training set.
In the experiments below the sequence of training sets
is always discrete. In fact the curriculum strategy
worked in some of our experiments with a sequence
of just two steps: rst a set of easy examples, and
then the target training set. At the other extreme,
if training proceeds in a stochastic manner by sam-
pling training examples from a distribution, then one
could imagine a continuous sequence of sampling dis-
tributions which gradually gives more weight W(z) to
the more dicult examples, until all examples have an
equal weight of 1.
Up to now we have not dened what \easy examples"
meant, or equivalently, how to sort examples into a
sequence that illustrates the simpler concepts rst. In
the following experiments we explore a few simple ways
to dene a curriculum, but clearly a lot more work is
needed to explore di
erent curriculum strategies, some
of which may be very specic to particular tasks.
4. Toy Experiments with a Convex
Criterion
4.1. Cleaner Examples May Yield Better
Generalization Faster
One simple way in which easy examples could help is
by being less \noisy", as shown theoretically (Der enyi
et al., 1994) in the case of a Teacher-Learner pair
of Perceptrons. In the supervised classication set-
ting, an example is considered noisy if it falls on the
incorrect side of the decision surface of the Bayes
classier. Noisy examples can slow down conver-
gence, as illustrated with the following toy experiment.
Two-dimensional inputs are generated from a di
er-
ent Gaussian for each one of the two classes. We de-
ne class targets y= 1 andy= 1 respectively. The
Gaussian mean for class yis at (y=p
2;y=p
2) and both
Gaussians have standard deviation 1. Starting from
random initial parameters (50 times), we train a linear
SVM with 50 training examples. Let wbe the weight
vector of the Bayes classier. We nd that training
only with \easy" examples (for which yw0x>0) gives
rise to lower generalization error: 16.3% error vs 17.1%
error (average over 50 runs), and the di
erence is sta-
tistically signicant.Curriculum Learning
In principle one could argue that dicult examples
can be more informative than easy examples. Here
the dicult examples are probably not useful because
they confuse the learner rather than help it establish
the right location of the decision surface. This exper-
iment does not involve a curriculum strategy yet, but
it may help to understand why easier examples could
be useful, by avoiding to confuse the learner.
4.2. Introducing Gradually More Dicult
Examples Speeds-up Online Training
We train a Perceptron from articially generated data
where the target is y= sign(w0xrelevant ) andwis sam-
pled from a Normal(0,1). The training pairs are ( x;y)
withx= (xrelevant;xirrelevant ), i.e., some of the inputs
are irrelevant, not predictive of the target class. Rel-
evant inputs are sampled from a Uniform(0,1) distri-
bution. Irrelevant inputs can either be set to 0 or to
a Uniform(0,1). The number of irrelevant inputs that
is set to 0 varies randomly (uniformly) from example
to example, and can be used to sort examples from
the easiest (with all irrelevant inputs zeroed out) to
the most dicult (with none of the irrelevant inputs
zeroed out). Another way to sort examples is by the
marginyw0x, with easiest examples corresponding to
larger values. The learning rate is 1 (it does not matter
since there is no margin and the classier output does
not depend on the magnitude of w0xbut only on its
sign). Initial weights are sampled from a Normal(0,1).
We train the Perceptron with 200 examples (i.e., 200
Perceptron updates) and measure generalization error
at the end. Figure 1 shows average estimated gen-
eralization error measured at the end of training and
averaged across 500 repetitions from di
erent initial
conditions and di
erent random sampling of training
examples. We compare a no curriculum setting (ran-
dom ordering), with a curriculum setting in which
examples are ordered by easiness, starting with the
easiest examples, and two easiness criteria (number of
noisy irrelevant inputs, margin yw0x). All error rate
di
erences between the curriculum strategy and the
no-curriculum are statistically signicant (di
erences
of more than .01 were all statistically signicant at 5%
under a t-test).
5. Experiments on shape recognition
The task of interest here is to classify geometri-
cal shapes into 3 classes (rectangle, ellipse, trian-
gle), where the input is a 32 32 grey-scale image.
As shown in Figure 2, two di
erent datasets were
generated: whereas GeomShapes data consist in im-
ages of rectangles, ellipses and triangles, BasicShapes
data only include special cases of the above: squares,
Figure 1. Average error rate of Perceptron, with or with-
out the curriculum. Top: the number of nonzero irrelevant
inputs determines easiness. Bottom: the margin yw0xde-
termines easiness.
circles and equilateral triangles. The di
erence be-
tween BasicShapes data and GeomShapes data is that
BasicShapes images exhibit less variability in shape.
Other degrees of variability which are present in both
sets are the following: object position, size, orienta-
tion, and also the grey levels of the foreground and
background. Besides, some geometrical constraints are
also added so as to ensure that any shape object ts
entirely within the image, and a minimum size and
minimum contrast (di
erence in grey levels) between
foreground and background is imposed.
Note that the above \easy distribution" occupying a
very small volume in input space compared to the tar-
get distribution does not contradict condition 4. In-
deed, the non-zero weights (on easy examples) can ini-
tially be very small, so that their nal weight in the
target distribution can be very small.
Figure 2. Sample inputs from BasicShapes (top) and
GeomShapes (bottom). Images are shown here with a
higher resolution than the actual dataset (32x32 pixels).
The experiments were carried out on a multi-layer neu-
ral network with 3 hidden layers, trained by stochas-Curriculum Learning
tic gradient descent on the negative conditional log-
likelihood, i.e., a task which is known to involve a dif-
cult non-convex optimization problem (Erhan et al.,
2009). An epoch is a stochastic gradient descent pass
through a training set of 10 000 examples. The cur-
riculum consists in a 2-step schedule:
1. Perform gradient descent on the BasicShapes
training set, until \switch epoch" is reached.
2. Then perform gradient descent on the GeomShapes
training set.
Generalization error is always evaluted on the
GeomShapes test set. The baseline corresponds to
training the network only on the GeomShapes train-
ing set (for the same number of training epochs),
and corresponds to \switch epoch"=0. In our ex-
periments, there is a total of 10 000 examples in
both training sets, and 5 000 examples for valida-
tion, 5 000 for testing. All datasets are available at
www.iro.umontreal.ca/ lisa/ptwiki/BabyAIShapesDatasets
The hyper-parameters are the following: learning rate
of stochastic gradient descent and number of hidden
units. The selection of hyper-parameters is simpli-
ed using the following heuristic: all hyper-parameters
were chosen so as to have the best baseline perfor-
mance on the GeomShapes validation set without cur-
riculum. These hyper-parameter values are then used
for the curriculum experiments.
Figure (3) shows the distribution of test errors over
20 di
erent random seeds, for di
erent values of the
\switch epoch": 0 (the baseline with no curriculum)
and the powers of 2 until 128. After switching to the
target training distribution, training continues either
until 256 epochs or until validation set error reaches
a minimum (early stopping). The gure shows the
distribution of test error (after early stopping) as a
function of the \switch epoch". Clearly, the best gen-
eralization is obtained by doing a 2-stage curriculum
where the rst half of the total allowed training time
(of 256 epochs) is spent on the easier examples rather
than on the target examples.
One potential issue with this experiment is that the
curriculum-trained model overall saw more examples
than the no-curriculum examples, although in the sec-
ond part of training (with the target distribution) both
types of models converge (in the sense of early stop-
ping) to a local minimum with respect to the error on
the target training distribution, suggesting that di
er-
ent local minima are obtained. Note also that the easy
examples have less variability than the hard examples
(only a subset of the shape variations are shown, e.g.
only squares instead of all kinds of rectangles). To
|
0 2 4 8 16 32 64 1280.15 0.16 0.17 0.18 0.19 0.20 0.21
switch epochbest validation classification errorFigure 3. Box plot of test classication error distribution
as a function of the \switch epoch", with a 3-hidden-
layers neural network trained by stochastic gradient de-
scent. Each box corresponds to 20 seeds for initializing the
parameters. The horizontal line inside the box represents
the median (50th percentile), the borders of the box the
25th and the 75th percentile and the ends of the bars the
5th and 95th percentiles.
eliminate the explanation that better results are ob-
tained with the curriculum because of seeing more ex-
amples, we trained a no-curriculum model with the
union of the BasicShapes and GeomShapes training sets,
with a nal test error still signicantly worse than
with the curriculum (with errors similar to \switch
epoch"=16). We also veried that training only with
BasicShapes yielded poor results.
6. Experiments on language modeling
We are interested here in training a language model ,
predicting the best word which can follow a given con-
text of words in a correct English sentence. Follow-
ing Collobert and Weston (2008) we only try to com-
pute a score for the next word that will have a large
rank compared to the scores of other words, and we
compute the score with the architecture of Figure 4.
Whereas other language models prior to Collobert and
Weston (2008) optimized the log-likelihood of the next
word, the ranking approach does not require com-
puting the score over all the vocabulary words dur-
ing training, as shown below. Instead it is enough to
sample a negative example. In Collobert and Weston
(2008), the main objective is to learn an embedding
for the words as a side-e
ect of learning to compute
this score. The authors showed how to use these em-
beddings in several language modeling tasks, in a form
of multi-task learning, yielding improved results.
Given any xed size window of text s, we consider a
language model f(s) which produces a score for these
windows of text. We want the score of a correct win-
dow of text sto be larger, with a margin of 1, than any
other word sequence swwhere the last word has been
replaced by another word wof the vocabulary. This
corresponds to minimizing the expected value of theCurriculum Learning
Input Window
the cat sat on theword to score
s(1) s(2) s(3) s(4) s(5)text
indices
Lookup Table
LTw
Tanh
LinearLinear50
250 (concatenation)
Score100context
Figure 4. Architecture of the deep neural network comput-
ing the score of the next word given the previous ones.
following ranking loss over sequences ssampled from
a datasetSof valid English text windows:
Cs=X
w2D1
jDjCs;w=X
w2D1
jDjmax(0;1 f(s)+f(sw))
(5)
whereDis the considered word vocabulary and S
is the set of training word sequences. Note that a
stochastic sample of the gradient with respect to Cs
can be obtained by sampling a counter-example word
wuniformly fromD. For each word sequence swe
then compute f(s) andf(sw) and the gradient of
max(0;1 f(s) +f(sw)) with respect to parameters.
6.1. Architecture
The architecture of our language model (Figure 4)
follows the work introduced by Bengio et al. (2001)
and Schwenk and Gauvain (2002), and closely resem-
bles the one used in Collobert and Weston (2008).
Each word i2 D isembedded into ad-dimensional
space using a look-up table LTW():LTW(i) =Wi;
whereW2RdjDjis a matrix of parameters to
be learnt, Wi2Rdis theithcolumn of Wand
dis the embedding dimension hyper-parameter. In
the rst layer an input window fs1; s2; ::: sngofn
words inDis thus transformed into a series of vectors
fWs1; Ws2; ::: Wsngby applying the look-up table to
each of its words.
The feature vectors obtained by the look-up table layer
are then concatenated and fed to a classical linear
layer. A non-linearity (like tanh( )) follows and the
score of the language model is nally obtained after
applying another linear layer with one output.The cost (5) is minimized using stochastic gradient
descent, by iteratively sampling pairs ( s; w) composed
of a window of text sfrom the training set Sand a
random word w, and performing a step in the direction
of the gradient of Cs;wwith respect to the parameters,
including the matrix of embeddings W.
Figure 5. Ranking language model trained with vs without
curriculum on Wikipedia. \Error" is log of the rank of the
next word (within 20k-word vocabulary). In its rst pass
through Wikipedia, the curriculum-trained model skips ex-
amples with words outside of 5k most frequent words (down
to 270 million from 631 million), then skips examples out-
side 10k most frequent words (doing 370 million updates),
etc. The drop in rank occurs when the vocabulary size
is increased, as the curriculum-trained model quickly gets
better on the new words.
6.2. Experiments
We chose the training set Sas all possible win-
dows of text of size n= 5 from Wikipedia
(http://en.wikipedia.org ), obtaining 631 million
windows processed as in Collobert and Weston (2008).
We chose as a curriculum strategy to grow the vocabu-
lary size: the rst pass over Wikipedia was performed
using the 5 ;000 most frequent words in the vocabu-
lary, which was then increased by 5 ;000 words at each
subsequent pass through Wikipedia. At each pass, any
window of text containing a word not in the consid-
ered vocabulary was discarded. The training set is
thus increased after each pass through Wikipedia. We
compare against no curriculum, where the network
is trained using the nal desired vocabulary size of
20;000. The evaluation criterion was the average of
the log of the rank of the last word in each test win-
dow, taken in a test set of 10 ;000 windows of text not
seen during the training, with words from the most
20;000 frequent ones (i.e. from the target distribu-
tion). We chose the word embedding dimension to be
d= 50, and the number of hidden units as 100.
In Figure 5, we observe that the log rank on the targetCurriculum Learning
distribution with the curriculum strategy crosses the
error of the no-curriculum strategy after about 1 bil-
lion updates, shortly after switching to the target vo-
cabulary size of 20,000 words, and the di
erence keeps
increasing afterwards. The nal test set average log-
ranks are 2.78 and 2.83 respectively, and the di
erence
is statistically signicant.
7. Discussion and Future Work
We started with the following question left from previ-
ous cognitive science research (Elman, 1993; Rohde &
Plaut, 1999): can machine learning algorithms ben-
et from a curriculum strategy? Our experimental
results in many di
erent settings bring evidence to-
wards a positive answer to that question. It is plausi-
ble that some curriculum strategies work better than
others, that some are actually useless for some tasks
(as in Rohde and Plaut (1999)), and that better re-
sults could be obtained on our data sets with more
appropriate curriculum strategies. After all, the art of
teaching is dicult and humans do not agree among
themselves about the order in which concepts should
be introduced to pupils.
From the machine learning point of view, once the
success of some curriculum strategies has been estab-
lished, the important questions are: why? and how?
This is important to help us devise better curriculum
strategies and automate that process to some extent.
We proposed a number of hypotheses to explain the
potential advantages of a curriculum strategy:
faster training in the online setting (i.e. faster
both from an optimization and statistical point of
view) because the learner wastes less time with
noisy or harder to predict examples (when it is
not ready to incorporate them),
guiding training towards better regions in param-
eter space, i.e. into basins of attraction (local
minima) of the descent procedure associated with
better generalization: a curriculum can be seen as
a particular continuation method.
Faster convergence with a curriculum was already ob-
served in (Krueger & Dayan, 2009). However, unlike
in our experiments where capacity is xed throughout
the curriculum, they found that compared to using
no curriculum, worse results were obtained with xed
neural resources. The reasons for these di
erences re-
main to be claried. In both cases, though, an appro-
priate curriculum strategy acts to help the training
process (faster convergence to better solutions), and
we even nd that it regularizes, giving rise to lower
generalization error for the same training error. This
is like in the case of unsupervised pre-training (Erhan
et al., 2009), and again it remains to be claried whyone would expect improved generalization, for both
curriculum and unsupervised pre-training procedures.
The way we have dened curriculum strategies leaves
a lot to be dened by the teacher. It would be nice
to understand general principles that make some cur-
riculum strategies work better than others, and this
clearly should be the subject of future work on curricu-
lum learning. In particular, to reap the advantages of
a curriculum strategy while minimizing the amount of
human (teacher) e
ort involved, it is natural to con-
sider a form of active selection of examples similar to
what humans (and in particular children) do. At any
point during the \education" of a learner, some exam-
ples can be considered \too easy" (not helping much
to improve the current model), while some examples
can be considered \too dicult" (no small change in
the model would allow to capture these examples). It
would be advantageous for a learner to focus on \in-
teresting" examples, which would be standing near the
frontier of the learner's knowledge and abilities, nei-
ther too easy nor too hard. Such an approach could be
used to at least automate the pace at which a learner
would move along a predened curriculum. In the ex-
periments we performed, that pace was xed arbitrar-
ily. This kind of strategy is clearly connected to active
learning (Cohn et al., 1995), but with a view that is
di
erent from the standard one: instead of focusing on
the examples near the decision surface to quickly infer
its location, we think of the set of examples that the
learner succeeds to capture and gradually expand that
set by preferentially adding examples near its border.
Curriculum learning is related to boosting algorithms,
in that dicult examples are gradually emphasized.
However, a curriculum starts with a focus on the eas-
ier examples, rather than a uniform distribution over
the training set. Furthermore, from the point of view
of the boosted weighted sum of weak learners, there is
no change in the training criterion: the change is only
from the point of view of the next weak learner. As
far as the boosted sum is concerned, we are following a
functional gradient on the same training criterion (the
sum of exponentiated margins). Curriculum strategies
are also connected to transfer (or multi-task) learning
and lifelong learning (Thrun, 1996). Curriculum learn-
ing strategies can be seen as a special form of trans-
fer learning where the initial tasks are used to guide
the learner so that it will perform better on the nal
task. Whereas the traditional motivation for multi-
task learning is to improve generalization by sharing
across tasks , curriculum learning adds the notion of
guiding the optimization process , either to converge
faster, or more importantly, to guide the learner to-
wards better local minima .Curriculum Learning
Acknowledgements: The authors thank NSERC,
CIFAR, and MITACS for support.
References
Allgower, E. L., & Georg, K. (1980). Numerical contin-
uation methods. An introduction . Springer-Verlag.
Bengio, Y. (2009). Learning deep architectures for AI.
Foundations & Trends in Mach. Learn. ,to appear .
Bengio, Y., Ducharme, R., & Vincent, P. (2001). A
neural probabilistic language model. Adv. Neural
Inf. Proc. Sys. 13 (pp. 932{938).
Bengio, Y., Lamblin, P., Popovici, D., & Larochelle, H.
(2007). Greedy layer-wise training of deep networks.
Adv. Neural Inf. Proc. Sys. 19 (pp. 153{160).
Cohn, D., Ghahramani, Z., & Jordan, M. (1995). Ac-
tive learning with statistical models. Adv. Neural
Inf. Proc. Sys. 7 (pp. 705{712).
Coleman, T., & Wu, Z. (1994). Parallel continuation-
based global optimization for molecular conforma-
tion and protein folding (Technical Report). Cornell
University, Dept. of Computer Science.
Collobert, R., & Weston, J. (2008). A unied archi-
tecture for natural language processing: Deep neural
networks with multitask learning. Int. Conf. Mach.
Learn. 2008 (pp. 160{167).
Der enyi, I., Geszti, T., & Gy orgyi, G. (1994). Gener-
alization in the programed teaching of a perceptron.
Physical Review E ,50, 3192{3200.
Elman, J. L. (1993). Learning and development in
neural networks: The importance of starting small.
Cognition ,48, 781{799.
Erhan, D., Manzagol, P.-A., Bengio, Y., Bengio, S.,
& Vincent, P. (2009). The diculty of training
deep architectures and the e
ect of unsupervised
pre-training. AI & Stat.'2009 .
Freund, Y., & Haussler, D. (1994). Unsupervised
learning of distributions on binary vectors using two
layer networks (Technical Report UCSC-CRL-94-
25). University of California, Santa Cruz.
H astad, J., & Goldmann, M. (1991). On the power of
small-depth threshold circuits. Computational Com-
plexity ,1, 113{129.
Hinton, G. E., Osindero, S., & Teh, Y.-W. (2006). A
fast learning algorithm for deep belief nets. Neural
Computation ,18, 1527{1554.
Hinton, G. E., & Salakhutdinov, R. (2006). Reduc-
ing the dimensionality of data with neural networks.
Science ,313, 504{507.
Krueger, K. A., & Dayan, P. (2009). Flexible shaping:
how learning in small steps helps. Cognition ,110,
380{394.Larochelle, H., Erhan, D., Courville, A., Bergstra, J.,
& Bengio, Y. (2007). An empirical evaluation of
deep architectures on problems with many factors
of variation. Int. Conf. Mach. Learn. (pp. 473{480).
Peterson, G. B. (2004). A day of great illumination:
B. F. Skinner's discovery of shaping. Journal of the
Experimental Analysis of Behavior ,82, 317{328.
Ranzato, M., Boureau, Y., & LeCun, Y. (2008). Sparse
feature learning for deep belief networks. Adv. Neu-
ral Inf. Proc. Sys. 20 (pp. 1185{1192).
Ranzato, M., Poultney, C., Chopra, S., & LeCun, Y.
(2007). Ecient learning of sparse representations
with an energy-based model. Adv. Neural Inf. Proc.
Sys. 19 (pp. 1137{1144).
Rohde, D., & Plaut, D. (1999). Language acquisition
in the absence of explicit negative evidence: How
important is starting small? Cognition ,72, 67{109.
Salakhutdinov, R., & Hinton, G. (2007). Learning a
nonlinear embedding by preserving class neighbour-
hood structure. AI & Stat.'2007 .
Salakhutdinov, R., & Hinton, G. (2008). Using Deep
Belief Nets to learn covariance kernels for Gaussian
processes. Adv. Neural Inf. Proc. Sys. 20 (pp. 1249{
1256).
Salakhutdinov, R., Mnih, A., & Hinton, G. (2007). Re-
stricted Boltzmann machines for collaborative lter-
ing. Int. Conf. Mach. Learn. 2007 (pp. 791{798).
Sanger, T. D. (1994). Neural network learning con-
trol of robot manipulators using gradually increas-
ing task diculty. IEEE Trans. on Robotics and
Automation ,10.
Schwenk, H., & Gauvain, J.-L. (2002). Connection-
ist language modeling for large vocabulary continu-
ous speech recognition. International Conference on
Acoustics, Speech and Signal Processing (pp. 765{
768). Orlando, Florida.
Skinner, B. F. (1958). Reinforcement today. American
Psychologist ,13, 94{99.
Thrun, S. (1996). Explanation-based neural network
learning: A lifelong learning approach . Boston, MA:
Kluwer Academic Publishers.
Vincent, P., Larochelle, H., Bengio, Y., & Manzagol,
P.-A. (2008). Extracting and composing robust fea-
tures with denoising autoencoders. Int. Conf. Mach.
Learn. (pp. 1096{1103).
Weston, J., Ratle, F., & Collobert, R. (2008). Deep
learning via semi-supervised embedding. Int. Conf.
Mach. Learn. 2008 (pp. 1168{1175).
Wu, Z. (1997). Global continuation for distance geom-
etry problems. SIAM Journal of Optimization ,7,
814{836.