Group Lasso with Overlap and Graph Lasso

Laurent Jacob LAURENT .JACOB@MINES-PARISTECH .FR
Mines ParisTech – CBIO, INSERM U900, InstitutCurie, 26rue d ’Ulm,Paris cedex 05, F-75248 France
Guillaume Obozinski1GOBO@STAT.BERKELEY .EDU
INRIA –Willow Project-Team, Ecole Normale Supérieure, 45r ue d’Ulm,Paris cedex 05, F-75230 France
Jean-Philippe Vert JEAN-PHILIPPE .VERT@MINES-PARISTECH .FR
Mines ParisTech – CBIO, INSERM U900, InstitutCurie, 26rue d ’Ulm,Paris cedex 05, F-75248 France
Abstract
We propose a new penalty function which, when
used as regularization for empirical risk mini-
mization procedures, leads to sparse estimators.
The support of the sparse vector is typically a
union of potentially overlapping groups of co-
variates deﬁned a priori, or a set of covariates
which tend to be connected to each other when
a graph of covariates is given. We study theo-
retical properties of the estimator, and illustrate
its behavior on simulated and breast cancer gene
expression data.
1.Introduction
Estimation of sparse linear models by the minimization of
an empirical error penalized by a regularization term is
a very popular and successful approach in statistics and
machine learning. Controlling the trade-off between data
ﬁtting and regularization, one can obtain estimators with
good statistical properties, even in very large dimension.
Moreover, sparse classiﬁers lend themselves particularly
welltointerpretation,whichisoftenofprimaryimportanc e
in many applications such as biology or social sciences. A
popular example is the penalization of a ℓ2criterion by the
ℓ1normoftheestimator,knownas lasso(Tibshirani,1996)
orbasispursuit (Chenet al.,1998). Interestingly,thelasso
is able to recover the exact support of a sparse model from
data generated by this model if the covariates are not too
correlated (Zhao & Yu, 2006; Wainwright, 2006).
1This work was undertaken while Guillaume Obozinski was
afﬁliated withUC Berkeley, Department of Statistics.
Appearing in Proceedings of the 26thInternational Conference
on Machine Learning , Montreal, Canada, 2009. Copyright 2009
bythe author(s)/owner(s).While the ℓ1norm penalty leads to sparse models, it does
not contain any prior information about, e.g., possible
groups of covariates that one may wish to see selected
jointly. Several authors have recently proposed new penal-
ties to enforce the estimation of models with speciﬁc spar-
sity patterns. For example, when the covariates are parti-
tioned into groups, the group lasso leads to the selection
of groups of covariates (Yuan & Lin, 2006). The group
lasso penalty for a model, also called ℓ1/ℓ2penalty, is the
sum(i.e.,ℓ1norm)ofthe ℓ2normsoftherestrictionsofthe
model to the different groups of covariates. It recovers the
support of a model if the support is a union of groups and
if covariates of different groups are not too correlated. It
canbegeneralizedtoaninﬁnite-dimensionalsetting(Bach ,
2008). Othervariantsofthegrouplassoincludejointselec -
tionofcovariatesformulti-tasklearning(Obozinskiet al .,
2009) andpenalties toenforcehierarchical selectionofco -
variates,e.g., when one has a hierarchy over the covariates
and wants to select covariates only if their ancestors in the
hierarchyarealsoselected(Zhaoet al.,2009;Bach,2009).
In this paper we are interested in a more general situation.
We assume that either (i) groups of covariates are given,
potentially with overlap between the groups, and we wish
to estimate a model whose support is a union of groups, or
(ii)thatagraphwithcovariatesasverticesisgiven,andwe
wish to estimate a model whose support contains covari-
ateswhichtendtobeconnectedtoeachothersonthegraph.
Althoughquitegeneral,thisframeworkismotivatedinpar-
ticular by applications in bioinformatics, when we have to
solve classiﬁcation or regression problems with few sam-
ples in high dimension, such as predicting the class of a
tumour from gene expression measurements with microar-
rays, and simultaneously select a few genes to establish a
predictive signature (Roth, 2002). Selecting a few genes
that either belong to the same functional groups, where
the groups are given a priori and may overlap, or tend to
be connected to each other in a given biological network,Group Lassowith Overlap and Graph Lasso
couldthenleadtoincreasedinterpretabilityofthesignat ure
and potential better performances (Rapaport et al., 2007).
To reach this goal, we propose and study a new penalty
whichgeneralizesthe ℓ1/ℓ2normtooverlappinggroupsfor
the ﬁrst case, and propose to cast the problem of selecting
connectedcovariatesinagraphastheproblemofselecting
a union of overlapping groups, with adequate deﬁnition of
groups,forthesecondcase. Wementionvariousproperties
of this penalty, and provide conditions for the consistency
of support estimation in the regression setting. Finally, w e
report promising resultsonboth simulated and real data.
2.Problemand notations
Foranyvector w∈Rp,/ba∇dblw/ba∇dbldenotestheEuclideannormof
w, and supp (w)⊂[1,p]denotes the support of w,i.e., the
setofcovariates i∈[1,p]suchthat wi/\e}atio\slash= 0. Agroupofco-
variatesisasubset g⊂[1,p]. Thesetofallpossiblegroups
is therefore P([1,p]), the power set of [1,p]. Throughout
the paper, G ⊂ P ([1,p])denotes a set of groups, usually
ﬁxed in advance for each application. We say that two
groups overlap if they have at least one covariate in com-
mon. For any vector w∈Rp, and any group g∈ G, we
denote wg∈Rpthe vector whose entries are the same as
wfor the covariates in g, and are 0for other other covari-
ates. However, we use a different convention for elements
ofVG⊂Rp×Gthesetof |G|-tuplesofvectors v= (vg)g∈G,
where each vgis this time a separate vector in Rp, which
satisﬁes supp (vg)⊂gfor each g∈ G. For any differen-
tiable function f:Rp→R, we denote by ∇f(w)∈Rp
the gradient of fatw∈Rpand by ∇gf(w)∈Rgthe
partial gradient of fwithrespect totothe covariates in g.
3.Group lassowith overlappinggroups
When the groups in Gdo not overlap, the group lasso
penalty (Yuan & Lin, 2006) isdeﬁned as:
∀w∈Rp,ΩG
group(w) =/summationdisplay
g∈G/ba∇dblwg/ba∇dbl.(1)
When the groups in Gform a partition of the set of covari-
ates, then ΩG
group(w)is a norm whose balls have singulari-
tieswhensome wgareequaltozero. Minimizingasmooth
convex risk functional over such a ball often leads to a so-
lution that lies on a singularity, i.e., to a vector wsuch that
wg= 0forsome ofthe ginG.
When some of the groups in Goverlap, the penalty (1)
is still a norm (if all covariates are in at least one group)
whose ball has singularities when some wgare equal to
zero. Indeed, for a vector w, if we denote by G0⊂ Gthe
set ofgroups such that wg= 0, then
supp(w)⊂/parenleftBig/uniontext
g∈G0g/parenrightBigc
.
Figure1. Balls for ΩG
group(·)(left) and ΩG
overlap(·)(right) for the
groups G={{1,2},{2,3}}where w2is represented as the ver-
tical coordinate.
We see that this penalty induces the estimation of sparse
vectors, whose support in typically the complement of
a union of groups. Although this may be relevant for
some applications, with appropriately designed families o f
groups — as considered by (Jenatton et al., 2009) — , we
are interested in this paper in penalties which induce the
oppositeeffect: that thesupport of wbeaunionofgroups.
Forthatpurpose,weproposeinsteadthefollowingpenalty:
ΩG
overlap(w) = inf
v∈VG,P
g∈Gvg=w/summationdisplay
g∈G/ba∇dblvg/ba∇dbl.(2)
When the groups do not overlap and form a partition of
[1,p], there exists a unique decomposition of w∈Rpas
w=/summationtext
g∈Gvgwith supp (vg)⊂g, namely, vg=wgfor
allg∈ G. In that case, both penalties (1) and (2) are the
same. Ifsomegroupsoverlap,thenweshowbelowthatthis
penalty induces the selection of wthat can be decomposed
asw=/summationtext
g∈Gvgwhere some vgare equal to 0. If we
denoteby G1⊂ Gthesetofgroups gwithvg/\e}atio\slash= 0,thenwe
immediately get w=/summationtext
g∈G1vg,and therefore:
supp(w)⊂/uniontext
g∈G1g .
In other words, the penalty (2) leads to sparse solutions
whose support is typically a union of groups, matching
the setting of applications that motivate this work. In the
rest of this paper, we therefore investigate in more details
ΩG
overlap(.), both theoretically and empirically.
Figure 1 shows the ball for both norms in R3with groups
G={{1,2},{2,3}}. The pillow shaped ball of ΩG
group(≤)
has four singularities corresponding to cases where either
onlyw1or only w3is non-zero. By contrast, ΩG
overlap(≤)
hastwocircularsetsofsingularitiescorrespondingtocas es
where (w1,w2)only or (w2,w3)onlyisnon zero.
4.Somepropertiesof ΩG
overlap(.)
We ﬁrst analyze the decomposition of a vector w∈Rpas/summationtext
g∈Gvginducedby(2). Forthatpurpose,let V(w)⊂ VGGroup Lassowith Overlap and Graph Lasso
bethesetof |G|-tuplesofvectors v= (vg)g∈Gwhichreach
theminimum in(2), i.e., which satisfy
w=/summationtext
g∈GvgandΩG
overlap(w) =/summationtext
g∈G/ba∇dblvg/ba∇dbl.
Theoptimizationproblem(2)deﬁning ΩG
overlap(w)isacon-
vex problem and its objective is coercive, so that the set of
solutions V(w)isnon-empty and convex. Moreover,
Lemma 1. w/ma√sto→ΩG
overlap(w)isanorm.
Proof.Positivehomogeneityandpositivedeﬁnitenesshold
trivially. We show the triangular inequality. Consider
w,w′∈Rp; let(vg)g∈Gand(v′
g)g∈Gbe respectively op-
timal decompositions of wandw′so that ΩG
overlap(w) =/summationtext
g/ba∇dblvg/ba∇dblandΩG
overlap(w′) =/summationtext
g/ba∇dblv′
g/ba∇dbl. Since (vg+v′
g)g∈G
is a (a priori non-optimal) decomposition of w+w′, we
clearly have ΩG
overlap(w+w′)≤/summationtext
g∈G/ba∇dblvg+v′
g/ba∇dbl ≤/summationtext
g(/ba∇dblvg/ba∇dbl+/ba∇dblv′
g/ba∇dbl) = ΩG
overlap(w) + ΩG
overlap(w′).
Usingtheconicdualof(2),wegiveanotherformulationof
thenorm ΩG
overlap(.)yelding someimportant properties.
Lemma 2. 1. Itholds that:
ΩG
overlap(w) = supα∈Rp:∀g∈G,/bardblαg/bardbl≤1α⊤w .(3)
2. Avector α∈Rpisasolutionof (3)ifandonlyifthere
existsv= (vg)g∈G∈V(w)such that:
∀g∈ G,ifvg/\e}atio\slash= 0, αg=vg
/bardblvg/bardblelse/ba∇dblαg/ba∇dbl ≤1(4)
3. Conversely, a G-tuple of vectors v= (vg)g∈G∈ VG
suchthat w=/summationtext
gvgisasolutionto (2)ifandonlyif
there existsavector α∈Rpsuch that (4)holds.
Proof.Let us introduce slack variables t= (tg)g∈G∈RG
and rewritethe optimization problem (2) asfollows:
min
t∈RG,v∈VG/summationdisplay
g∈Gtgs.t./summationdisplay
g∈Gvg=wand∀g∈ G,/ba∇dblvg/ba∇dbl ≤tg.
We can form a Lagrangian for this problem with the dual
variables α∈Rpfor the constraint/summationtext
g∈Gvg=w, and
(β,γ)∈ VG×RGwith/ba∇dblβg/ba∇dbl ≤γgfortheconicconstraints
/ba∇dblvg/ba∇dbl ≤tg, and get:
L=/summationdisplay
g∈Gtg+α⊤/parenleftBig
w−/summationdisplay
g∈Gvg/parenrightBig
−/summationdisplay
g∈G/parenleftbig
β⊤
gvg+γgtg/parenrightbig
.
The minimum of Lwith respect to the primal variables t
andvis non trivial only if γg= 1andαg=−βgfor any
g∈ G. Therefore, weget thedual function:
min
t,vL=/braceleftBigg
α⊤wifγg= 1andαg=−βgfor all g∈ G,
−∞otherwise.Bystrongduality(since, e.g.,Slater’sconditionisfulﬁlled),
the optimal value ΩG
overlap(w)of the primal is equal to the
maximumofthedualproblem. Maximizingthisdualfunc-
tion over γg= 1,/ba∇dblβg/ba∇dbl ≤γgandαg=−βgis equivalent
to maximizing α⊤wover the vectors α∈Rpsuch that
/ba∇dblαg/ba∇dbl ≤1for all g∈ G, which proves (3). To prove the
second point, we note that the variables (t,v,α,β,γ )are
primal/dual optimal for this convex optimization problem
if and only if the Karush-Kuhn-Tucker (KKT) conditions
aresatisﬁed, i.e., ifand only if,forall g∈ G:


supp(vg) =g,/ba∇dblvg/ba∇dbl ≤tgandw=/summationtext
g∈Gvg
supp(βg) =g,/ba∇dblβg/ba∇dbl ≤γg
αg=−βgandγg= 1
β⊤
gvg+γgtg= 0
Eliminating βandγwith the stationarity conditions, all
conditionsarefulﬁlledifandonlyif w=/summationtext
g∈Gvgandfor
allg∈ G, (i) either vg= 0and/ba∇dblαg/ba∇dbl ≤1, (ii) or vg/\e}atio\slash= 0
andαg=vg//ba∇dblvg/ba∇dbl. Ifapair (α,v)fulﬁllstheseconditions,
thenweobtainaprimal/dualsolutionbytaking tg=/ba∇dblvg/ba∇dbl,
βg=−αgandγg= 1. Thisproves points 2and3.
Denoteby G1thegroup-supportof w,i.e.,thesetofgroups
belonging to the support of at least one optimal decompo-
sition of w:G1={g∈ G | ∃ v= (vg)g∈V(w), vg/\e}atio\slash= 0}
andJ1the corresponding set of variables J1=∪g∈G1g.
Lemma 3. Letαbe an optimum in the formulation (3)of
theΩG
overlap(≤)norm, then αJ1isuniquely deﬁned.
Proof.Consideranysolution v= (vg)g∈Gof(2). Let αbe
anyoptimalsolutionof(3). Since (v,α)formaprimal/dual
pair, they must satisfy the KKT conditions. In particular,
for all gsuch that vg/\e}atio\slash= 0,αgis deﬁned uniquely by αg=
vg
/bardblvg/bardbl. Since this is true for all solutions v∈V(w),αJ1is
uniquely deﬁned.
Corollary 1. For any v,v′∈V(w)and for any g∈ G,
/ba∇dblvg/ba∇dbl ×/vextenddouble/vextenddoublev′
g/vextenddouble/vextenddouble= 0or∃γg≥0s.t.v′
g=γvg.(5)
Proof.Ifvg/\e}atio\slash= 0andv′
g/\e}atio\slash= 0,letαbesolutionof(3),bythe
previous lemma αgisunique and αg=vg
/bardblvg/bardbl=v′
g
/bardblv′
g/bardbl.
5.Using ΩG
overlap(.)as apenalty
We now consider a learning scenario where we use
ΩG
overlap(w)as a regularization term to the minimization of
anobjectivefunction R(w),typicallyanempiricalrisk. We
assume that R(w)is convex and differentiable in w, and
consider the optimization problem:
minw∈RpR(w) +λΩG
overlap(w), (6)Group Lassowith Overlap and Graph Lasso
where λ >0is a regularization parameter. We ﬁrst de-
rive optimality conditions for any solution of (6). For that
purpose, let us denote AG(w)the set of vectors α∈Rp
solutionof (3).
Lemma4. Avector w∈Rpisasolutionof (6)ifandonly
if−∇R(w)/λ∈ AG(w).
Proof.The proof follows from the same Lagrangian based
derivation as forLemma 2, adding only thelossterm.
Remark1. Bypoint 2ofLemma2,anequivalentformula-
tionisthefollowing: avector w∈Rpisasolutionof (6)if
and only if it can be decomposed as w=/summationtext
g∈Gvgwhere,
for any g∈ G,vg∈Rp, supp (vg) =g, and if vg= 0then
/ba∇dbl∇gR(w)/ba∇dbl ≤λ,and∇gR(w) =−λvg//ba∇dblvg/ba∇dblotherwise.
6.Consistency
Before we present a consistency result on ΩG
overlap(.), we
willneed thefollowing lemma.
Lemma 5. Assume that for all w′in a small neighbor-
hoodUofw,w′admits a unique decomposition (v′
g)g∈G
of minimal norm supported by the same set of groups G1
asw. Writing ηg=/ba∇dblvg/ba∇dbl, there exists a neighborhood U0
ofwJ1inR|J1|and a neighborhood U′
0of(αJ1,ηG1)in
R|J1|×|G1|such that there exists a unique continuous func-
tionφ:wJ1/ma√sto→(αJ1(w),ηG1(w))fromU0toU′
0.
Proof.The dual problem (3) is equivalent to the saddle-
point problem minαmax ηL′(α,η,w )s.t.ηg∈R+with
lagrangian L′(α,η,w ) =−α⊤w+/summationtext
g∈Gηg
2(/ba∇dblαg/ba∇dbl2−1)
and KKT conditions:

∀g∈ G,/ba∇dblαg/ba∇dbl2≤1, (primal feas.)
∀g∈ G,ηg≥0, (dual feas.)
∀i∈[1,p],−wi+/parenleftBig/summationtext
g∋iηg/parenrightBig
αi= 0,(stationarity)
∀g∈ G,ηg(/ba∇dblαg/ba∇dbl2−1) = 0 , (comp.slack.)
Bystationarity, (vg)g∈Gdeﬁnedby vg=ηgαgisadecom-
positionof w;itisoptimalbecauseitsatisﬁesproperty3of
lemma 2; ﬁnally we have ηg=/ba∇dblvg/ba∇dblconsistently with our
deﬁnitionof ηg(w). Forany wwiththesamesetofsupport-
ing groups G1, we have /ba∇dblαg(w)/ba∇dbl= 1for all g∈ G1and
ηg= 0forall g∈ G\G 1. Forall wJ1withgroup-supportno
smaller than G1, the corresponding pair (αJ1(w),ηG1(w))
istherefore asolutionof the setof non-linear equations:
/braceleftBigg
∀i∈J1,−wi+/parenleftBig/summationtext
g∋iηg/parenrightBig
αi= 0
∀g∈ G1,/ba∇dblαg/ba∇dbl2−1 = 0(7)
Inother words consider thefunction
F:R|J1|×|J1|×|G1|→R|J1|×|G1|
(wJ1,αJ1,ηG1)/ma√sto→/parenleftBigg/parenleftBig
−wi+/bracketleftBig/summationtext
g∋iηg/bracketrightBig
αi/parenrightBig
i∈J1
(/ba∇dblαg/ba∇dbl2−1)g∈G1/parenrightBigg
,then (7) is equivalent to F(wJ1,αJ1,ηG1) = 0. We use the
implicitfunctiontheoremfornon-differentiablefunctio nof
(Kumagai, 1980). The theorem states that for a continuous
function F:R|J1|×R|J1|×|G1|→R|J1|×|G1|such that
F(w0,(α0,η0)) = 0, if there exist open neighborhoods
U⊂R|J1|andU′⊂R|J1|×|G1|ofw0and(α0,η0)respec-
tively, such that, for all w∈U,F(w,≤) :U′→R|J1|×|G1|
is locally one-to-one then there exist open neighborhoods
U0⊂R|J1|andU′
0⊂R|J1|×|G1|ofw0and(α0,η0), such
that, for all w∈U0, the equation F(w,(α,η)) = 0has a
unique solution (α,η) =φ(w)∈U′
0, where φis a con-
tinuous function from U0intoU′
0. By continuity of the
addition, the product and the Euclidean norm, the above
deﬁned Fiscontinuous. Foreach wﬁxed,F(w,≤)isbijec-
tive,becauseoftheassumptionoftheexistenceofaunique
decompositioninaneighborhoodof w. Applyingthetheo-
rem of (Kumagai, 1980) then yields the desiredresult.
We are now ready to prove the consistency of ΩG
overlap(.).
Considerthelinearregressionmodel Y=X¯w+ǫ,where
X∈Rn×pis a design matrix, Y∈Rpis the response
vectorand ǫ∈Rpisavectorofi.i.d. randomvariableswith
mean0and ﬁnite variance. We denote the true regression
function by ¯w. We assumethat
1. (H1) Σ :=1
nX⊤X≻0
2. (H2) There exists a neighborhood of ¯win which (2)
has aunique solution.
IfG1is the set of group supporting the unique solution of
(2), we denote G2∆=G\G1andJ2∆= [1,p]\J1. For con-
venience, for any group of covariates gwe note Xgthe
n× |g|design matrix restricted to the predictors in g, and
for any two groups g,g′we note Σgg′=X⊤
gXg′. We can
thenprovideaconditionunderwhichminimizingtheleast-
square error penalized by ΩG
overlap(w)leads to an estimator
withthecorrect support. Consider thetwoconditions:
∀g∈ G2,/ba∇dblΣgJ1Σ−1
J1J1αJ1( ¯w)/ba∇dbl ≤1(C1)
∀g∈ G2,/ba∇dblΣgJ1Σ−1
J1J1αJ1( ¯w)/ba∇dbl<1(C2)
Lemma 6. With assumptions (H1-2), for λn→0and
λnn1/2→ ∞, conditions (C1)and(C2)are respectively
necessary and sufﬁcient for the solution of (6)to estimate
consistently thegroup-support of ¯w.
Proof.We follow the line of proof of (Bach, 2008) but
consider a ﬁxed design for simplicity of notations. Let
us ﬁrst consider the subproblem of estimating a vector
only on the support of ¯wby using only the groups in
J1in the penalty, i.e., consider w1∈RJ1a solution ofGroup Lassowith Overlap and Graph Lasso
minwJ1∈RJ11
2n/ba∇dblY−XJ1wJ1/ba∇dbl2+λnΩG1
overlap(wJ1).By
standard arguments, we can prove that w1converges in
Euclidean norm to ¯wrestricted to J1asntends to in-
ﬁnity (Fu & Knight, 2000). In the rest of the proof we
show how to construct a vector w∈Rpfromw1which
under condition (C2) is with high probability a solution
to (6). By adding null components to w1, we obtain a vec-
torw∈Rpwhose support is also J1, andu=w−¯w
therefore satisﬁes supp (u)⊂J1. A direct computation
of the gradient of the risk R(w) =/ba∇dblY−Xw/ba∇dbl2gives
∇R(w) = Σ u−W, where W=1
nXǫ. From this
we deduce that u= Σ−1
J1J1(∇J1R(w) +WJ1), and since
∇J1R(w) =−λnαJ1(w)we have :
∇J2R(w) = Σ J2J1Σ−1
J1J1(WJ1−λnαJ1(w))−WJ2.
To show that wis a feasible solution to (6) it is enough to
show that ∀g∈ G2,/ba∇dbl∇gR(w)/ba∇dbl ≤λn. Moreover, since
thenoisehasboundedvariance, ΣJ2J1Σ−1
J1J1WJ1−WJ2=
X⊤
J2/bracketleftbig1
nXJ1Σ−1
J1J1X⊤
J1−I/bracketrightbig
ǫis√n-consistent and
1
λn/ba∇dbl∇gR(w)/ba∇dbl ≤ /ba∇dblΣgJ1Σ−1
J1J1αJ1(w)/ba∇dbl+Op(λ−1
nn−1/2).
By Lemma 5, we have that αJ1is a continuous function
ofwin a neighborhood of ¯wso that wJ1P→¯wJ1im-
pliesαJ1(w)P→αJ1( ¯w). Since we chose λnsuch that
λ−1
nn−1/2→0, wehave
1
λn/ba∇dbl∇gR(w)/ba∇dbl ≤ /ba∇dblΣgJ1Σ−1
J1J1αJ1( ¯w)/ba∇dbl+op(1).
Hence the result for the sufﬁcient condition. Symmetri-
cally, for thenecessary condition wehave
1
λn/ba∇dbl∇gR(w)/ba∇dbl ≥ /ba∇dblΣgJ1Σ−1
J1J1αJ1( ¯w)/ba∇dbl −op(1).
7.Graph lasso
We now consider the situation where we have a simple
undirectedgraph (I,E),wherethesetofvertices I= [1,k]
is the set of covariates and E⊂I×Iis a set of edges
that connect covariates. We suppose that we wish to es-
timate a sparse model such that selected covariates tend
to be connected to each other, i.e., form a limited num-
ber of connected components on the graph. An obvious
approach is to consider the prior ΩG
overlap(.)where Gis
a set that generates by union the connected components.
For example, we may consider for Gthe set of edges,
cliques, or small linear subgraphs. As an example, con-
sidering all edges, i.e.,G=Eleads to Ωgraph(w) =
minv∈VE/summationtext
e∈E/ba∇dblve/ba∇dbls.t./summationtext
e∈Eve=w,supp(ve) =e.
Alternatively, we will consider in the experiments the set
of all linear subgraphs of length k≥1. Although we haveno formal statement on how to chose k, it intuitively con-
trols the size of the groups of connected variables which
are selected, and should therefore be typically chosen to
be slightly smaller than the size of the minimal connected
component expected inthe support of the model.
8.Implementation
A simple way to implement empirical risk minimization
using ΩG
overlap(.)as the regularizer is to explicitly dupli-
cate the variables in the design matrix, i.e., to replace
X∈Rn×pby˜X∈Rn×P|g|deﬁned by the concatena-
tion of copies of the design matrix restricted each to a
certain group g, i.e., ˜X= [Xg1,Xg2,...,X g|G|], where
G={g1,... ,g G}. To see this, denote ˜vg= (vgi)i∈gand
˜v= (˜v⊤
g1,... ,˜v⊤
g|G|)⊤, and consider that, for an empiri-
cal risk of the form R(w) =˜R(Xw), we can eliminate w
from(6)toget R(w) =˜R(X(/summationtext
gvg)) =˜R(˜X˜v)andthus
for the full objective : ˜R(˜X˜v) +λ/summationtext
g/ba∇dbl˜vg/ba∇dbl.That way the
vector ˜v∈RP|g|can be directly estimated from ˜Xwith a
classical group lasso for non-overlapping groups. We im-
plemented theapproach of(Meieret al.,2008) toestimate
the group lasso in the expanded space. Note that (Roth
& Fischer, 2008) provides a faster algorithm for the group
Lasso. When there are many groups with important over-
laphowever,analternativeimplementationwithoutexplic it
data duplication, e.g., with a variational formulation simi-
lar to the one of (Rakotomamonjy et al., 2008) might be
more scalable.
9.Experiments
9.1. Synthetic data: given overlapping groups
To assess the performance of our method when overlap-
ping groups are given as a priori, we simulated data with
p= 82variables, covered by 10groups of 10variables
with2variablesofoverlapbetweentwosuccessivegroups:
{1,... ,10},{9,... ,18},... ,{73,... ,82}. We chose the
supportof wtobetheunionofgroups 4and5andsampled
boththesupportweightsandtheoffsetfromi.i.d. Gaussian
variables. Note that in this setting, the support can be ex-
pressedasaunionofgroups,butnotasthecomplementofa
union. Therefore, ΩG
overlap(.)can recover the right support,
whereas by construction ΩG
group(≤)using the same groups
would be unable torecover it.
Themodelislearnedfrom ndatapoints (xi,yi),withyi=
w⊤xi+ε,ε∼ N(0,σ2),σ=|E(Xw+b)|. Using an ℓ2
lossR(w) =/ba∇dblY−Xw−b/ba∇dbl2, we learn models from 50
such training sets. On Figure 2, for each variable (on the
verticalaxis),weplotitsfrequencyofselectioninlevels of
gray as a function of the regularization parameter λ, both
forthe lassopenalty and ΩG
overlap(.).Group Lassowith Overlap and Graph Lasso
log2(λ)20
40
60
80
log2(λ)20
40
60
80
log2(λ)20
40
60
80
log2(λ)20
40
60
80
Figure2. Frequency of selection of each variable with the lasso
(left) and ΩG
overlap(.)(right)for n= 50(top)and 100(bottom).
For any choice of λthe lasso frequently misses some vari-
ables from the support, while ΩG
overlap(.)never misses any
variable from the support for a large part of the regulariza-
tionpath. Besides,weobservedthatoverthereplicates,th e
lasso never selected the exact correct pattern for n <100.
Forn= 100, the right pattern was selected with low fre-
quencyonasmallpartoftheregularizationpath. ΩG
overlap(.)
on the other hand selected it up to 92%of the times for
n= 50and more than 99%on more than one third of the
pathfor n= 100. Wetriedthesameexperimentforvarious
nandaslongas nwastoosmallforthelassotorecoverthe
right support, the group regularization always helped.
1 1.5 20246810
log10(n)RMSE
  
overlapping
lasso
Figure3. Root mean squared error of overlapped group lasso and
lassoas afunction ofthe number of trainingpoints.
Figure 3 shows the root mean squared error of both meth-
ods for various n. For both methods, the full regulariza-
tion path is computed and tested on three replicates of n
training and 100testing points. The best average parame-
terisselectedandusedtotrainandtestamodelonafourth
replicate. On a large range of n,ΩG
overlap(.), not only helps
to recover the right pattern, improves the regression per-
formance. A possible explanation is that if several vari-
ables from the support are correlated in the design matrixX, the lasso selects one and is less robust than ΩG
overlap(.)
which uses all the variables. Note that when enough train-
ing points become available (last point on Figure 3), Fig-
ure 2 shows that the selected model is generally better but
still not correct whereas ΩG
overlap(.)selects the right model,
even if itdoes not give much lower erroranymore.
9.2. Synthetic data: given linear graph structure
We now consider that the prior given on the variables is
a graph structure and that we are interested by solutions
which are connected components on this graph. As a ﬁrst
simple illustration, we consider a chain. We use w∈Rp,
p= 100, supp (w) = [20 ,40]. The nodes of the graph are
the variables wi, the edges are all the pairs (wi,wi+1),i=
1,... ,n. The model’s weights, offset and the 50training
examples (x,y)are drawn using the same protocol as in
the previous experiment. We take for the groups all the
sub-chains of length k. We present the results for various
choices of kand compare tothe lasso( k= 1).
log2(λ)20
40
60
80
100
log2(λ)20
40
60
80
100
log2(λ)20
40
60
80
100
log2(λ)20
40
60
80
100
Figure4. Variable selection frequency with ΩG
overlap(.)using the
chains oflength k(left)as groups,for k= 1,2,4,8.
Figure 4 shows the frequency of each variable selection
over20replications. Here again, using a group prior helps
the pattern recovery. We also observe as expected that the
choice of kplays aroleinthe improvement.
9.3. Synthetic data: given non-linear graph structure
Here we consider the same setting as in the linear case,
except that instead of a chain we are given a grid structure
on the variables. Each node is connected to the 4nodes
above, below, left and right. The support is a 20-variable
region in the center of the grid, x-axis4to7,y-axis4to 8.
As groups, we use all the 4-cycles, which is a natural prior
given the graph topology and the expected pattern.Group Lassowith Overlap and Graph Lasso
Figure 5 shows the variable selection frequency of each
variableforbothmethodsataﬁxed λ(choseninbothcases
to give the best behavior). ΩG
overlap(.)seems to generally
give better selection performances thanlasso.
Besides, we observed that on each run, variables incor-
rectly selected where always unions of groups whereas the
lasso selected disconnected variables on the graph. We
madethesameobservationforthelineargraphcase. Thisis
an expected property of our method, and implies that even
ifvariableswhicharenotinthemodelareselected,theyen-
ter the model as large connected components, whereas the
falsepositiveofthelassoaremorerandomlydistributedon
thegraph, oftenasisolatedvariables. Thisisaninteresti ng
propertyforrealapplicationsbecauseitmaythenbeeasier
to discard manually a few large connected components of
false positives, than many isolated variables (assuming of
course that theright variables are selected as well).
2468102
4
6
8
10
2468102
4
6
8
10
Figure5. Grid view of the variable selection frequencies with the
graph setting. Left: lasso, right: ΩG
overlap(.)using 4-cycles as
groups. n= 30trainingpoints, λis arbitrarilyﬁxed.
9.4. Breast cancer data: pathway analysis
An important motivation for our method is the possibility
toperformgeneselectionfrommicroarraydatausingpriors
whichareoverlappinggroups. Forexample,onemaywant
to analyse microarrays in terms of biologically meaning-
ful gene sets. In most such analysis, genes discriminating
the classes ( e.g.tumors leading to metastasis versus non-
metastasis)areselectedinaﬁrststep,thenenrichmentana l-
ysisisperformedbylookingforgenesetsinwhichselected
genesareoverrepresented(Subramanianet al.,2005). Sev-
eral organizations of the genes into gene sets are available
in various databases. We use the canonical pathways from
MSigDB(Subramanianet al.,2005)containing 639groups
of genes, 637of which involve genes from our study.
We use the breast cancer dataset compiled by (Van de Vi-
jver et al., 2002), which consists of gene expression data
for8,141genesin 295breastcancertumors( 78metastatic
and217non-metastatic). We restrict the analysis to the
3510genes which are in at least one pathway. Since the
dataset is very unbalanced, we balance it by using 3repli-
cates of each metastasis patient (keeping all duplicates in
thesame foldduring cross-validation).Table 1.Classiﬁcation error, number and proportion of pathways
selected bythe ℓ1andΩG
overlap(.)onthe 3folds.
METHOD ℓ1 ΩG
OVERLAP (.)
ERROR 0.38±0.04 0 .36±0.03
♯PATH. 148,58,183 6 ,5,78
PROP.PATH.0.32,0.14,0.41 0 .01,0.01,0.17
We estimate by 3-fold cross validation the accuracy of a
logistic regression with ℓ1andΩG
overlap(.)penalties, using
the pathways as groups. As a pre-processing, we keep the
300genesmostcorrelatedwiththeoutput(oneachtraining
set).λisselected by crossvalidation on each training set.
Table 1 shows the results of both methods. Using
ΩG
overlap(.)instead of the ℓ1penalty leads to a slight
improvement in the prediction performances, and much
sparsersolutionsatthepathwaylevel,whichmakesthese-
lected model easier tointerpret.
9.5. Breast cancer data: graph analysis
Another important application in microarray data analysis
is the search for potential drug targets. In order to iden-
tify genes which are related to a disease, one would like
to ﬁnd groups of genes forming connected components on
a graph carrying biological information such as regulation ,
involvement in the same chain of metabolic reactions, or
protein-protein interaction. Similarly to what is done in
pathwayanalysis,(Chuanget al.,2007)builtanetworkby
compiling several biological networks and performed such
graph analysis by identifying discriminant subnetworks in
one step and using these subnetworks to learn a classiﬁer
in a separate step. We use this network and the approach
described in section 7, taking all the edges on the network
as the groups, on the breast cancer dataset. Here again,
we restrict the data to the 7910genes which are present
in the network, and use the same correlation-based pre-
processing as forthe pathway analysis.
Table 2 shows the results of the logistic regression with
ℓ1andΩG
overlap(.). Here again, both methods give similar
performances, with a slight advantage for ΩG
overlap(.). On
the other hand, while the ℓ1mostly selects disconnected
variables on the graph, ΩG
overlap(.)tends to select variables
whicharegroupedintolargerconnectedcomponentsonthe
graph. This would make the interpretation and the search
fornew drug targetseasier.
10. Discussion
We have presented a generalization of the group lasso
penalty, which leads to sparse models with sparsity pat-Group Lassowith Overlap and Graph Lasso
Table 2.Classiﬁcation error and average size of the connected
components selected bythe ℓ1andΩG
overlap(.)onthe 3folds.
METHOD ℓ1 ΩG
OVERLAP (.)
ERROR 0.39±0.04 0 .36±0.01
AV.SIZE C.C.1.1,1,1.0 1 .3,1.4,1.2
terns that are unions of pre-deﬁned groups of covariates,
or,givenagraphofcovariates,groupsofconnectedcovari-
ates in the graph. We obtained promising results on both
simulated and real data.
From a theoretical point of view, we gave both sufﬁcient
and necessary conditions for the correct recovery of the
same union of groups as in the decomposition induced by
ΩG
overlap(≤)on the true optimal parameter vector. It still re-
mainstocharacterizewhenthelatterdecompositionhasthe
smallestnumberofgroups. Thesituationwhereseveralde-
compositionsexistshouldbeanalyzed. Also,theconstruc-
tion of an adaptive version of the Group Lasso with over-
lap that could possibly generalize the scheme proposed by
(Bach, 2008) would be of interest.
From a practical point of view, although algorithms for the
standardgroupLassocanbeusedtoimplement ΩG
overlap(≤),
more dedicated and scalable algorithms could be designed
forcases withlargeoverlaps.
Future work should compare more systematically
ΩG
overlap(≤)andΩG
group(≤)empiricallyand theoretically.
Acknowledgments
This work was supported by ANR grant ANR-07-BLAN-
0311andtheFrance-Berkeleyfund. TheauthorsthankBin
Yu and Michael Jordan foruseful discussions.
References
Bach, F. (2008). Consistency of the group lasso and multi-
plekernellearning. J.Mach.Learn.Res. ,9,1179–1225.
Bach, F. (2009). Exploring large feature spaces with hier-
archical multiple kernel learning. Adv. Neural. Inform.
Process Syst. ,105–112.
Chen, S. S., Donoho, D. L., & Saunders, M. (1998).
Atomic decomposition by basis pursuit. SIAM J. Sci.
Comput.,20, 33–61.
Chuang, H.-Y., Lee, E., Liu, Y.-T., Lee, D., & Ideker, T.
(2007). Network-based classiﬁcation of breast cancer
metastasis. Mol. Syst.Biol. ,3, 140.Fu, W., & Knight, K. (2000). Asymptotics for Lasso-type
estimators. Ann. Stat. ,28, 1356–1378.
Jenatton, R., Audibert, J.-Y., & Bach, F. (2009). Struc-
tured Variable Selection with Sparsity-Inducing Norms .
INRIA -Ecole Normale Supérieure de Paris.
Kumagai, S. (1980). An implicit function theorem: Com-
ment.J. Optim. Theor. Appl. ,31, 285–288.
Meier, L., van de Geer, S., & Bühlmann, P. (2008). The
group lasso for logistic regression. J. Roy. Stat. Soc. B ,
70, 53–71.
Obozinski, G., Taskar, B., & Jordan, M. (2009). Joint co-
variate selection and joint subspace selection for multi-
pleclassiﬁcation problems. Stat. Comput. . To appear.
Rakotomamonjy, A., Bach, F., Canu, S., & Grandvalet, Y.
(2008). SimpleMKL. J. Mach. Learn. Res. ,9, 2491–
2521.
Rapaport, F., Zynoviev, A., Dutreix, M., Barillot, E., &
Vert,J.-P.(2007). Classiﬁcationofmicroarraydatausing
gene networks. BMCBioinformatics ,8, 35.
Roth,V.(2002).Thegeneralizedlasso: awrapperapproach
to gene selection for microarray data. Proc. Conference
on Automated Deduction 14 , 252–255.
Roth, V., & Fischer, B. (2008). The group-lasso for gen-
eralized linear models: uniqueness of solutions and efﬁ-
cient algorithms. Int. Conf. Mach. Learn. , 848–855.
Subramanian, A., et al., (2005). Gene set enrichment
analysis: a knowledge-based approach for interpreting
genome-wide expression proﬁles. Proc. Natl. Acad. Sci.
USA,102, 15545–15550.
Tibshirani, R. (1996). Regression shrinkage and selection
viathe lasso. J. Royal. Statist.Soc. B. ,58, 267–288.
Van de Vijver, M. J., et al., (2002). A gene-expression sig-
natureasapredictorofsurvivalinbreastcancer. N.Engl.
J. Med.,347, 1999–2009.
Wainwright, M. J. (2006). Sharp thresholds for high-
dimensional and noisy recovery of sparsity (Technical
Report 709). UCBerkeley, Department of Statistics.
Yuan,M.,&Lin,Y.(2006).Modelselectionandestimation
inregressionwithgroupedvariables. J.R.Stat.Soc.Ser.
B,68, 49–67.
Zhao, P., Rocha, G., & Yu, B. (2009). Grouped and hi-
erarchical model selection through composite absolute
penalties. Ann. Stat. To appear.
Zhao,P.,&Yu,B.(2006). Onmodelselectionconsistency
of lasso.J. Mach. Learn. Res. ,7, 2541–2563.