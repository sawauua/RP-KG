Analysis of Kernel Mean Matching under Covariate Shift

Yao-Liang Yu yaoliang@cs.ualberta.ca
Csaba Szepesv ari szepesva@cs.ualberta.ca
Department of Computing Science, University of Alberta, Edmonton, AB, T6G 2E8, Canada
Abstract
In real supervised learning scenarios, it is not
uncommon that the training and test sam-
ple follow di
erent probability distributions,
thus rendering the necessity to correct the
sampling bias. Focusing on a particular co-
variate shift problem, we derive high proba-
bility condence bounds for the kernel mean
matching (KMM) estimator, whose conver-
gence rate turns out to depend on some reg-
ularity measure of the regression function and
also on some capacity measure of the ker-
nel. By comparing KMM with the natural
plug-in estimator, we establish the superior-
ity of the former hence provide concrete ev-
idence/understanding to the e
ectiveness of
KMM under covariate shift .
1. Introduction
In traditional supervised learning, the training and
test sample are usually assumed to be drawn from
thesame probability distribution, however, in prac-
tice, this assumption can be easily violated for a vari-
ety of reasons, for instance, due to the sampling bias
or the nonstationarity of the environment. It is there-
fore highly desirable to devise algorithms that remain
e
ective under such distribution shifts.
Needless to say the problem is hopeless if the training
and test distribution share nothing in common. On the
other hand, if the two distributions are indeed related
in a nontrivial manner, then it is a quite remarkable
fact that e
ective adaptation is possible. Under rea-
sonable assumptions, this problem has been attacked
by researchers from statistics (Heckman, 1979; Shi-
modaira, 2000) and more recently by many researchers
from machine learning, see for instance, Zadrozny
Appearing in Proceedings of the 29thInternational Confer-
ence on Machine Learning , Edinburgh, Scotland, UK, 2012.
Copyright 2012 by the author(s)/owner(s).(2004); Huang et al. (2007); Bickel et al. (2009); Ben-
David et al. (2007); Blitzer et al. (2008); Cortes et al.
(2008); Sugiyama et al. (2008); Kanamori et al. (2009).
We focus in this paper on the covariate shift assump-
tion which was rst formulated by Shimodaira (2000)
and has been followed by many others.
The assumption that the conditional probability dis-
tribution of the output variable given the input vari-
able remains xed in both the training and test set is
termed covariate shift ,i.e.the shift happens only for
the marginal probability distributions of the covari-
ates. It is well-known that under this setting, the key
to correct the sampling bias caused by covariate shift
is to estimate the Radon-Nikodym derivative (RND),
also called the importance weight or density ratio. A
number of methods have been proposed to estimate
the RND from nite samples, including kernel mean
matching (KMM) (Huang et al., 2007), logistic re-
gression (Bickel et al., 2009), Kullback-Leibler impor-
tance estimation (Sugiyama et al., 2008), least-squares
(Kanamori et al., 2009), and possibly some others.
Despite of the many algorithms, our current under-
standing of covariate shift still seems to be limited.
From the analyses we are aware of, such as (Gretton
et al., 2009) on the condence bound of the RND by
KMM, (Kanamori et al., 2012) on the convergence rate
of the least-squares estimate of the RND, and (Cortes
et al., 2008) on the distributional stability, they all
assume that certain functions lie in the reproducing
kernel Hilbert space (RKHS) induced by some user se-
lected kernel. Since this assumption is impossible to
verify (even worse, almost certainly violated in prac-
tice), one naturally wonders if we can replace it with
something more reasonable. Such goal is pursued in
this paper and constitutes our main contribution.
We consider the following simple problem: Given the
training samplef(Xtr
i;Ytr
i)gntr
i=1and the test sample
fXte
ignte
i=1, how well can we estimate the expected value
EYte, provided covariate shift has happened? Note
that we do notobserve the output Yte
ion the test
sample. This problem, at a rst glance, ought to beAnalysis of Kernel Mean Matching under Covariate Shift
\easy", after all we are humbly asking for estimat-
ing a scalar. Indeed, under usual assumptions, plus
the nearly impossible assumption that the regression
function lies in the RKHS, we prove a parametric rate,
that isO(n 1
2
tr+n 1
2
te), for the KMM estimator in The-
orem 1 below (to x ideas, we focus exclusively on
KMM in this paper). For a more realistic assump-
tion on the regression function that we borrow from
learning theory (Cucker & Zhou, 2007), the conver-
gence rate, proved in Theorem 2, degrades gracefully
toO(n 
2(+2)
tr +n 
2(+2)
te ), where>0 is a smoothness
parameter measuring certain regularity of the regres-
sion function (in terms of the kernel). Observe that in
the limit when !1 , the regression function even-
tually lies in the RKHS and we recover the previous
parametric rate. In this regard our bound in Theo-
rem 2 is asymptotically optimal. A very nice feature
we discovered for the KMM estimator is that it does
not require knowledge of the smoothness parameter ,
thus, it is in some sense adaptive .
On the negative side, we show that, if the cho-
sen kernel does not interact very well with the un-
known regression function, the convergence rate of the
KMM estimator could be exceedingly slow, roughly
O(log sntrnte
ntr+nte), wheres>0 again measures certain
regularity of the regression function. This unfortunate
result should draw attention to the importance of se-
lecting which kernel to be used in practice. A thorough
comparison between the KMM estimator and the nat-
ural plug-in estimator, conducted in Section 4.3, also
reveals the superiority of the former.
We point out that our results are far from giving a
complete picture even for the simple problem we con-
sider here, for instance, it is unclear to us whether or
not the rate in Theorem 2 can be improved, eventually,
to the parametric rate in Theorem 1? Nevertheless, we
hope that our paper will convince others about the im-
portance and possibility to work with more reasonable
assumptions under covariate shift , and as an example,
suggest relevant tools which can be used to achieve
that goal.
2. Preliminaries
In this section we formally state the covariate shift
problem under our consideration, followed by some rel-
evant discussions.
2.1. Problem Setup
Consider the familiar supervised learning setting,
where we are given independent and identically dis-
tributed ( i.i.d.) training samples f(Xtr
i;Ytr
i)gntr
i=1fromthe joint (Borel) probability measure P tr(dx;dy) on
the (topological) domain XY , and i.i.d. test sam-
plesfXte
ignte
i=1from the joint probability measure
Pte(dx;dy) on the same domain. Notice that we do
not observe the output Yte
ion the test sample, and
more importantly, we do notnecessarily assume that
the training and test sample are drawn from the same
probability measure. The problem we consider in this
paper is to estimate the expected value EYtefrom the
training samplef(Xtr
i;Ytr
i)gntr
i=1and the test sample
fXte
ignte
i=1. In particular, we would like to determine
how fast, say, the 1  condence interval for our es-
timate shrinks to 0 when the sample sizes ntrandnte
increase to innity.
This problem, in its full generality, cannot be solved
simply because the training probability measure can
be completely irrelevant to the test probability mea-
sure that we are interested in. However, if the two
probability measures are indeed related in a nontriv-
ial way, our problem becomes solvable. One particular
example, which we focus on hereafter, is known in the
literature as covariate shift (Shimodaira, 2000):
Assumption 1 (Covariate shift assumption)
Ptr(dyjx) = P te(dyjx): (1)
We use the same notation for the joint, conditional and
marginal probability measures, which should cause no
confusion as the arguments would reveal which mea-
sure is being referred to. Note that the equality
P(dx;dy) = P(dyjx)P(dx) holds from the denition of
the conditional probability measure, whose existence
can be conrmed under very mild assumptions.
Under the covariate shift assumption, the diculty of
our problem, of course, lies entirely on the potential
mismatch between the marginal probability measures
Ptr(dx) and P te(dx). But the Bayes rule already sug-
gests a straightforward approach:
Pte(dx;dy) = P te(dyjx)Pte(dx) = P tr(dx;dy)dPte
dPtr(x);
where the three quantities on the right-hand side can
all be estimated from the given samples. However, in
order for the above equation to make sense, we need
Assumption 2 (Continuity assumption) The
Radon-Nikodym derivative (x) :=dPte
dPtr(x)is well-
dened and bounded from above by B <1.
Note thatB1 due to the normalization constraintR
X(x)Ptr(dx) = 1. The Radon-Nikodym derivative
(RND) is also called the importance weight or the
density ratio in the literature. Evidently, if (x) isAnalysis of Kernel Mean Matching under Covariate Shift
not well-dened, i.e., there exists some measurable
setAsuch that P te(A)>0 and P tr(A) = 0, then
in general we cannot infer P te(dx;dy) from merely
Ptr(dx);Pte(dx) and P tr(dyjx), even under the covari-
ate shift assumption. The bounded from above as-
sumption is more articial. Recently, in a di
erent
setting, (Cortes et al., 2010) managed to replace this
assumption with a bounded second moment assump-
tion, at the expense of sacricing the rate a bit. For us,
since the domainXwill be assumed to be compact, the
bounded from above assumption is not too restrictive
(automatically holds when (x) is, say, continuous).
Once we have the RND (x), it becomes easy to cor-
rect the sampling bias caused by the mismatch be-
tween P tr(dx) and P te(dx), hence solving our problem.
Formally, let
m(x) :=Z
YyPte(dyjx) (2)
be the regression function, then
EYte=Z
Xm(x) Pte(dx) =Z
Xm(x)(x) Ptr(dx):
By the i.i.d. assumption, a reasonable estimator for
EYtewould then be1
ntrPntr
i=1(Xtr
i)Ytr
i. Hence,
similarly to most publications on covariate shift, our
problem boils down to estimating the RND (x).
2.2. A Naive Estimator?
An immediate solution for estimating (x) is to es-
timate the two marginal measures from the train-
ing samplefXtr
igand the test sample fXte
ig, respec-
tively. For instance, if we know a third (Borel) measure
Q(dx) (usually the Lebesgue measure on Rd) such that
bothdPte
dQ(x) anddPtr
dQ(x) exist, we can employ stan-
dard density estimators to estimate them and then set
^(x) =dPte
dQ(x)=dPtr
dQ(x). However, this naive approach
is known to be inferior since density estimation in high
dimensions is hard, and moreover, small estimation er-
ror indPtr
dQ(x) could change ^(x) signicantly. To our
knowledge, there is little theoretical analysis on this
seemingly naive approach.
2.3. A Better Estimator?
It seems more appealing to directly estimate the RND
(x). Indeed, a large body of work has been de-
voted to this line of research (Zadrozny, 2004; Huang
et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008;
Bickel et al., 2009; Kanamori et al., 2009). From the
many references, we single out the kernel mean match-
ing (KMM) algorithm, rst proposed by Huang et al.
(2007) and is also the basis of this paper.KMM tries to match the mean elements in a feature
space induced by a kernel k(;) on the domainXX :
min
^i(
^L(^) :=




1
ntrntrX
i=1^i(Xtr
i) 1
ntenteX
i=1(Xte
i)



H)
s:t:0^iB; (3)
where  :X7!H denotes the canonical feature map,
His the reproducing kernel Hilbert space1(RKHS) in-
duced by the kernel kandkkHstands for the norm in
H. To simplify later analysis, we have chosen to omit
the normalization constraint1
ntrPntr
i=1^i 1,
whereis a small positive number, mainly to re-

ect the 
uctuation caused by random samples. It
is not hard to verify that (3) is in fact an instance
of quadratic programming, hence can be eciently
solved. More details can be found in the paper of
Gretton et al. (2009).
A nite sample 1 condence bound for ^L() (similar
as (10) below) is established in Gretton et al. (2009).
This bound is further transferred into a condence
bound for the generalization error of some family of
loss minimization algorithms in Cortes et al. (2008),
under the notion of distributional stability. However,
neither results can provide a direct answer to our prob-
lem: a nite sample condence bound on the estimate
ofEYte.
2.4. Plug-in Estimator
Another natural approach is to estimate the regression
function from the training sample and then plug into
the test set. We postpone the discussion and compar-
ison with respect to this estimator until section 4.3.
3. Motivation
We motivate the relevance of our problem in this sec-
tion.
Suppose we have an ensemble of classiers, say,
ffjgN
j=1, all trained on the training sample
f(Xtr
i;Ytr
i)gntr
i=1. A useful task is to compare,
hence rank, the classiers by their generalization
errors. This is usually done by assessing the classiers
on some hold out test sample f(Xte
i;Yte
i)gnte
i=1. It is
not uncommon that the test sample is drawn from
some di
erent probability measure than the training
sample, i.e.covariate shift has happened. Since it
could be too costly to re-train the classiers when the
test sample is available, we nevertheless still like to
1A thorough background on the theory of reproducing
kernels can be found in Aronszajn (1950).Analysis of Kernel Mean Matching under Covariate Shift
have a principled way to rank the classiers.
Let`(;) be the user's favourite loss function, and set
Ztr
ij=`(fj(Xtr
i);Ytr
i);Zte
ij=`(fj(Xte
i);Yte
i), then we
can use the empirical average of fZte
ijgnte
i=1to estimate
the generalization error, that is E(Zte
ij), of classier
fj. But what if we do not have access to Yte
ihence
consequently Zte
ij? Can we still accomplish the ranking
job?
The answer is yes, and it is precisely the covariate
shift problem under our consideration. To see that,
consider the pair fXtr
i;Ztr
ijgntr
i=1andfXte
ignte
i=1. Under
the covariate shift assumption, that is P tr(dyjx) =
Pte(dyjx), it is not hard to see that P tr(dzjx) =
Pte(dzjx), hence the covariate shift assumption holds
for the ranking problem, therefore the condence
bounds derived in the next section provide an e
ec-
tive solution.
We do not report numerical experiments in this paper
for two reasons: 1). Our main interest is on theoretical
analysis; 2). Exhaustive experimental results on KMM
can already be found in Gretton et al. (2009).
4. Theoretical Analysis
This section contains our main contribution, i.e., a
theoretical analysis of the KMM estimator for EYte.
4.1. The population version
Let us rst take a look at the population version of
KMM2, which is much easier to analyze and provides
valuable insights:
^2arg min
^



Z
X(x)^(x)Ptr(dx) Z
X(x)Pte(dx)


H
s:t:0^B;Z
X^(x)Ptr(dx) = 1:
The minimum value is 0 since the true RND (x) is
apparently feasible, hence at optimum we always have
Z
X(x)^(x)Ptr(dx) =Z
X(x)Pte(dx):(4)
The question is whether the natural estimatorR
XY^(x)yPtr(dx;dy) is consistent? In other words,
is
Z
Xm(x)^(x)Ptr(dx)?=EYte=Z
Xm(x)(x)Ptr(dx);
(5)
2All Hilbert space valued integrals in this paper are to
be understood as the Bochner integral (Yosida, 1980).where recall that m(x) is the regression function de-
ned in (2) and (x) is the true RND.
The equality in (5) indeed holds under at least two con-
ditions (respectively). First, if the regression function
m2H, then taking inner products with min (4) and
applying the reproducing property we get (5). Second,
if the kernel kischaracteristic (Sriperumbudur et al.,
2010), meaning that the mapR
X(x)P(dx) from the
space of probability measures to the RKHS His injec-
tive, then we conclude ^=from (4) hence follows
(5).
The above two cases suggest the possibility of solv-
ing our problem by KMM. Of course, in reality one
only has nite samples from the underlying probability
measures, thus calls for a thorough study of the empir-
ical KMM, i.e.(3). Interestingly, our analysis reveals
that in the rst case above, we indeed can have a para-
metric rate while in the second case the rate becomes
nonparametric, hence inferior (but does not seem to
rely on the characteristic property of the kernel).
4.2. The empirical version
In this subsection we analyze KMM in details. The
following assumption will be needed:
Assumption 3 (Compactness assumption) Xis
a compact metrizable space, Y  [0;1], and the ker-
nelkis continuous, whence kkk1C2<1.
We usekk1for the supremum norm. Under the above
assumption, the feature map  is continuous hence
measurable (with respect to the Borel -elds), and
the RKHS is separable, therefore the Bochner integrals
in the previous subsection are well-dened. Moreover,
the conditional probability measure indeed exists un-
der our assumption.
We are now ready to derive a nite sample condence
bound for our estimate j1
ntrPntr
i=1^iYtr
i EYtej, where
^iis a minimizer of (3). We start by splitting the sum:
1
ntrntrX
i=1^iYtr
i EYte=1
ntrntrX
i=1^i(Ytr
i m(Xtr
i))
+1
ntrntrX
i=1(^i i)(m(Xtr
i) h(Xtr
i))
+1
ntrntrX
i=1(^i i)h(Xtr
i)
+1
ntrntrX
i=1im(Xtr
i) EYte; (6)
wherei:=(Xtr
i) andh2H is to be specied later.Analysis of Kernel Mean Matching under Covariate Shift
We bound each term individually. For the last term
in (6), we can apply Hoe
ding's inequality (Hoe
ding,
1963) to conclude that with probability at least 1  ,
1
ntrntrX
i=1im(Xtr
i) EYteBr
1
2ntrlog2
:(7)
The rst term in (6) can be bounded similarly. Con-
ditioned onfXtr
igandfXte
ig, we apply again Ho-
e
ding's inequality. Note that ^i(Ytr
i m(Xtr
i))2
[ ^im(Xtr
i);^i(1 m(Xtr
i))], therefore its range is of
size^i. With probability at least 1  ,
1
ntrntrX
i=1^i(Ytr
i m(Xtr
i))vuut1
ntrntrX
i=1^2
ir
1
2ntrlog2

Br
1
2ntrlog2
: (8)
The second and third terms in (6) require more work.
Consider rst the third term:
1
ntrntrX
i=1(^i i)h(Xtr
i)=1
ntrntrX
i=1(^i i)hh;(Xtr
i)i
khkH




1
ntrntrX
i=1(^i i)(Xtr
i)



H
khkH[^L(^) +^L(1:ntr)]
khkH2^L(1:ntr); (9)
where1:ntrdenotes the restriction of to the training
samplefXtr
ig,^L() is dened in (3), and the equality
is because h2H (and the reproducing property of
thecanonical feature map), the rst inequality is by
the Cauchy-Schwarz inequality, the second inequality
is due to the triangle inequality, and the last inequality
is by the optimality of ^and the feasibility of 1:ntrin
problem (3). Next, we bound ^L(1:ntr):
^L(1:ntr) :=




1
ntrntrX
i=1i(Xtr
i) 1
ntenteX
i=1(Xte
i)



H
Cs
2B2
ntr+1
nte
log2
(10)
with probability at least 1  , where the inequality
follows from the Hilbert space valued Hoe
ding in-
equality in (Pinelis, 1994, Theorem 3.5). Note that
Pinelis proved his inequality for martingales in any
2-smooth separable Banach space (Hilbert spaces are
bona de 2-smooth). We remark that another way, see
for instance (Gretton et al., 2009, Lemma 1.5), is to
use McDiarmid's inequality to bound ^L(1:ntr) by itsexpectation, and then bound the expectation straight-
forwardly. In general, Pinelis's inequality will lead to
(slightly) tighter bounds due to its known optimality
(in certain sense).
Finally, we come to the second term left in (6), which
is roughly the approximation error in learning the-
ory (Cucker & Zhou, 2007). Note that all condence
bounds we have derived so far shrink at the paramet-
ric rateO(p
1=ntr+ 1=nte). However, from here on
we will have to tolerate nonparametric rates. Since
we are going to apply di
erent approximation error
bounds to the second term in (6), it seems more con-
venient to collect the results separately. We start with
an encouraging result:
Theorem 1 Under Assumptions 1-3, if the regression
functionm2H (the RKHS induced by the kernel k),
then with probability at least31 ,
1
ntrntrX
i=1^iYtr
i EYteMs
2B2
ntr+1
nte
log6
;
whereM:= 1+2CkmkHand^iis computed from (3).
Proof: By assumption, setting h=mzeros out the
second term in (6). A standard union bound combin-
ing (7)-(10) completes the proof (and we simplied the
bound by slightly worsening the constant).
The condence bound shrinks at the parametric rate,
although the constant depends on kmkH, which in gen-
eral is not computable, but can be estimated from the
training samplef(Xtr
i;Ytr
i)gat a rate worse than para-
metric. Since this estimate inevitably introduces other
uncomputable quantities, we omit the relevant discus-
sion. On the other hand, our bound suggests that if
a priori information about mis indeed available, one
should choose a kernel that minimizes its induced norm
onm.
The case when m62His less satisfactory, despite of its
practicality. We point out that a denseness argument
cannot resolve this diculty. To be more precise, let
us assume for a moment m2C(X) (the space of con-
tinuous functions on X) andkbe a universal kernel
(Steinwart, 2002), meaning that the RKHS induced by
kis dense in ( C(X);kk1). By the assumed univer-
sal property of the kernel, there exists suitable h2H
that makes the second term in (6) arbitrarily small (in
fact, can be made vanishing), however, on the other
hand, recall that the bound (9) on the third term in
(6) depends onkhkHhence could blow up. If we trade
3Throughout this paper, the condence parameter is
always taken arbitrarily in (0;1).Analysis of Kernel Mean Matching under Covariate Shift
o
 the two terms appropriately, we might get a rate
that is acceptable (but worse than parametric). The
next theorem concretizes this idea.
Theorem 2 Under Assumptions 1-3, if A2(m;R ) :=
inf
kgkHRkm gkL2
PtrC2R =2for some >0and
constantC20, then with probability at least 1 ,
1
ntrntrX
i=1^iYtr
i EYte
Br
9
2ntrlog8
+C(BC2)2
+2D
+2
2;
whereD2:= 2Cr
2
B2
ntr+1
nte
log8
+BCq
1
2ntrlog8
,
C:= (1 + 2=) 
22
+2and ^iis computed from (3).
Proof: By the triangle inequality,
1
ntrntrX
i=1(^i i)(m(Xtr
i) h(Xtr
i))
B1
ntrntrX
i=1jm(Xtr
i) h(Xtr
i)j:
Not surprisingly, we apply yet again Hoe
ding's in-
equality to relate the last term above to its expecta-
tion. Since
km hk11 +khh;()ik11 +CkhkH;
we have with probability at least 1  ,
1
ntrntrX
i=1jm(Xtr
i) h(Xtr
i)j(1+CR)r
1
2ntrlog2
+A2(m;R );
whereR:=khkH. Combining this bound with (7)-
(10) and applying our assumption on A2(m;R ):
1
ntrntrX
i=1(^i i)(m(Xtr
i) h(Xtr
i))
Br
2
ntrlog8
+ 2RCs
2B2
ntr+1
nte
log8

+BC2R =2+B(1 +CR)r
1
2ntrlog8
:
SettingR=
BC 2
2D22
+2completes the proof.
In Theorem 2 we do not even assume m2C(X); all
we need ism2L2
Ptr, the space of P tr(dx) square inte-
grable functions. The latter condition always holdssince 0m1 by Assumption 3. The quan-
tityA2(m;R ) is called the approximation error in
learning theory and its polynomial decay is known
to be (almost) equivalent to m2Range(T
2+4
k),
see for instance Theorem 4.1 of Cucker & Zhou
(2007). HereTkis the integral operator ( Tkf)(x0) =R
Xk(x0;x)f(x)Ptr(dx) onL2
Ptr. The smoothness pa-
rameter > 0 measures the regularity of the regres-
sion function, and as it increases, the range space of
T
2+4
kbecomes smaller, hence our decay assumption
onA2(m;R ) becomes more stringent. Note that the
exponent
2+4is necessarily smaller than 1 =2 (but ap-
proaches 1=2 when!1 ) because by Mercer's theo-
remT1
2
kis ontoH(in which case the range assumption
would bring us back to Theorem 1).
Theorem 2 shows that the condence bound now
shrinks at a slower rate, roughly O(n 
2(+2)
tr +
n 
2(+2)
te ), which, as !1 , approaches the paramet-
ric rateO(n 1
2
tr+n 1
2
te) derived in Theorem 1 where
we assume m2H. We point out that the source of
this slower rate comes from the irregular nature of the
regression function (in the eye of the kernel k).
The polynomial decay assumption on A2(m;R ) is not
always satised, for instance, it is shown in Theorem
6.2 of Cucker & Zhou (2007) that for C1(indenite
times di
erentiable) kernels (such as the popular Gaus-
sian kernel), polynomial decay implies that the regres-
sion function m2C1(X) (under mild assumptions on
Xand P tr(dx)). Therefore, as long as one works with
smooth kernels but nonsmooth regression functions,
the approximation error has to decay logarithmically
slowly. We give a logarithmic bound for such cases.
Theorem 3 Under Assumptions 1-3, if A1(m;R ) :=
inf
kgkHRkm gk1C1(logR) sfor somes>0and
constantC10(assumingR1), then (for ntrand
ntelarger than some constant),
1
ntrntrX
i=1^iYtr
i EYte
1 +1
ss
BC1
logsBC1
D1 s
+Br
2
ntrlog6
+ (sBC1)s
s+1D1
s+11
holds with probability at least 1 , whereD1=
2Cr
2
B2
ntr+1
nte
log6
and ^iis computed from (3).
The proof is similar as that of Theorem 2 except that
we setR= (sBC1
D1)s
s+1.
Theorem 3 shows that in such unfavourable cases,
the condence bound shrinks at an exceedingly slowAnalysis of Kernel Mean Matching under Covariate Shift
rate, roughly,O(log sntrnte
ntr+nte). The reason, of course,
is due to the slow decay of the approximation error
A1(m;R ). It is proved in Theorem 6.1 of Cucker &
Zhou (2007) that for the Gaussian kernel k(x0;x) =
exp( kx x0k2
2=2), ifX Rdhas smooth bound-
ary and the regression function m2Hs(X) with in-
dexs > d= 2, then the logarithmic decay assumed in
Theorem 3 holds. Here Hs(X) is the Sobolev space
(the completion of C1(X) under the inner product
hf;gis:=R
XP
j
jsd
f
dxd
g
dx, assuming s2N). Simi-
lar bounds also hold for the inverse multiquadrics ker-
nelk(x0;x) = (c2+kx x0k2
2) 
with
 > 0. We
remark that in this regard Theorem 3 disrespects the
popular Gaussian kernel used ubiquitously in practice
and should draw the attention of researchers.
4.3. Discussion
It seems worthwhile to devote a subsection to dis-
cussing a very natural question that the reader might
already have: why not estimate the regression function
mon the training set and then plug into the test set,
after allmdoes not change under the covariate shift
assumption? Algorithmically, this is perfectly doable,
perhaps conceptually even simpler since the algorithm
does not need to see the test data beforehand. We note
that estimating the regression function from i.i.d. sam-
ples has been well studied in the learning theory lit-
erature, see for instance, Chapter 8 of Cucker & Zhou
(2007) and the many references therein.
The diculty, though, lies in the appropriate error
metric on the estimate. Recall that when estimating
the regression function from i.i.d. training samples,
one usually measures the progress ( i.e.the discrep-
ancy between the estimate ^ mandm) by the L2norm
under the training probability measure P tr(dx), while
what we really want is a condence bound on the term
1
ntenteX
i=1^m(Xte
i) EYte: (11)
Since P tr6= P te, there is evidently a probability mea-
sure mismatch between the bound we have from es-
timatingmand the true interested quantity. Indeed,
conditioned on the training sample f(Xtr
i;Ytr
i)g, using
the triangle inequality we can bound (11) by :
1
ntenteX
i=1^m(Xte
i) Z
^m(x)Pte(dx)+k^m mkL2
Pte:
The rst term above can be bounded again through
Hoe
ding's inequality, while the second term is
close to what we usually have from estimat-
ingm: the only di
erence being that the L2norm is now under the test probability measure
Pte(dx). Fortunately, since the norm of the identity
map id : ([ 1;1]X;kkL2
Ptr)7!([ 1;1]X;kkL2
Pte) is
bounded byp
B(see Assumption 2), we can deduce
a bound for (11) based upon results from estimating
m, though less appealingly, a much looser bound than
the one given in Theorem 2. We record such a result
for the purpose of comparison:
Theorem 4 Under Assumptions 1-3, if the regression
functionm2Range(T
2+4
k)for some>0, then with
probability at least 1 ,
1
ntenteX
i=1^m(Yte
i) EYter
1
2ntelog4
+p
BC1n 3
12+16
tr;
whereC1is some constant that does not depend on
ntr;nte, and ^mis the (regularized least-squares) esti-
mate ofmin Smale & Zhou (2007).
The theorem follows from the bound on k^m mkL2
Ptr
in Corollary 3.2 of Sun & Wu (2009), which is an im-
provement over Smale & Zhou (2007).
Carefully comparing the current theorem with Theo-
rem 2, we observe: 1). Theorem 4, which is based
on the regularized least-squares estimate of the regres-
sion function, needs to know in advance the parameter
(in order to tune the regularization constant) while
Theorem 2, derived for KMM, does notrequire any
such information, hence in some sense KMM is \adap-
tive"; 2). Theorem 4 has much worse dependence on
the training sample size ntr; it does not recover the
parametric rate even when the smoothness parameter
goes to1(we getn 1=4
tr, instead of n 1=2
tr). On the
other hand, Theorem 4 has better dependence on the
test sample size nte, which is, however, probably not so
important since usually one has much more test sam-
ples than training samples because the lack of labels
make the former much easier to acquire; 3). Theorem
4 seems to have better dependence on the parameter
B; 4). Given the fact that KMM utilizes both the
training data and the test data in the learning phase,
it is not entirely a surprise that KMM wins in terms of
convergence rate, nevertheless, we nd it quite stun-
ning that by sacricing the rate slightly on nte, KMM
is able to improve the rate on ntrso signicantly.
5. Conclusion
For estimating the expected value of the output on
the test set where covariate shift has happened, we
have derived high probability condence bounds for
the kernel mean matching (KMM) estimator, whichAnalysis of Kernel Mean Matching under Covariate Shift
converges, roughly O(n 1
2
tr+n 1
2
te) when the regres-
sion function lies in the RKHS, and more generally
O(n 
2(+2)
tr +n 
2(+2)
te ) when the regression function
exhibits certain regularity measured by . An ex-
tremely slow rate, roughly O(log sntrnte
ntr+nte), is also
provided, calling attention of choosing the right ker-
nel. From the comparison of the bounds, KMM proves
to be much more superior than the plug-in estima-
tor hence provides concrete evidence/understanding to
the e
ectiveness of KMM under covariate shift .
Although it is unclear to us if it is possible to avoid
approximating the regression function, we suspect the
bound in Theorem 2 is in some sense optimal and we
are currently investigating it. We also plan to general-
ize our results to the least-squares estimation problem.
Acknowledgements
This work was supported by Alberta Innovates Tech-
nology Futures and NSERC.
References
Aronszajn, Nachman. Theory of reproducing kernels.
Transactions of the American Mathematical Sociery ,
68:337{404, 1950.
Ben-David, Shai, Blitzer, John, Crammer, Koby, and
Pereira, Fernando. Analysis of representations for
domain adaptation. In NIPS , pp. 137{144. 2007.
Bickel, Ste
en, Br uckner, Michael, and Sche
er, To-
bias. Discriminative learning under covariate shift.
JMLR , 10:2137{2155, 2009.
Blitzer, John, Crammer, Koby, Kulesza, Alex, Pereira,
Fernando, and Wortman, Jennifer. Learning bounds
for domain adaptation. In NIPS , pp. 129{136. 2008.
Cortes, Corinna, Mohri, Mehryar, Riley, Michael, and
Rostamizadeh, Afshin. Sample selection bias correc-
tion theory. In ALT, pp. 38{53. 2008.
Cortes, Corinna, Mansour, Yishay, and Mohri,
Mehryar. Learning bounds for importance weight-
ing. In NIPS , pp. 442{450. 2010.
Cucker, Felipe and Zhou, Ding-Xuan. Learning theory:
an approximation theory viewpoint . Cambridge Uni-
versity Press, 2007.
Gretton, Arthur, Smola, Alexander J., Huang, Ji-
ayuan, Schmittfull, Marcel, Borgwardt, Karsten M.,
and Sch olkopf, Bernhard. Covariate Shift by Kernel
Mean Matching , pp. 131{160. MIT Press, 2009.Heckman, James J. Sample selection bias as a speci-
cation error. Econometrica , 47(1):153{161, 1979.
Hoe
ding, Wassily. Probability inequalities for sums of
bounded random variables. Journal of the American
Statistical Association , 58(301):13{30, 1963.
Huang, Jiayuan, Smola, Alexander J., Gretton,
Arthur, Borgwardt, Karsten M., and Sch olkopf,
Bernhard. Correcting sample selection bias by un-
labeled data. In NIPS , pp. 601{608. 2007.
Kanamori, Takafumi, Hido, Shohei, and Sugiyama,
Masashi. A least-squares approach to direct impor-
tance estimation. JMLR , 10:1391{1445, 2009.
Kanamori, Takafumi, Suzuki, Taiji, and Sugiyama,
Masashi. Statistical analysis of kernel-based least-
squares density-ratio estimation. Machine Learning ,
86:335{367, 2012.
Pinelis, Iosif. Optimum bounds for the distributions of
martingales in Banach spaces. The Annals of Prob-
ability , 22(4):1679{1706, 1994.
Shimodaira, Hidetoshi. Improving predictive inference
under covariate shift by weighting the log-likelihood
function. Journal of Statistical Planning and Infer-
ence, 90(2):227{244, 2000.
Smale, Steve and Zhou, Ding-Xuan. Learning theory
estimates via integral operators and their approxi-
mations. Constructive Approximation , 26:153{172,
2007.
Sriperumbudur, Bharath K., Gretton, Arthur, Fuku-
mizu, Kenji, Sch olkopf, Bernhard, and Lanckriet,
Gert R. G. Hilbert space embeddings and metrics on
probability measures. JMLR , 11:1517{1561, 2010.
Steinwart, Ingo. On the in
uence of the kernel on the
consistency of support vector machines. JMLR , 2:
67{93, 2002.
Sugiyama, Masashi, Nakajima, Shinichi, Kashima,
Hisashi, Buenau, Paul Von, and Kawanabe, Mo-
toaki. Direct importance estimation with model se-
lection and its application to covariate shift adapta-
tion. In NIPS , pp. 1433{1440. 2008.
Sun, Hongwei and Wu, Qiang. A note on applica-
tion of integral operator in learning theory. Applied
and Computational Harmonic Analysis , 26:416{421,
2009.
Yosida, K^ osaku. Functional Analysis . Springer, 6th
edition, 1980.
Zadrozny, Bianca. Learning and evaluating classiers
under sample selection bias. In ICML . 2004.