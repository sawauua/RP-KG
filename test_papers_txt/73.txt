Which Training Methods for GANs do actually Converge?

Lars Mescheder1Andreas Geiger1 2Sebastian Nowozin3
Abstract
Recent work has shown local convergence of
GAN training for absolutely continuous data and
generator distributions. In this paper, we show
that the requirement of absolute continuity is nec-
essary: we describe a simple yet prototypical
counterexample showing that in the more real-
istic case of distributions that are not absolutely
continuous, unregularized GAN training is not
always convergent. Furthermore, we discuss reg-
ularization strategies that were recently proposed
to stabilize GAN training. Our analysis shows
that GAN training with instance noise or zero-
centered gradient penalties converges. On the
other hand, we show that Wasserstein-GANs and
WGAN-GP with a Ô¨Ånite number of discriminator
updates per generator update do not always con-
verge to the equilibrium point. We discuss these
results, leading us to a new explanation for the
stability problems of GAN training. Based on
our analysis, we extend our convergence results
to more general GANs and prove local conver-
gence for simpliÔ¨Åed gradient penalties even if the
generator and data distributions lie on lower di-
mensional manifolds. We Ô¨Ånd these penalties to
work well in practice and use them to learn high-
resolution generative image models for a variety
of datasets with little hyperparameter tuning.
1. Introduction
Generative Adversarial Networks (GANs) (Goodfellow
et al., 2014) are powerful latent variable models that can be
used to learn complex real-world distributions. Especially
for images, GANs have emerged as one of the dominant
approaches for generating new realistically looking samples
after the model has been trained on some dataset.
1MPI T√ºbingen, Germany2ETH Z√ºrich, Switzerland
3Microsoft Research, Cambridge, UK. Correspondence to: Lars
Mescheder <lars.mescheder@tue.mpg.de>.
Proceedings of the 35thInternational Conference on Machine
Learning , Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).Method Local
convergence
(a.c. case)Local
convergence
(general case)
unregularized (Goodfellow et al., 2014) 3 7
WGAN (Arjovsky et al., 2017) 7 7
WGAN-GP (Gulrajani et al., 2017) 7 7
DRAGAN (Kodali et al., 2017) 3 7
Instance noise (S√∏nderby et al., 2016) 3 3
ConOpt (Mescheder et al., 2017) 3 3
Gradient penalties (Roth et al., 2017) 3 3
Gradient penalty on real data only 3 3
Gradient penalty on fake data only 3 3
Table 1. Convergence properties of different GAN training algo-
rithms for general GAN-architectures. Here, we distinguish be-
tween the case where both the data and generator distributions are
absolute continuous (a.c.) and the general case where they may lie
on lower dimensional manifolds.
However, while very powerful, GANs can be hard to train
and in practice it is often observed that gradient descent
based GAN optimization does not lead to convergence. As
a result, a lot of recent research has focused on Ô¨Ånding
better training algorithms (Arjovsky et al., 2017; Gulrajani
et al., 2017; Kodali et al., 2017; S√∏nderby et al., 2016; Roth
et al., 2017) for GANs as well as gaining better theoretically
understanding of their training dynamics (Arjovsky et al.,
2017; Arjovsky & Bottou, 2017; Mescheder et al., 2017;
Nagarajan & Kolter, 2017; Heusel et al., 2017).
Despite practical advances, the training dynamics of GANs
are still not completely understood. Recently, Mescheder
et al. (2017) and Nagarajan & Kolter (2017) showed that
local convergence and stability properties of GAN train-
ing can be analyzed by examining the eigenvalues of the
Jacobian of the the associated gradient vector Ô¨Åeld: if the
Jacobian has only eigenvalues with negative real-part at the
equilibrium point, GAN training converges locally for small
enough learning rates. On the other hand, if the Jacobian has
eigenvalues on the imaginary axis, it is generally not locally
convergent. Moreover, Mescheder et al. (2017) showed that
if there are eigenvalues close but not on the imaginary axis,
the training algorithm can require intractably small learning
rates to achieve convergence. While Mescheder et al. (2017)
observe eigenvalues close to the imaginary axis in practice,
this observation does not answer the question if eigenvalues
close to the imaginary axis are a general phenomenon and if
yes, whether they are indeed the root cause for the trainingWhich Training Methods for GANs do actually Converge?
instabilities that people observe in practice.
A partial answer to this question was given by Nagarajan
& Kolter (2017), who showed that for absolutely continu-
ous data and generator distributions1all eigenvalues of the
Jacobian have negative real-part. As a result, GANs are lo-
cally convergent for small enough learning rates in this case.
However, the assumption of absolute continuity is not true
for common use cases of GANs, where both distributions
lie on lower dimensional manifolds (S√∏nderby et al., 2016;
Arjovsky & Bottou, 2017).
In this paper we show that this assumption is indeed nec-
essary: by considering a simple yet prototypical example
of GAN training we analytically show that (unregularized)
GAN training is not always locally convergent. We also
discuss how recent techniques for stabilizing GAN train-
ing affect local convergence on our example problem. Our
Ô¨Åndings show that neither Wasserstein GANs (WGANs) (Ar-
jovsky et al., 2017) nor Wasserstein GANs with Gradient
Penalty (WGAN-GP) (Gulrajani et al., 2017) nor DRAGAN
(Kodali et al., 2017) converge on this simple example for a
Ô¨Åxed number of discriminator updates per generator update.
On the other hand, we show that instance noise (S√∏nderby
et al., 2016; Arjovsky & Bottou, 2017), zero-centered gradi-
ent penalties (Roth et al., 2017) and consensus optimization
(Mescheder et al., 2017) lead to local convergence.
Based on our analysis, we give a new explanation for the
instabilities commonly observed when training GANs based
on discriminator gradients orthogonal to the tangent space
of the data manifold. We also introduce simpliÔ¨Åed gradient
penalties for which we prove local convergence. We Ô¨Ånd
that these gradient penalties work well in practice, allowing
us, among others, to learn a generative image model of all
1000 Imagenet classes in a single GAN.
In summary, our contributions are as follows:
We identify a simple yet prototypical counterexample
showing that (unregularized) gradient descent based
GAN optimization is not always locally convergent
We discuss if and how recently introduced regulariza-
tion techniques stabilize the training
We introduce simpliÔ¨Åed gradient penalties and prove
local convergence for the regularized GAN training
dynamics
All proofs can be found in the supplementary material.
1Nagarajan & Kolter (2017) also proved local convergence for
a slightly more general family of probability distributions where
the support of the generator is equal to the support of the true data
distribution near the equilibrium point. Alternatively, they show
that their results also hold when the discriminator satisÔ¨Åes certain
(strong) smoothness conditions. However, these conditions are
usually hard to satisfy in practice without prior knowledge about
the support of the true data distribution.2. Instabilities in GAN training
2.1. Background
GANs are deÔ¨Åned by a min-max two-player game between
a discriminative network D (x)and generative network
G(z). While the discriminator tries to distinguish between
real data point and data points produced by the generator,
the generator tries to fool the discriminator. It can be shown
(Goodfellow et al., 2014) that if both the generator and
discriminator are powerful enough to approximate any real-
valued function, the unique Nash-equilibrium of this two
player game is given by a generator that produces the true
data distribution and a discriminator which is 0everywhere
on the data distribution.
Following the notation of Nagarajan & Kolter (2017), the
training objective for the two players can be described by
an objective function of the form
L(; ) = Ep(z)[f(D (G(z)))]
+ EpD(x)[f( D (x))] (1)
for some real-valued function f. The common choice
f(t) = log(1 + exp( t))leads to the loss function con-
sidered in the original GAN paper (Goodfellow et al., 2014).
For technical reasons we assume that fis continuously dif-
ferentiable and satisÔ¨Åes f0(t)6= 0for allt2R.
The goal of the generator is to minimize this loss whereas
the discriminator tries to maximize it. Our goal when train-
ing GANs is to Ô¨Ånd a Nash-equilibrium, i.e. a parameter
assignment (; )where neither the discriminator nor the
generator can improve their utilities.
GANs are usually trained using Simultaneous or Alternating
Gradient Descent (SimGD and AltGD). Both algorithms
can be described as Ô¨Åxed point algorithms (Mescheder et al.,
2017) that apply some operator Fh(; )to the parameter
values (; )of the generator and discriminator, respectively.
For example, simultaneous gradient descent corresponds to
the operator Fh(; ) = (; ) +hv(; ), wherev(; )
denotes the gradient vector Ô¨Åeld
v(; ) := rL(; )
r L(; )
: (2)
Similarly, alternating gradient descent can be described by
an operatorFh=F2;hF1;hwhereF1;handF2;hperform
an update for the generator and discriminator, respectively.
Recently, it was shown (Mescheder et al., 2017) that local
convergence of GAN training near an equilibrium point
(; )can be analyzed by looking at the spectrum of the
JacobianF0
h(; )at the equilibrium: if F0
h(; )has
eigenvalues with absolute value bigger than 1, the training
algorithm will generally not converge to (; ). On the
other hand, if all eigenvalues have absolute value smallerWhich Training Methods for GANs do actually Converge?
pD=0 p=
D (x)
xy
(a)t=t0pD=0p=D (x)
xy
(b)t=t1
Figure 1. Visualization of the counterexample showing that in the
general case, gradient descent GAN optimization is not convergent:
(a) In the beginning, the discriminator pushes the generator towards
the true data distribution and the discriminator‚Äôs slope increases.
(b) When the generator reaches the target distribution, the slope of
the discriminator is largest, pushing the generator away from the
target distribution. This results in oscillatory training dynamics
that never converge.
than1, the training algorithm will converge to (; )with
linear rateO(jmaxjk)wheremax is the eigenvalue of
F0(; )with the biggest absolute value. If all eigenval-
uesF0(; )are on the unit circle, the algorithm can be
convergent, divergent or neither, but if it is convergent it will
generally converge with a sublinear rate. A similar result
(Khalil, 1996; Nagarajan & Kolter, 2017) also holds for the
(idealized) continuous system
_(t)
_ (t)
=
 r L(; )
rL(; )
(3)
which corresponds to training the GAN with inÔ¨Ånitely small
learning rate: if all eigenvalues of the Jacobian v0(; )
at a stationary point (; )have negative real-part, the
continuous system converges locally to (; )with lin-
ear convergence rate. On the other hand, if v0(; )has
eigenvalues with positive real-part, the continuous system
is not locally convergent. If all eigenvalues have zero real-
part, it can be convergent, divergent or neither, but if it is
convergent, it will generally converge with a sublinear rate.
For simultaneous gradient descent linear convergence can
be achieved if and only if all eigenvalues of the Jacobian
of the gradient vector Ô¨Åeld v(; )have negative real part
(Mescheder et al., 2017). This situation was also considered
by Nagarajan & Kolter (2017) who examined the asymptotic
case of step sizes hthat go to 0and proved local convergence
for absolutely continuous generator and data distributions
under certain regularity assumptions.
2.2. The Dirac-GAN
Simple experiments, simple theorems are the building
blocks that help us understand more complicated systems.
Ali Rahimi - Test of Time Award speech, NIPS 2017
In this section, we describe a simple yet prototypical coun-
terexample which shows that in the general case, unregular-
ized GAN training is neither locally nor globally convergent.DeÔ¨Ånition 2.1. In the Dirac-GAN, the true (univariate) data
distribution pDis given bypD=0and the generator is
given byp=. The discriminator is given given by a
linear function: D (x) = x.
Note that in the Dirac-GAN, both the generator and the
discriminator have exactly one parameter. This situation
is visualized in Figure 1. In this setup, the GAN training
objective (1) is given by
L(; ) =f( ) +f(0) (4)
While using linear discriminators might appear restrictive,
the class of linear discriminators is in fact as powerful as
the class of all real-valued functions for this example: when
we usef(t) = log(1 + exp( t))and we take the supre-
mum over in(4), we obtain (up to scalar and additive
constants) the Jensen-Shannon divergence between pand
pD. The same holds true for the Wasserstein-divergence,
when we use f(t) =tand put a Lipschitz constraint on the
discriminator (see Section 3.1).
We show that the training dynamics of GANs lead to diver-
gent behavior in this simple setup.
Lemma 2.2. The unique equilibrium point of the training
objective in (4)is given by = = 0. Moreover, the
Jacobian of the gradient vector Ô¨Åeld at the equilibrium point
has the two eigenvalues f0(0)iwhich are both on the
imaginary axis.
We now take a closer look at the training dynamics produced
by various algorithms for training the Dirac-GAN. First, we
consider the (idealized) continuous system in (3): while
Lemma 2.2 shows that the continuous system is generally
not linearly convergent to the equilibrium point, it could
in principle converge with a sublinear convergence rate.
However, this is not the case as the next lemma shows:
Lemma 2.3. The integral curves of the gradient vector Ô¨Åeld
v(; )do not converge to the Nash-equilibrium. More
speciÔ¨Åcally, every integral curve ((t); (t))of the gradient
vector Ô¨Åeldv(; )satisÔ¨Åes(t)2+ (t)2=const for all
t2[0;1).
Note that our results do not contradict the results of Nagara-
jan & Kolter (2017) and Heusel et al. (2017): our example
violates Assumption IV in Nagarajan & Kolter (2017) that
the support of the generator distribution is equal to the sup-
port of the true data distribution near the equilibrium. It also
violates the assumption in Heusel et al. (2017) that the opti-
mal discriminator parameter vector is a continuous function
of the current generator parameters2. In fact, unless = 0,
2This assumption is usually even violated by Wasserstein-
GANs, as the optimal discriminator parameter vector as a function
of the current generator parameters can have discontinuities near
the Nash-equilibrium. See Section 3.1 for details.Which Training Methods for GANs do actually Converge?
(a) SimGD
 (b) AltGD
Figure 2. Training behavior of the Dirac-GAN. The starting iterate
is marked in red.
there is not even an optimal discriminator parameter vec-
tor for the Dirac-GAN. Indeed, we Ô¨Ånd that two-time scale
updates as suggested by Heusel et al. (2017) do not help con-
vergence towards the Nash-equilibrium (see Figure 22 in the
supplementary material). However, our example seems to
be a prototypical situation for (unregularized) GAN training
which usually deals with distributions that are concentrated
on lower dimensional manifolds (Arjovsky & Bottou, 2017).
We now take a closer look at the discretized system .
Lemma 2.4. For simultaneous gradient descent, the Ja-
cobian of the update operator Fh(; )has eigenvalues
1=2= 1hf0(0)iwith absolute valuesp
1 +h2f0(0)2
at the Nash-equilibrium. Independently of the learning rate,
simultaneous gradient descent is therefore not stable near
the equilibrium. Even stronger, for every initial condition
and learning rate h>0, the norm of the iterates (k; k)
obtained by simultaneous gradient descent is monotonically
increasing.
The behavior of simultaneous gradient descent on our exam-
ple problem is visualized in Figure 2a.
Similarly, for alternating gradient descent we have
Lemma 2.5. For alternating gradient descent with nggen-
erator andnddiscriminator updates, the Jacobian of the
update operator Fh(; )has eigenvalues
1=2= 1 
2
2s
1 
2
22
 1: (5)
with
:=pngndhf0(0). For
2, all eigenvalues are
hence on the unit circle. Moreover for 
 > 2, there are
eigenvalues outside the unit circle.
Even though Lemma 2.5 shows that alternating gradient
descent does not converge linearly to the Nash-equilibrium,
it could in principle converge with a sublinear convergence
rate. However, this is very unlikely because ‚Äì as Lemma 2.3
shows ‚Äì even the continuous system does not converge. In-
deed, we empirically found that alternating gradient descent
oscillates in stable cycles around the equilibrium and shows
no sign of convergence (Figure 2b).2.3. Where do instabilities come from?
Our simple example shows that naive gradient based GAN
optimization does not always converge to the equilibrium
point. To get a better understanding of what can go wrong
for more complicated GANs, it is instructive to analyze
these instabilities in depth for this simple example problem.
To understand the instabilities, we have to take a closer
look at the oscillatory behavior that GANs exhibit both for
the Dirac-GAN and for more complex systems. An intu-
itive explanation for the oscillations is given in Figure 1:
when the generator is far from the true data distribution,
the discriminator pushes the generator towards the true data
distribution. At the same time, the discriminator becomes
more certain, which increases the discriminator‚Äôs slope (Fig-
ure 1a). Now, when the generator reaches the target distri-
bution (Figure 1b), the slope of the discriminator is largest,
pushing the generator away from the target distribution. As
a result, the generator moves away again from the true data
distribution and the discriminator has to change its slope
from positive to negative. After a while, we end up with a
similar situation as in the beginning of training, only on the
other side of the true data distribution. This process repeats
indeÔ¨Ånitely and does not converge.
Another way to look at this is to consider the local behavior
of the training algorithm near the Nash-equilibrium. Indeed,
near the Nash-equilibrium, there is nothing that pushes the
discriminator towards having zero slope on the true data
distribution. Even if the generator is initialized exactly on
the target distribution, there is no incentive for the discrimi-
nator to move to the equilibrium discriminator. As a result,
training is unstable near the equilibrium point.
This phenomenon of discriminator gradients orthogonal to
the data distribution can also arise for more complex exam-
ples: as long as the data distribution is concentrated on a
low dimensional manifold and the class of discriminators
is big enough, there is no incentive for the discriminator to
produce zero gradients orthogonal to the tangent space of
the data manifold and hence converge to the equilibrium
discriminator. Even if the generator produces exactly the
true data distribution, there is no incentive for the discrim-
inator to produce zero gradients orthogonal to the tangent
space. When this happens, the discriminator does not pro-
vide useful gradients for the generator orthogonal to the data
distribution and the generator does not converge.
Note that these instabilities can only arise if the true data
distribution is concentrated on a lower dimensional man-
ifold. Indeed, Nagarajan & Kolter (2017) showed that -
under some suitable assumptions - gradient descent based
GAN optimization is locally convergent for absolutely con-
tinuous distributions. Unfortunately, this assumption may
not be satisÔ¨Åed for data distributions like natural images toWhich Training Methods for GANs do actually Converge?
(a) Standard GAN
 (b) Non-saturating GAN
(c) WGAN ( nd= 5)
 (d) WGAN-GP ( nd= 5)
(e) Consensus optimization
 (f) Instance noise
(g) Gradient penalty
 (h) Gradient penalty (CR)
Figure 3. Convergence properties of different GAN training al-
gorithms using alternating gradient descent with recommended
number of discriminator updates per generator update ( nd= 1
if not noted otherwise). The shaded area in Figure 3c visualizes
the set of forbidden values for the discriminator parameter  . The
starting iterate is marked in red.
which GANs are commonly applied (Arjovsky & Bottou,
2017). Moreover, even if the data distribution is absolutely
continuous but concentrated along some lower dimensional
manifold, the eigenvalues of the Jacobian of the gradient
vector Ô¨Åeld will be very close to the imaginary axis, result-
ing in a highly ill-conditioned problem. This was observed
by Mescheder et al. (2017) who examined the spectrum
of the Jacobian for a data distribution given by a circular
mixture of Gaussians with small variance.
3. Regularization strategies
As we have seen in Section 2, unregularized GAN training
does not always converge to the Nash-equilibrium. In this
section, we discuss how several regularization techniques
that have recently been proposed, inÔ¨Çuence convergence of
the Dirac-GAN.
Interestingly, we also Ô¨Ånd that the non-saturating loss pro-posed in the original GAN paper (Goodfellow et al., 2014)
leads to convergence of the continuous system, albeit with
an extremely slow convergence rate. A more detailed discus-
sion and an analysis of Consensus optimization (Mescheder
et al., 2017) can be found in the supplementary material.
3.1. Wasserstein GAN
The two-player GAN game can be interpreted as minimizing
a probabilistic divergence between the true data distribution
and the distribution produced by the generator (Nowozin
et al., 2016; Goodfellow et al., 2014). This divergence is
obtained by considering the best-response strategy for the
discriminator, resulting in an objective function that only
contains the generator parameters. Many recent regular-
ization techniques for GANs are based on the observation
(Arjovsky & Bottou, 2017) that this divergence may be dis-
continuous with respect to the parameters of the generator
or may even take on inÔ¨Ånite values if the support of the data
distribution and the generator distribution do not match.
To make the divergence continuous with respect to the pa-
rameters of the generator, Wasserstein GANs (WGANs)
Arjovsky et al. (2017) replace the Jensen-Shannon diver-
gence used in the original derivation of GANs (Goodfellow
et al., 2014) with the Wasserstein-divergence. As a result,
Arjovsky et al. (2017) propose to use f(t) =tand restrict
the class of discriminators to Lipschitz continuous functions
with Lipschitz constant equal to some g0>0. While a
WGAN converges if the discriminator is always trained un-
til convergence, in practice WGANs are usually trained by
running only a Ô¨Åxed Ô¨Ånite number of discriminator updates
per generator update. However, near the Nash-equilibrium
the optimal discriminator parameters can have a disconti-
nuity as a function of the current generator parameters: in
the Dirac-GAN, the optimal discriminator has to move from
 = 1to = 1whenchanges signs. As the gradients
get smaller near the equilibrium point, the gradient updates
do not lead to convergence for the discriminator. Overall,
the training dynamics are again determined by the Jacobian
of the gradient vector Ô¨Åeld near the Nash-equilibrium:
Lemma 3.1. WGANs trained with simultaneous or alternat-
ing gradient descent with a Ô¨Åxed number of discriminator
updates per generator update and a Ô¨Åxed learning rate
h >0do generally not converge to the Nash equilibrium
for the Dirac-GAN.
The training behavior of the WGAN is visualized in Fig-
ure 3c. We stress that this analysis only holds if the discrimi-
nator is trained with a Ô¨Åxed number of discriminator updates
(as it is usually done in practice). More careful training that
ensures that the discriminator is kept exactly optimal or
two-timescale training (Heusel et al., 2017) might be able
to ensure convergence for WGANs.Which Training Methods for GANs do actually Converge?
D (x)
xy
(a) Example with instance noise
 (b) Eigenvalues
Figure 4. Dirac-GAN with instance noise. While unregularized
GAN training is inherently unstable, instance noise can stabilize it:
(a) Near the Nash-equilibrium, the discriminator is pushed towards
the zero discriminator. (b) As we increase the noise level from
0tocritical , the real part of the eigenvalues at the equilibrium
point becomes negative and the absolute value of the imaginary
part becomes smaller. For noise levels bigger than critical all
eigenvalues are real-valued and GAN training hence behaves like
a normal optimization problem.
The convergence properties of WGANs were also consid-
ered by Nagarajan & Kolter (2017) who showed that even
for absolutely continuous densities and inÔ¨Ånitesimal learn-
ing rates, WGANs are not always locally convergent.
We also found that WGAN-GP (Gulrajani et al., 2017) does
not converge for the Dirac-GAN (Figure 3d). Please see the
supplementary material for details.3
3.2. Instance noise
A common technique to stabilize GANs is to add instance
noise (S√∏nderby et al., 2016; Arjovsky & Bottou, 2017), i.e.
independent Gaussian noise, to the data points. While the
original motivation was to make the probabilistic divergence
between data and generator distribution well-deÔ¨Åned for dis-
tributions that do not have common support, this does not
clarify the effects of instance noise on the training algorithm
itself and its ability to Ô¨Ånd a Nash-equilibrium. Interestingly,
however, it was recently shown (Nagarajan & Kolter, 2017)
that in the case of absolutely continuous distributions, gra-
dient descent based GAN optimization is - under suitable
assumptions - locally convergent.
Indeed, for the Dirac-GAN we have:
Lemma 3.2. When using Gaussian instance noise with stan-
dard deviation , the eigenvalues of the Jacobian of the
gradient vector Ô¨Åeld are given by
1=2=f00(0)2p
f00(0)24 f0(0)2: (6)
In particular, all eigenvalues of the Jacobian have negative
real-part at the Nash-equilibrium if f00(0)<0and>0.
Hence, simultaneous and alternating gradient descent are
both locally convergent for small enough learning rates.
3Despite these negative results, WGAN-GP has been success-
fully applied in practice (Gulrajani et al., 2017; Karras et al., 2017)
and we leave a theoretical analysis of these empirical results to
future research.Interestingly, Lemma 3.2 shows that there is a critical noise
level given by 2
critical =jf0(0)j=jf00(0)j. If the noise level
is smaller than the critical noise level, the eigenvalues of
the Jacobian have non-zero imaginary part which results
in a rotational component in the gradient vector Ô¨Åeld near
the equilibrium point. If the noise level is larger than the
critical noise level, all eigenvalues of the Jacobian become
real-valued and the rotational component in the gradient
vector Ô¨Åeld disappears. The optimization problem is best
behaved when we select =critical : in this case we can
even achieve quadratic convergence for h=jf0(0)j 1. The
effect of instance noise on the eigenvalues is visualized in
Figure 4b, which shows the traces of the two eigenvalues as
we increase from 0to2critical .
Figure 3f shows the training behavior of the GAN with
instance noise, showing that instance noise indeed creates a
strong radial component in the gradient vector Ô¨Åeld which
makes the training algorithm converge.
3.3. Zero-centered gradient penalties
Motivated by the success of instance noise to make the f-
divergence between two distributions well-deÔ¨Åned, Roth
et al. (2017) derived a local approximation to instance noise
that results in a zero-centered4gradient penalty for the dis-
criminator.
In our simple example, a penalty on the squared norm of the
gradients of the discriminator (no matter where) results in
the regularizer
R( ) =
2 2: (7)
This regularizer does not include the weighting terms con-
sidered by Roth et al. (2017). However, the same analysis
can also be applied to the regularizer with the additional
weighting, yielding almost exactly the same results (see
Section D.2 of the supplementary material).
Lemma 3.3. The eigenvalues of the Jacobian of the gra-
dient vector Ô¨Åeld for the gradient-regularized GAN at the
equilibrium point are given by
1=2= 
2r

2
4 f0(0)2: (8)
In particular, for 
 >0all eigenvalues have negative real
part. Hence, simultaneous and alternating gradient descent
are both locally convergent for small enough learning rates.
As for instance noise, there is a critical regularization pa-
rameter
critical = 2jf0(0)jthat results in a locally rotation
free vector Ô¨Åeld. A visualization of the training behavior of
the Dirac-GAN with gradient penalty is shown in Figure 3g.
Figure 3h illustrates the training behavior of the GAN with
4In contrast to the gradient regularizers used in WGAN-GP
and DRAGAN which are not zero-centered.Which Training Methods for GANs do actually Converge?
gradient penalty and critical regularization (CR). In particu-
lar, we see that near the Nash-equilibrium the vector Ô¨Åeld
does not have a rotational component anymore and hence
behaves like a normal optimization problem.
4. General convergence results
In Section 3 we analyzed the convergence properties of var-
ious regularization strategies for the Dirac-GAN. In this
section, we consider general GAN problems. First, we intro-
duce two simpliÔ¨Åed versions of the zero-centered gradient
penalty proposed by Roth et al. (2017). We then show that
these gradient penalties allow us to extend the convergence
proof by Nagarajan & Kolter (2017) to the case where the
generator and data distribution do not locally have the same
support.5As a result, our convergence proof for the regular-
ized training dynamics also holds for the more realistic case
where both the generator and data distributions may lie on
lower dimensional manifolds.
4.1. SimpliÔ¨Åed gradient penalties
Our analysis suggests that the main effect of the zero-
centered gradient penalties proposed by Roth et al. (2017)
on local stability is to penalize the discriminator for deviat-
ing from the Nash-equilibrium. The simplest way to achieve
this is to penalize the gradient on real data alone: when the
generator distribution produces the true data distribution
and the discriminator is equal to 0 on the data manifold, the
gradient penalty ensures that the discriminator cannot create
a non-zero gradient orthogonal to the data manifold without
suffering a loss in the GAN game.
This leads to the following regularization term:
R1( ) :=
2EpD(x)
krD (x)k2
: (9)
Note that this regularizer is a simpliÔ¨Åed version of to the
regularizer derived by Roth et al. (2017). However, our
regularizer does not contain the additional weighting terms
and penalizes the discriminator gradients only on the true
data distribution.
We also consider a similar regularization term given by
R2(; ) :=
2Ep(x)
krD (x)k2
(10)
where we penalize the discriminator gradients on the current
generator distribution instead of the true data distribution.
Note that on the Dirac-GAN from Section 2, both regulariz-
ers reduce to the gradient penalty from Section 3.3 whose
behavior is visualized in Figure 3g and Figure 3h.
5Assumption IV in Nagarajan & Kolter (2017)4.2. Convergence
In this section we present convergence results for the regular-
ized GAN-training dynamics for both regularization terms
R1( )andR2( )under some suitable assumptions.6
Let(; )denote an equilibrium point of the regularized
training dynamics. In our convergence analysis, we consider
the realizable case, i.e. we assume that there are generator
parameters that make the generator produce the true data
distribution:
Assumption I. We havep=pDandD (x) = 0 in
some local neighborhood of supppD.
Like Nagarajan & Kolter (2017), we assume that fsatisÔ¨Åes
the following property:
Assumption II. We havef0(0)6= 0andf00(0)<0.
An extension of our convergence proof for f(t) =t(as in
WGANs) can be found in the supplementary material.
The convergence proof is complicated by the fact that for
neural networks, there generally is not a single equilibrium
point (; ), but a submanifold of equivalent equilibria
corresponding to different parameterizations of the same
function. We therefore deÔ¨Åne the reparameterization mani-
foldsMGandMD. To this end, let
h( ) := EpD(x)
jD (x)j2+krxD (x)k2
: (11)
Thereparameterization manifolds are then deÔ¨Åned as
MG:=fjp=pDg MD:=f jh( ) = 0g:(12)
To prove local convergence, we have to assume some reg-
ularity properties for MGandMDnear the equilibrium
point. To state these assumptions, we need
g() := Ep(x)[r D (x)j = ]: (13)
Assumption III. There are-ballsB()andB( )
aroundand so thatMG\B()andMD\B( )
deÔ¨ÅneC1- manifolds. Moreover, the following holds:
(i)ifv2Rnis not in the tangent space of MDat ,
then@2
vh( )6= 0.
(ii)ifw2Rmis not in the tangent space of MGat,
then@wg()6= 0.
While formally similar, the two conditions in Assumption III
have very different meanings: the Ô¨Årst condition is a simple
regularity property that means that the geometry of MD
can be locally described by the second derivative of h. The
second condition implies that the discriminator is strong
6Our results also hold for any convex combination of R1and
R2and the regularizer with the additional weighting terms derived
by Roth et al. (2017). See the supplementary material for details.Which Training Methods for GANs do actually Converge?
enough so that it can detect any deviation from the equilib-
rium generator distribution. Indeed, this is the only point
where we assume that the class of representable discrimi-
nators is sufÔ¨Åciently expressive (and excludes, for example,
the trivial case D = 0for all ).
We are now ready to state our main convergence result. To
this end, consider the regularized gradient vector Ô¨Åeld
~vi(; ) := rL(; )
r L(; ) r Ri(; )
: (14)
Theorem 4.1. Assume Assumption I, II and III hold for
(; ). For small enough learning rates, simultaneous
and alternating gradient descent for ~v1and~v2are both
convergent toMGMDin a neighborhood of (; ).
Moreover, the rate of convergence is at least linear.
Theorem 4.1 shows that GAN training with our gradient
penalties is convergent when initialized sufÔ¨Åciently close
the equilibrium point. While this does not show that the
method is globally convergent, it at least shows that near the
equilibrium the method is well-behaved.
4.3. Stable equilibria for unregularized GAN training
As we have seen in Section 2, unregularized GAN training
does not always converge to the Nash-equilibrium. However,
this does not rule out the existence of stable equilibria for
every GAN architecture. In Section E of the supplementary
material, we identify two forms of stable equilibria that may
exist for unregularized GAN training ( Energy Solutions and
Full-Rank Solutions ). However, it is not yet clear under
what conditions such solutions exist for high dimensional
data distributions.
5. Experiments
2D-Problems Measuring convergence for GANs is hard
for high dimensional problems, because we lack an evalua-
tion metric that can reliably detect non-convergent behavior.
We therefore Ô¨Årst examine the behavior of the different reg-
ularizers on simple 2D examples where we can assess con-
vergence using an estimate of the Wasserstein-1-distance.
To this end, we run 5different training algorithms on 4dif-
ferent 2D-examples for 6different GAN architectures. For
each method, we try both stochastic gradient descent and
RMS-Prop with 4different learning rates. For the R1-,R2-
and WGAN-GP-regularizers we try 3different regulariza-
tion parameters. We train all methods for 50k iterations and
report the results for the best hyperparameter setup. Please
see the supplementary material for details.
The results are shown in Figure 5. We see that the R1- and
R2-regularizers perform similarly and they achieve slightly
better results than unregularized training or training with
(a) 2D Gaussian
 (b) Line segment
(c) Circle
 (d) Four line segments
Figure 5. Wasserstein-1-distance to true data distribution for 4dif-
ferent 2D-data-distributions, 6different architectures (small bars)
and5different training methods. Here, we abbreviate WGAN-
GP with 1and5discriminator update(s) per generator update as
WGP-1 and WGP-5.
WGAN-GP. In the supplementary material we show that the
R1- andR2-regularizers Ô¨Ånd solutions where the discrim-
inator is 0in a neighborhood of the true data distribution,
whereas unregularized training and WGAN-GP converge
toenergy solutions which we deÔ¨Åne in Section E.1 of the
supplementary material.
Imagenet To test how well the gradient penalties from
Section 4.1 perform on more complicated tasks, we train a
convolutional GAN consisting of ResNet-architectures (He
et al., 2016) for both the generator and discriminator on
the ILSVRC dataset (Russakovsky et al., 2015). While we
Ô¨Ånd that unregularized GAN training quickly leads to mode-
collapse on this architecture, our simple R1-regularizer en-
ables stable training. Some samples from the model after
35epochs of training and more details on the experimental
setup can be found in the supplementary material.
6. Conclusion
In this paper, we analyzed the stability of GAN training on
a simple yet prototypical example. Due to the simplicity of
the example, we were able to analyze the convergence prop-
erties of the training dynamics analytically and we showed
that (unregularized) gradient based GAN optimization is
not always locally convergent. Our Ô¨Åndings also show that
WGANs and WGAN-GP do not always lead to local con-
vergence whereas instance noise and zero-centered gradient
penalties do. Based on our analysis, we extended our results
to more general GANs and we proved local convergence for
simpliÔ¨Åed zero-centered gradient penalties under suitable
assumptions. In the future, we would like to extend our
theory to the non-realizable case and examine the effect of
Ô¨Ånite sampling sizes on the GAN training dynamics.Which Training Methods for GANs do actually Converge?
Acknowledgements
We would like to thank Vaishnavh Nagarajan and Kevin
Roth for insightful discussions. We also thank Vaishnavh
Nagarajan for giving helpful feedback on an early draft of
this manuscript. We thank NVIDIA for donating the GPUs
for the experiments presented in the supplementary material.
This work was supported by Microsoft Research through its
PhD Scholarship Programme.
References
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,
J., Devin, M., Ghemawat, S., Irving, G., Isard, M., Kud-
lur, M., Levenberg, J., Monga, R., Moore, S., Murray,
D. G., Steiner, B., Tucker, P. A., Vasudevan, V ., Warden,
P., Wicke, M., Yu, Y ., and Zheng, X. TensorÔ¨Çow: A
system for large-scale machine learning. In 12th USENIX
Symposium on Operating Systems Design and Implemen-
tation, OSDI 2016, Savannah, GA, USA, November 2-4,
2016. , pp. 265‚Äì283, 2016.
Arjovsky, M. and Bottou, L. Towards principled meth-
ods for training generative adversarial networks. CoRR ,
abs/1701.04862, 2017.
Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein
GAN. CoRR , abs/1701.07875, 2017.
Barratt, S. and Sharma, R. A note on the inception score.
arXiv preprint arXiv:1801.01973 , 2018.
Berthelot, D., Schumm, T., and Metz, L. BEGAN: bound-
ary equilibrium generative adversarial networks. CoRR ,
abs/1703.10717, 2017.
Bertsekas, D. P. Nonlinear programming . Athena scientiÔ¨Åc
Belmont, 1999.
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A. C., and Bengio,
Y . Generative adversarial nets. In Advances in Neural In-
formation Processing Systems 27: Annual Conference on
Neural Information Processing Systems 2014, December
8-13 2014, Montreal, Quebec, Canada , pp. 2672‚Äì2680,
2014.
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V ., and
Courville, A. C. Improved training of wasserstein gans. In
Advances in Neural Information Processing Systems 30:
Annual Conference on Neural Information Processing
Systems 2017, 4-9 December 2017, Long Beach, CA,
USA, pp. 5769‚Äì5779, 2017.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition ,
pp. 770‚Äì778, 2016.Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and
Hochreiter, S. Gans trained by a two time-scale update
rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems
2017, 4-9 December 2017, Long Beach, CA, USA , pp.
6629‚Äì6640, 2017.
Karras, T., Aila, T., Laine, S., and Lehtinen, J. Progres-
sive growing of gans for improved quality, stability, and
variation. arXiv preprint arXiv:1710.10196 , 2017.
Khalil, H. K. Nonlinear systems. Prentice-Hall, New Jersey ,
2(5):5‚Äì1, 1996.
Kodali, N., Abernethy, J. D., Hays, J., and Kira, Z. How to
train your DRAGAN. CoRR , abs/1705.07215, 2017.
Krizhevsky, A. and Hinton, G. Learning multiple layers of
features from tiny images. 2009.
Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face
attributes in the wild. In Proceedings of International
Conference on Computer Vision (ICCV) , 2015.
Mescheder, L. M., Nowozin, S., and Geiger, A. The numer-
ics of gans. In Advances in Neural Information Process-
ing Systems 30: Annual Conference on Neural Informa-
tion Processing Systems 2017, 4-9 December 2017, Long
Beach, CA, USA , pp. 1823‚Äì1833, 2017.
Nagarajan, V . and Kolter, J. Z. Gradient descent GAN
optimization is locally stable. In Advances in Neural
Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, 4-9
December 2017, Long Beach, CA, USA , pp. 5591‚Äì5600,
2017.
Nowozin, S., Cseke, B., and Tomioka, R. f-gan: Training
generative neural samplers using variational divergence
minimization. In Advances in Neural Information Pro-
cessing Systems 29: Annual Conference on Neural Infor-
mation Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain , pp. 271‚Äì279, 2016.
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,
DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,
A. Automatic differentiation in pytorch. 2017.
Radford, A., Metz, L., and Chintala, S. Unsupervised rep-
resentation learning with deep convolutional generative
adversarial networks. arXiv preprint arXiv:1511.06434 ,
2015.
Roth, K., Lucchi, A., Nowozin, S., and Hofmann, T. Stabi-
lizing training of generative adversarial networks throughWhich Training Methods for GANs do actually Converge?
regularization. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural In-
formation Processing Systems 2017, 4-9 December 2017,
Long Beach, CA, USA , pp. 2015‚Äì2025, 2017.
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,
M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale
Visual Recognition Challenge. International Journal of
Computer Vision (IJCV) , 115(3):211‚Äì252, 2015. doi:
10.1007/s11263-015-0816-y.
Salimans, T., Goodfellow, I. J., Zaremba, W., Cheung, V .,
Radford, A., and Chen, X. Improved techniques for
training gans. In Advances in Neural Information Pro-
cessing Systems 29: Annual Conference on Neural Infor-
mation Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain , pp. 2226‚Äì2234, 2016.
S√∏nderby, C. K., Caballero, J., Theis, L., Shi, W., and
Husz√°r, F. Amortised MAP inference for image super-
resolution. CoRR , abs/1610.04490, 2016.
Tieleman, T. and Hinton, G. Lecture 6.5-rmsprop: Divide
the gradient by a running average of its recent magnitude,
2012.
Yu, F., Zhang, Y ., Song, S., Seff, A., and Xiao, J. Lsun:
Construction of a large-scale image dataset using deep
learning with humans in the loop. arXiv preprint
arXiv:1506.03365 , 2015.
Zhao, J. J., Mathieu, M., and LeCun, Y . Energy-based
generative adversarial network. CoRR , abs/1609.03126,
2016.