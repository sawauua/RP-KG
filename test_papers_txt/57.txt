Theoretically Principled Trade-off between Robustness and Accuracy

Hongyang Zhang12Yaodong Yu3Jiantao Jiao4Eric P. Xing15Laurent El Ghaoui4Michael I. Jordan4
Abstract
We identify a trade-off between robustness and
accuracy that serves as a guiding principle in the
design of defenses against adversarial examples.
Although this problem has been widely studied
empirically, much remains unknown concerning
the theory underlying this trade-off. In this work,
we decompose the prediction error for adversarial
examples (robust error) as the sum of the natural
(classiﬁcation) error and boundary error, and pro-
vide a differentiable upper bound using the theory
of classiﬁcation-calibrated loss, which is shown to
be the tightest possible upper bound uniform over
all probability distributions and measurable pre-
dictors. Inspired by our theoretical analysis, we
also design a new defense method, TRADES, to
trade adversarial robustness off against accuracy.
Our proposed algorithm performs well experimen-
tally in real-world datasets. The methodology is
the foundation of our entry to the NeurIPS 2018
Adversarial Vision Challenge in which we won
the 1st place out of ~2,000 submissions, surpass-
ing the runner-up approach by 11.41% in terms
of mean `2perturbation distance.
1. Introduction
In response to the vulnerability of deep neural networks
to small perturbations around input data ( Szegedy et al. ,
2013 ), adversarial defenses have been an imperative object
of study in machine learning ( Huang et al. ,2017 ), computer
vision ( Song et al. ,2018 ;Xie et al. ,2017 ;Meng & Chen ,
2017 ), natural language processing ( Jia & Liang ,2017 ),
and many other domains. In machine learning, study of
adversarial defenses has led to signiﬁcant advances in under-
standing and defending against adversarial threat ( He et al. ,
2017 ). In computer vision and natural language process-
ing, adversarial defenses serve as indispensable building
1Carnegie Mellon University2Toyota Technological Insti-
tute at Chicago3University of Virginia4University of California,
Berkeley5Petuum Inc.. Correspondence to: Hongyang Zhang
<hongyanz@cs.cmu.edu>, Yaodong Yu <yy8ms@virginia.edu>.
Proceedings of the 36thInternational Conference on Machine
Learning , Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).blocks for a range of security-critical systems and appli-
cations, such as autonomous cars and speech recognition
authorization. The problem of adversarial defenses can be
stated as that of learning a classiﬁer with high test accuracy
on both natural and adversarial examples . The adversarial
example for a given labeled data (x,y)is a data point x0
that causes a classiﬁer cto output a different label on x0than
y, but is “imperceptibly similar” to x. Given the difﬁculty
of providing an operational deﬁnition of “imperceptible sim-
ilarity,” adversarial examples typically come in the form of
restricted attacks such as ✏-bounded perturbations ( Szegedy
et al.,2013 ), orunrestricted attacks such as adversarial ro-
tations, translations, and deformations ( Brown et al. ,2018 ;
Engstrom et al. ,2017 ;Gilmer et al. ,2018 ;Xiao et al. ,2018 ;
Alaifari et al. ,2019 ;Zhang et al. ,2019a ). The focus of this
work is the former setting, though our framework can be
generalized to the latter.
Despite a large literature devoted to improving the robust-
ness of deep-learning models, many fundamental questions
remain unresolved. One of the most important questions
is how to trade off adversarial robustness against natural
accuracy. Statistically, robustness can be be at odds with
accuracy when no assumptions are made on the data distri-
bution ( Tsipras et al. ,2019 ). This has led to an empirical
line of work on adversarial defense that incorporates var-
ious kinds of assumptions ( Su et al. ,2018 ;Kurakin et al. ,
2017 ). On the theoretical front, methods such as relaxation
based defenses (Kolter & Wong ,2018 ;Raghunathan et al. ,
2018a ) provide provable guarantees for adversarial robust-
ness. They, however, ignore the performance of classiﬁer
on the non-adversarial examples, and thus leave open the
theoretical treatment of the putative robustness/accuracy
trade-off.
The problem of adversarial defense becomes more challeng-
ing when computational issues are considered. For example,
the straightforward empirical risk minimization (ERM) for-
mulation of robust classiﬁcation involves minimizing the
robust 0-1 loss max x0:kx0 xk✏1{c(x0)6=y},a loss which
is NP-hard to optimize even if ✏=0in general. Hence, it
is natural to expect that some prior work on adversarial de-
fense replaced the 0-1 loss 1(·)with a surrogate loss ( Madry
et al.,2018 ;Kurakin et al. ,2017 ;Uesato et al. ,2018 ). How-
ever, there is little theoretical guarantee on the tightness of
this approximation.Theoretically Principled Trade-off between Robustness and Accuracy
	Figure 1. Left ﬁgure: decision boundary learned by natural train-
ing method. Right ﬁgure: decision boundary learned by our
adversarial training method, where the orange dotted line repre-
sents the decision boundary in the left ﬁgure. It shows that both
methods achieve zero natural training error, while our adversar-
ial training method achieves better robust training error than the
natural training method.
1.1. Our methodology and results
We begin with an example that illustrates the trade-off be-
tween accuracy and adversarial robustness in Section 2.4,a
phenomenon which has been demonstrated by Tsipras et al.
(2019 ), but without theoretical guarantees. We constructed
a toy example where the Bayes optimal classiﬁer achieves
natural error 0%androbust error 100% , while the trivial
all-one classiﬁer achieves both natural error androbust er-
ror50% (Table 1). Despite a large literature on the analysis
of robust error in terms of generalization ( Schmidt et al. ,
2018 ;Cullina et al. ,2018 ;Yin et al. ,2018 ) and computa-
tional complexity ( Bubeck et al. ,2018b ;a), the trade-off
between the natural error and the robust error has not been
a focus of theoretical study.
We show that the robust error can in general be bounded
tightly using two terms: one corresponds to the natural er-
rormeasured by a surrogate loss function, and the other
corresponds to how likely the input features are close to the
✏-extension of the decision boundary, termed as the bound-
ary error . We then minimize the differentiable upper bound.
Our theoretical analysis naturally leads to a new formulation
of adversarial defense which has several appealing proper-
ties; in particular, it inherits the beneﬁts of scalability to
large datasets exhibited by Tiny ImageNet, and the algo-
rithm achieves state-of-the-art performance on a range of
benchmarks while providing theoretical guarantees. For
example, while the defenses overviewed in ( Athalye et al. ,
2018 ) achieve robust accuracy no higher than ~ 47% under
white-box attacks, our method achieves robust accuracy as
high as ~ 57% in the same setting. The methodology is the
foundation of our entry to the NeurIPS 2018 Adversarial
Vision Challenge where we won ﬁrst place out of ~2,000
submissions, surpassing the runner-up approach by 11.41%
in terms of mean `2perturbation distance.1.2. Summary of contributions
Our work tackles the problem of trading accuracy off against
robustness and advances the state-of-the-art in multiple
ways.
•Theoretically, we characterize the trade-off between
accuracy and robustness for classiﬁcation problems
via decomposing the robust error as the sum of the
natural error and the boundary error. We provide differ-
entiable upper bounds on both terms using the theory
of classiﬁcation-calibrated loss, which are shown to be
the tightest upper bounds uniform over all probability
distributions and measurable predictors.
•Algorithmically, inspired by our theoretical analysis,
we propose a new formulation of adversarial defense,
TRADES, as optimizing a regularized surrogate loss.
The loss consists of two terms: the term of empirical
risk minimization encourages the algorithm to maxi-
mize the natural accuracy, while the regularization term
encourages the algorithm to push the decision bound-
ary away from the data, so as to improve adversarial
robustness (see Figure 1).
•Experimentally, we show that our proposed algorithm
outperforms state-of-the-art methods under both black-
box and white-box threat models. In particular, the
methodology won the ﬁnal round of the NeurIPS 2018
Adversarial Vision Challenge.
2. Preliminaries
We illustrate our methodology using the framework of bi-
nary classiﬁcation, but it can be generalized to other settings
as well.
2.1. Notation
We will use bold capital letters such as XandYto repre-
sent random vector, bold lower-case letters such as xandy
to represent realization of random vector, capital letters such
asXandYto represent random variable, and lower-case
letters such as xandyto represent realization of random
variable. Speciﬁcally, we denote by x2Xthe sample
instance, and by y2{ 1,+1}the label, where X✓Rd
indicates the instance space. sign (x)represents the sign
of scalar xwith sign (0) = +1 . Denote by f:X!R
thescore function which maps an instance to a conﬁdence
value associated with being positive. It can be parametrized,
e.g., by deep neural networks. The associated binary clas-
siﬁer is sign (f(·)). We will frequently use 1{event }, the
0-1 loss, to represent an indicator function that is 1if an
event happens and 0otherwise. For norms, we denote by
kxka generic norm. Examples of norms include kxk1,
the inﬁnity norm of vector x, and kxk2, the `2norm ofTheoretically Principled Trade-off between Robustness and Accuracy
vector x. We use B(x,✏)to represent a neighborhood of
x:{x02X:kx0 xk✏}. For a given score function
f, we denote by DB(f)the decision boundary of f; that
is, the set {x2X :f(x)=0 }. The set B(DB( f),✏)
denotes the neighborhood of the decision boundary of f:
{x2X:9x02B(x,✏)s.t.f(x)f(x0)0}. For a given
function  (u), we denote by  ⇤(v): =s u pu{uTv  (u)}
the conjugate function of  , by ⇤⇤the bi-conjugate, and
by  1the inverse function. We will frequently use  (·)to
indicate the surrogate of 0-1 loss.
2.2. Robust (classiﬁcation) error
In the setting of adversarial learning, we are given a set of
instances x1,. . . ,xn2Xand labels y1,. . . ,y n2{ 1,+1}.
We assume that the data are sampled from an unknown dis-
tribution (X,Y)⇠D. To characterize the robustness of a
score function f:X!R,Schmidt et al. (2018 );Cullina
et al. (2018 );Bubeck et al. (2018b ) deﬁned robust (classiﬁca-
tion) error under the threat model of bounded ✏perturbation:
Rrob(f): = E(X,Y)⇠D1{9X02B(X,✏)s.t.f(X0)Y
0}.This is in sharp contrast to the standard measure of
classiﬁer performance—the natural (classiﬁcation) error
Rnat(f): = E(X,Y)⇠D1{f(X)Y0}We note that the
two errors satisfy Rrob(f) R nat(f)for all f; the robust
error is equal to the natural error when ✏=0.
2.3. Boundary error
We introduce the boundary error deﬁned as Rbdy(f): =
E(X,Y)⇠D1{X2B(DB(f),✏),f(X)Y> 0}.We have
the following decomposition of Rrob(f):
Rrob(f)=Rnat(f)+Rbdy(f). (1)
2.4. Trade-off between natural and robust errors
Our study is motivated by the trade-off between natural and
robust errors. Tsipras et al. (2019 ) showed that training
robust models may lead to a reduction of standard accuracy.
To illustrate the phenomenon, we provide a toy example.
Example. Consider the case (X,Y)⇠D, where the
marginal distribution over the instance space is a uniform
distribution over [0,1], and for k=0,1,. . . ,d1
2✏ 1e,
⌘(x): =P r ( Y=1|X=x)
=(
0,x2[2k✏,(2k+ 1)✏),
1,x2((2k+ 1)✏,(2k+ 2)✏].(2)
See Figure 2for the visualization of ⌘(x). We consider two
classiﬁers: a) the Bayes optimal classiﬁer sign (2⌘(x) 1);
b) the all-one classiﬁer which always outputs “positive.”
Table 1displays the trade-off between natural and robust
errors: the minimal natural error is achieved by the Bayes	
!(#)	
0	
1	
%	
%	
1/2	
#	
1	Figure 2. Counterexample given by Eqn. ( 2).
Table 1. Comparisons of natural and robust errors of Bayes optimal
classiﬁer and all-one classiﬁer in example (2). The Bayes optimal
classiﬁer has the optimal natural error while the all-one classiﬁer
has the optimal robust error.
Bayes Optimal Classiﬁer All-One Classiﬁer
Rnat 0 (optimal) 1/2
Rbdy 1 0
Rrob 1 1/2 (optimal)
optimal classiﬁer with large robust error, while the optimal
robust error is achieved by the all-one classiﬁer with large
natural error.
Our goal. In practice, one may prefer to trade-off between
robustness and accuracy by introducing weights in ( 1) to
bias more towards the natural error or the boundary error.
Noting that both the natural error and the boundary error
involve 0-1 loss functions, our goal is to devise tight differ-
entiable upper bounds on both of these terms. Towards this
goal, we utilize the theory of classiﬁcation-calibrated loss.
2.5. Classiﬁcation-calibrated surrogate loss
Deﬁnition. Minimization of the 0-1 loss in the natural and
robust errors is computationally intractable and the demands
of computational efﬁciency have led researchers to focus
on minimization of a tractable surrogate loss ,R (f): =
E(X,Y)⇠D (f(X)Y). We then need to ﬁnd quantitative re-
lationships between the excess errors associated with  and
those associated with 0–1 loss. We make a weak assumption
on : it is classiﬁcation-calibrated (Bartlett et al. ,2006 ).
Formally, for ⌘2[0,1], deﬁne the conditional  -risk by
H(⌘): = i n f
↵2RC⌘(↵): = i n f
↵2R(⌘ (↵)+( 1  ⌘) ( ↵)),
and deﬁne H (⌘): =i n f ↵(2⌘ 1)0C⌘(↵). The
classiﬁcation-calibrated condition requires that imposing
the constraint that ↵has an inconsistent sign with the Bayes
decision rule sign (2⌘ 1)leads to a strictly larger  -risk:
Assumption 1 (Classiﬁcation-Calibrated Loss) .We assume
that the surrogate loss  is classiﬁcation-calibrated, mean-
ing that for any ⌘6=1/2,H (⌘)>H(⌘).
We argue that Assumption 1is indispensable for classiﬁ-
cation problems, since without it the Bayes optimal clas-Theoretically Principled Trade-off between Robustness and Accuracy
Table 2. Examples of classiﬁcation-calibrated loss  and associated
 -transform. Here  log(✓)=1
2(1 ✓)l o g2(1 ✓)+1
2(1 +
✓)l o g2(1 + ✓).
Loss  (↵)  (✓)
Hinge max{1 ↵,0} ✓
Sigmoid 1 tanh( ↵) ✓
Exponential exp(  ↵)1  p
1 ✓2
Logistic log2(1 + exp(  ↵))  log(✓)
siﬁer cannot be the minimizer of the  -risk. Examples of
classiﬁcation-calibrated loss include hinge loss, sigmoid
loss, exponential loss, logistic loss, and many others (see
Table 2).
Properties. Classiﬁcation-calibrated loss has many struc-
tural properties that one can exploit. We begin by intro-
ducing a functional transform of classiﬁcation-calibrated
loss which was proposed by Bartlett et al. (2006 ). De-
ﬁne the function  :[ 0,1]![0,1)by =e ⇤⇤, where
e (✓): = H  1+✓
2 
 H 1+✓
2 
. Indeed, the function  (✓)
is the largest convex lower bound on H  1+✓
2 
 H 1+✓
2 
.
The value H  1+✓
2 
 H 1+✓
2 
characterizes how close
the surrogate loss  is to the class of non-classiﬁcation-
calibrated losses.
Below we state useful properties of the  -transform. We
will frequently use the function  to bound Rrob(f) R⇤
nat.
Lemma 2.1 (Bartlett et al. (2006 )).Under Assumption
1, the function  has the following properties:  is non-
decreasing, continuous, convex on [0,1]and (0) = 0 .
3. Relating 0-1 loss to surrogate loss
In this section, we present our main theoretical contributions
for binary classiﬁcation and compare our results with prior
literature. Binary classiﬁcation problems have received sig-
niﬁcant attention in recent years as many competitions eval-
uate the performance of robust models on binary classiﬁca-
tion problems ( Brown et al. ,2018 ). We defer the discussion
of multi-class problems to Section 4.
3.1. Upper bound
Our analysis leads to a guarantee on the performance of
surrogate loss minimization. Intuitively, by Eqn. (1),
Rrob(f) R⇤
nat=Rnat(f) R⇤
nat+Rbdy(f)
  1(R (f) R⇤
 )+Rbdy(f), where the last inequality
holds because we choose  as a classiﬁcation-calibrated
loss ( Bartlett et al. ,2006 ). This leads to the following result.
Theorem 3.1. LetR (f): = E (f(X)Y)and R⇤
 :=
min fR (f). Under Assumption 1, for any non-negative
loss function  such that  (0) 1, any measurable f:
X!R, any probability distribution on X ⇥{± 1}, andany >0, we have1
Rrob(f) R⇤
nat
  1(R (f) R⇤
 )+Pr[ X2B(DB( f),✏),f(X)Y>0]
  1(R (f) R⇤
 )+Emax
X02B(X,✏) (f(X0)f(X)/ ).
Quantity governing model robustness. Our result pro-
vides a formal justiﬁcation for the existence of adversar-
ial examples: learning models are vulnerable to small
adversarial attacks because the probability that data lie
around the decision boundary of the model, Pr[X2
B(DB( f),✏),f(X)Y> 0], is large. As a result, small
perturbations may move the data point to the wrong side
of the decision boundary, leading to weak robustness of
classiﬁcation models.
3.2. Lower bound
We now establish a lower bound on Rrob(f) R⇤
nat. Our
lower bound matches our analysis of the upper bound in
Section 3.1up to an arbitrarily small constant.
Theorem 3.2. Suppose that |X|  2. Under Assumption
1, for any non-negative loss function  such that  (x)!0
asx!+1, any ⇠>0, and any ✓2[0,1], there exists
a probability distribution on X ⇥{± 1}, a function f:
Rd!R, and a regularization parameter  >0such that
Rrob(f) R⇤
nat=✓and
 ⇣
✓ Emax
X02B(X,✏) (f(X0)f(X)/ )⌘
R  (f) R⇤
 
 ✓
✓ Emax
X02B(X,✏) (f(X0)f(X)/ )◆
+⇠.
Theorem 3.2demonstrates that in the presence of extra
conditions on the loss function, i.e., lim x!+1 (x)=0 ,
the upper bound in Section 3.1is tight. The condition holds
for all the losses in Table 2.
4. Algorithmic Design for Defenses
Optimization. Theorems 3.1and3.2shed light on algorith-
mic designs of adversarial defenses. In order to minimize
Rrob(f) R⇤
nat, the theorems suggest minimizing2
min
fEn
 (f(X)Y)|{z}
for accuracy+ max
X02B(X,✏) (f(X)f(X0)/ )
| {z }
regularization for robustnesso
.
(3)
1We study the population form of the risk functions, and
mention that by incorporating the generalization theory for
classiﬁcation-calibrated losses ( Bartlett et al. ,2006 ) one can ex-
tend the analysis to ﬁnite samples. We leave this analysis for future
research.
2For simplicity of implementation, we do not use the function
  1and rely on  to approximately reﬂect the effect of   1, the
trade-off between the natural error and the boundary error, and the
tight approximation of the boundary error using the corresponding
surrogate loss function.Theoretically Principled Trade-off between Robustness and Accuracy
We name our method TRADES (TRadeoff-inspired Adver-
sarial DEfense via Surrogate-loss minimization).
Intuition behind the optimization. Problem (3)captures
the trade-off between the natural and robust errors: the ﬁrst
term in (3)encourages the natural error to be optimized by
minimizing the “difference” between f(X)andY, while
the second regularization term encourages the output to be
smooth, that is, it pushes the decision boundary of classiﬁer
away from the sample instances via minimizing the “dif-
ference” between the prediction of natural example f(X)
and that of adversarial example f(X0). This is conceptually
consistent with the argument that smoothness is an indis-
pensable property of robust models ( Cisse et al. ,2017 ). The
tuning parameter  plays a critical role on balancing the
importance of natural and robust errors. To see how the  
affects the solution in the example of Section 2.4, problem
(3)tends to the Bayes optimal classiﬁer when  !+1,
and tends to the all-one classiﬁer when  !0.
Comparisons with prior work. We compare our approach
with several related lines of research in the prior litera-
ture. One of the best known algorithms for adversarial
defense is based on robust optimization (Madry et al. ,2018 ;
Kolter & Wong ,2018 ;Wong et al. ,2018 ;Raghunathan et al. ,
2018a ;b). Most results in this direction involve algorithms
that approximately minimize
min
fE⇢
max
X02B(X,✏) (f(X0)Y) 
, (4)
where the objective function in problem (4)serves as an up-
per bound of the robust error Rrob(f). In complex problem
domains, however, this objective function might not be tight
as an upper bound of the robust error, and may not capture
the trade-off between natural and robust errors.
A related line of research is adversarial training by regular-
ization ( Kurakin et al. ,2017 ;Ross & Doshi-Velez ,2017 ;
Zheng et al. ,2016 ). There are several key differences
between the results in this paper and those of ( Kurakin
et al.,2017 ;Ross & Doshi-Velez ,2017 ;Zheng et al. ,2016 ).
Firstly, the optimization formulations are different. In the
previous works, the regularization term either measures the
“difference” between f(X0)andY(Kurakin et al. ,2017 ),
or its gradient ( Ross & Doshi-Velez ,2017 ). In contrast,
our regularization term measures the “difference” between
f(X)andf(X0). While Zheng et al. (2016 ) generated the
adversarial example X0by adding random Gaussian noise
toX, our method simulates the adversarial example by solv-
ing the inner maximization problem in Eqn. (3). Secondly,
we note that the losses in ( Kurakin et al. ,2017 ;Ross &
Doshi-Velez ,2017 ;Zheng et al. ,2016 ) lack of theoretical
guarantees. Our loss, with the presence of the second term
in problem (3), makes our theoretical analysis signiﬁcantly
more subtle. Moreover, our algorithm takes the same com-
putational resources as ( Kurakin et al. ,2017 ), which makesAlgorithm 1 Adversarial training by TRADES
input Step sizes ⌘1and⌘2, batch size m, number of iter-
ations Kin inner optimization, network architecture
parametrized by ✓
output Robust network f✓
1:Randomly initialize network f✓, or initialize network
with pre-trained conﬁguration
2:repeat
3: Read mini-batch B={x1,. . . ,xm}from training set
4: fori=1,. . . ,m (in parallel) do
5: x0
i xi+0.001·N(0,I), where N(0,I)is the
Gaussian distribution with zero mean and identity
variance
6: fork=1,. . . ,K do
7: x0
i ⇧B(xi,✏)(⌘1sign (rx0
iL(f✓(xi),f✓(x0
i)))+
x0
i), where ⇧is the projection operator
8: end for
9: end for
10: ✓ ✓ ⌘2Pm
i=1r✓[L(f✓(xi),yi)+
L(f✓(xi),f✓(x0
i))/ ]/m
11:until training converged
our method scalable to large-scale datasets. We defer the
experimental comparisons of various regularization based
methods to Table 5.
Heuristic algorithm. In response to the optimization for-
mulation (3), we use two heuristics to achieve more general
defenses: a) extending to multi-class problems by involv-
ing multi-class calibrated loss; b) approximately solving
the minimax problem via alternating gradient descent. For
multi-class problems, a surrogate loss is calibrated if mini-
mizers of the surrogate risk are also minimizers of the 0-1
risk ( Pires & Szepesvári ,2016 ). Examples of multi-class
calibrated loss include cross-entropy loss. Algorithmically,
we extend problem (3)to the case of multi-class classiﬁca-
tions by replacing  with a multi-class calibrated loss L(·,·):
min
fE⇢
L(f(X),Y) + max
X02B(X,✏)L(f(X),f(X0))/  
,
(5)
where f(X)is the output vector of learning model (with
softmax operator in the top layer for the cross-entropy loss
L(·,·)),Yis the label-indicator vector, and  >0is the
regularization parameter. The pseudocode of adversarial
training procedure, which aims at minimizing the empirical
form of problem ( 5), is displayed in Algorithm 1.
The key ingredient of the algorithm is to approximately
solve the linearization of inner maximization in problem (5)
by the projected gradient descent (see Step 7). We note that
xiis a global minimizer with zero gradient to the objective
function g(x0): = L(f(xi),f(x0))in the inner problem.
Therefore, we initialize x0
iby adding a small, random per-
turbation around xiin Step 5 to start the inner optimizer.Theoretically Principled Trade-off between Robustness and Accuracy
Table 3. Theoretical veriﬁcation on the optimality of Theorem 3.1.
 Arob(f)( % ) R (f)  =   RHS  LHS
2.0 99.43 0.0006728 0.006708
3.0 99.41 0.0004067 0.005914
4.0 99.37 0.0003746 0.006757
5.0 99.34 0.0003430 0.005860
More exhaustive approximations of the inner maximization
problem in terms of either optimization formulations or
solvers would lead to better defense performance.
5. Experimental Results
In this section, we verify the effectiveness of TRADES
by numerical experiments. We denote by Arob(f)=
1 R rob(f)the robust accuracy, and by Anat(f)=
1 R nat(f)the natural accuracy on test dataset. We release
our code and trained models at https://github.com/
yaodongyu/TRADES .
5.1. Optimality of Theorem 3.1
We verify the tightness of the established upper bound in
Theorem 3.1for binary classiﬁcation problem on MNIST
dataset. The negative examples are ‘1’ and the positive
examples are ‘3’. Here we use a Convolutional Neural
Network (CNN) with two convolutional layers, followed
by two fully-connected layers. The output size of the last
layer is 1. To learn the robust classiﬁer, we minimize the
regularized surrogate loss in Eqn. (3), and use the hinge
loss in Table 2as the surrogate loss  , where the associated
 -transform is  (✓)=✓.
To verify the tightness of our upper bound, we calculate the
left hand side in Theorem 3.1, i.e.,
 LHS=Rrob(f) R⇤
nat,
and the right hand side, i.e.,
 RHS=(R (f) R⇤
 )+Emax
X02B(X,✏) (f(X0)f(X)/ ).
As we cannot have access to the unknown distribution D,
we approximate the above expectation terms by test dataset.
We ﬁrst use natural training method to train a classiﬁer so
as to approximately estimate R⇤
natandR⇤
 , where we ﬁnd
that the naturally trained classiﬁer can achieve natural error
R⇤
nat= 0% , and loss value R⇤
 =0.0for the binary classi-
ﬁcation problem. Next, we optimize problem (3)to train a
robust classiﬁer f. We take perturbation ✏=0.1, number of
iterations K= 20 and run 30epochs on the training dataset.
Finally, to approximate the second term in  RHS, we use
FGSMk(white-box) attack (a.k.a. PGD attack) ( Kurakin
et al. ,2017 ) with 20iterations to approximately calculate
the worst-case perturbed data X0.
The results in Table 3show the tightness of our upper bound
in Theorem 3.1. It shows that the differences between  RHS
and LHSunder various  ’s are very small.5.2. Sensitivity of regularization hyperparameter  
The regularization parameter  is an important hyperparame-
ter in our proposed method. We show how the regularization
parameter affects the performance of our robust classiﬁers
by numerical experiments on two datasets, MNIST and CI-
FAR10. For both datasets, we minimize the loss in Eqn. (5)
to learn robust classiﬁers for multi-class problems, where
we choose Las the cross-entropy loss.
MNIST setup. We use the CNN which has two convolu-
tional layers, followed by two fully-connected layers. The
output size of the last layer is 10. We set perturbation
✏=0.1, perturbation step size ⌘1=0.01, number of itera-
tions K= 20 , learning rate ⌘2=0.01, batch size m= 128 ,
and run 50epochs on the training dataset. To evaluate the
robust error, we apply FGSMk(white-box) attack with 40
iterations and 0.005step size. The results are in Table 4.
CIFAR10 setup. We apply ResNet-18 ( He et al. ,2016 ) for
classiﬁcation. The output size of the last layer is 10. We set
perturbation ✏=0.031, perturbation step size ⌘1=0.007,
number of iterations K= 10 , learning rate ⌘2=0.1, batch
sizem= 128 , and run 100epochs on the training dataset.
To evaluate the robust error, we apply FGSMk(white-box)
attack with 20iterations and the step size is 0.003. The
results are in Table 4.
We observe that as the regularization parameter 1/ in-
creases, the natural accuracy Anat(f)decreases while the
robust accuracy Arob(f)increases, which veriﬁes our the-
ory on the trade-off between robustness and accuracy. Note
that for MNIST dataset, the natural accuracy does not de-
crease too much as the regularization term 1/ increases,
which is different from the results of CIFAR10. This is
probably because the classiﬁcation task for MNIST is easier.
Meanwhile, our proposed method is not very sensitive to the
choice of  . Empirically, when we set the hyperparameter
1/ in[1,10], our method is able to learn classiﬁers with
both high robustness and high accuracy. We will set 1/ as
either 1 or 6 in the following experiments.
5.3. Adversarial defenses under various attacks
Previously, Athalye et al. (2018 ) showed that 7 defenses in
ICLR 2018 which relied on obfuscated gradients may easily
break down. In this section, we verify the effectiveness of
our method with the same experimental setup under both
white-box and black-box threat models.
MNIST setup. We use the CNN architecture in ( Carlini &
Wagner ,2017 ) with four convolutional layers, followed by
three fully-connected layers. We set perturbation ✏=0.3,
perturbation step size ⌘1=0.01, number of iterations K=
40, learning rate ⌘2=0.01, batch size m= 128 , and run
100epochs on the training dataset.
CIFAR10 setup. We use the same neural network architec-
ture as ( Madry et al. ,2018 ), i.e., the wide residual networkTheoretically Principled Trade-off between Robustness and Accuracy
Table 4. Sensitivity of regularization hyperparameter  on MNIST and CIFAR10 datasets.
1/ Arob(f)( % ) on MNIST Anat(f)( % ) on MNIST Arob(f)( % ) on CIFAR10 Anat(f)( % ) on CIFAR10
1.0 94.75 ±0.0712 99.28 ±0.0125 44.68 ±0.3088 87.01 ±0.2819
2.0 95.45 ±0.0883 99.29 ±0.0262 48.22 ±0.0740 85.22 ±0.0543
3.0 95.57 ±0.0262 99.24 ±0.0216 49.67 ±0.3179 83.82 ±0.4050
4.0 95.65 ±0.0340 99.16 ±0.0205 50.25 ±0.1883 82.90 ±0.2217
5.0 95.65 ±0.1851 99.16 ±0.0403 50.64 ±0.3336 81.72 ±0.0286
WRN-34-10 ( Zagoruyko & Komodakis ,2016 ). We set per-
turbation ✏=0.031, perturbation step size ⌘1=0.007,
number of iterations K= 10 , learning rate ⌘2=0.1, batch
sizem= 128 , and run 100epochs on the training dataset.
5.3.1. W HITE -BOX ATTACKS
We summarize our results in Table 5together with the re-
sults from ( Athalye et al. ,2018 ). We also implement meth-
ods in ( Zheng et al. ,2016 ;Kurakin et al. ,2017 ;Ross &
Doshi-Velez ,2017 ) on the CIFAR10 dataset as they are also
regularization based methods. For MNIST dataset, we ap-
ply FGSMk(white-box) attack with 40iterations and the
step size is 0.01. For CIFAR10 dataset, we apply FGSMk
(white-box) attack with 20iterations and the step size is
0.003, under which the defense model in ( Madry et al. ,
2018 ) achieves 47.04% robust accuracy. Table 5shows that
our proposed defense method can signiﬁcantly improve the
robust accuracy of models, which is able to achieve robust
accuracy as high as 56.61%. We also evaluate our robust
model on MNIST dataset under the same threat model as
in (Samangouei et al. ,2018 ) (C&W white-box attack Carlini
& Wagner (2017 )), and the robust accuracy is 99.46%. See
appendix for detailed information of models in Table 5.
5.3.2. B LACK -BOX ATTACKS
We verify the robustness of our models under black-box at-
tacks. We ﬁrst train models without using adversarial train-
ing on the MNIST and CIFAR10 datasets. We use the same
network architectures that are speciﬁed in the beginning of
this section, i.e., the CNN architecture in ( Carlini & Wag-
ner,2017 ) and the WRN-34-10 architecture in ( Zagoruyko
& Komodakis ,2016 ). We denote these models by natu-
rally trained models ( Natural ). The accuracy of the natu-
rally trained CNN model is 99.50% on the MNIST dataset.
The accuracy of the naturally trained WRN-34-10 model is
95.29% on the CIFAR10 dataset. We also implement the
method proposed in ( Madry et al. ,2018 ) on both datasets.
We denote these models by Madry’s models ( Madry ). The
accuracy of Madry et al. (2018 )’s CNN model is 99.36% on
the MNIST dataset. The accuracy of Madry et al. (2018 )’s
WRN-34-10 model is 85.49% on the CIFAR10 dataset.
For both datasets, we use FGSMk(black-box) method to
attack various defense models. For MNIST dataset, we set
perturbation ✏=0.3and apply FGSMk(black-box) attack
with 40iterations and the step size is 0.01. For CIFAR10dataset, we set ✏=0.031and apply FGSMk(black-box)
attack with 20iterations and the step size is 0.003. Note that
the setup is the same as the setup speciﬁed in Section 5.3.1 .
We summarize our results in Table 6and Table 7. In both
tables, we use two source models (noted in the parentheses)
to generate adversarial perturbations: we compute the per-
turbation directions according to the gradients of the source
models on the input images. It shows that our models are
more robust against black-box attacks transfered from nat-
urally trained models and Madry et al. (2018 )’s models.
Moreover, our models can generate stronger adversarial
examples for black-box attacks compared with naturally
trained models and Madry et al. (2018 )’s models.
5.4. Case study: NeurIPS 2018 Adversarial Vision
Challenge
Competition settings. In the adversarial competition, the
adversarial attacks and defenses are under the black-box
setting. The dataset in this competition is Tiny ImageNet,
which consists of 550,000 data (with our data augmentation)
and 200 classes. The robust models only return label pre-
dictions instead of explicit gradients and conﬁdence scores.
The task for robust models is to defend against adversarial
examples that are generated by the top-5 submissions in the
un-targeted attack track. The score for each defense model
is evaluated by the smallest perturbation distance that makes
the defense model fail to output correct labels.
Competition results. The methodology in this paper was
applied to the competition, where our entry ranked the 1st
place. We implemented our method to train ResNet models.
We report the mean `2perturbation distance of the top-6
entries in Figure 3. It shows that our method outperforms
other approaches with a large margin. In particular, we
surpass the runner-up submission by 11.41% in terms of
mean `2perturbation distance.
6. Conclusions
In this paper, we study the problem of adversarial defenses
against structural perturbations around input data. We focus
on the trade-off between robustness and accuracy, and show
an upper bound on the gap between robust error and optimal
natural error. Our result advances the state-of-the-art work
and matches the lower bound in the worst-case scenario.
The bounds motivate us to minimize a new form of regu-
larized surrogate loss, TRADES, for adversarial training.Theoretically Principled Trade-off between Robustness and Accuracy
Table 5. Comparisons of TRADES with prior defense models under white-box attacks.
Defense Defense type Under which attack Dataset Distance Anat(f)Arob(f)
Buckman et al. (2018 ) gradient mask Athalye et al. (2018 )CIFAR10 0.031(`1) - 0%
Ma et al. (2018 ) gradient mask Athalye et al. (2018 )CIFAR10 0.031(`1) - 5%
Dhillon et al. (2018 ) gradient mask Athalye et al. (2018 )CIFAR10 0.031(`1) - 0%
Song et al. (2018 ) gradient mask Athalye et al. (2018 )CIFAR10 0.031(`1) - 9%
Na et al. (2017 ) gradient mask Athalye et al. (2018 )CIFAR10 0.015(`1) - 15%
Wong et al. (2018 ) robust opt. FGSM20(PGD) CIFAR10 0.031(`1)27.07% 23.54%
Madry et al. (2018 ) robust opt. FGSM20(PGD) CIFAR10 0.031(`1)87.30% 47.04%
Zheng et al. (2016 ) regularization FGSM20(PGD) CIFAR10 0.031(`1)94.64% 0.15%
Kurakin et al. (2017 ) regularization FGSM20(PGD) CIFAR10 0.031(`1)85.25% 45.89%
Ross & Doshi-Velez (2017 )regularization FGSM20(PGD) CIFAR10 0.031(`1)95.34% 0%
TRADES ( 1/ =1.0) regularization FGSM20(PGD) CIFAR10 0.031(`1)88.64% 49.14%
TRADES ( 1/ =6.0) regularization FGSM20(PGD) CIFAR10 0.031(`1)84.92% 56.61%
TRADES ( 1/ =1.0) regularization DeepFool ( `1) CIFAR10 0.031(`1)88.64% 59.10%
TRADES ( 1/ =6.0) regularization DeepFool ( `1) CIFAR10 0.031(`1)84.92% 61.38%
TRADES ( 1/ =1.0) regularization LBFGSAttack CIFAR10 0.031(`1)88.64% 84.41%
TRADES ( 1/ =6.0) regularization LBFGSAttack CIFAR10 0.031(`1)84.92% 81.58%
TRADES ( 1/ =1.0) regularization MI-FGSM CIFAR10 0.031(`1)88.64% 51.26%
TRADES ( 1/ =6.0) regularization MI-FGSM CIFAR10 0.031(`1)84.92% 57.95%
TRADES ( 1/ =1.0) regularization C&W CIFAR10 0.031(`1)88.64% 84.03%
TRADES ( 1/ =6.0) regularization C&W CIFAR10 0.031(`1)84.92% 81.24%
Samangouei et al. (2018 ) gradient mask Athalye et al. (2018 )MNIST 0.005(`2) - 55%
Madry et al. (2018 ) robust opt. FGSM40(PGD) MNIST 0.3(`1) 99.36% 96.01%
TRADES ( 1/ =6.0) regularization FGSM40(PGD) MNIST 0.3(`1) 99.48% 96.07%
TRADES ( 1/ =6.0) regularization C&W MNIST 0.005(`2)99.48% 99.46%
Table 6. Comparisons of TRADES with prior defenses under black-
box FGSM40attack on the MNIST dataset. The models inside
parentheses are source models which provide gradients to adver-
sarial attackers. We provide the average cross-entropy loss value
L(f(X),Y)of each defense model in the bracket. The defense
model ‘Madry’ is the same model as in the antepenultimate line of
Table 5. The defense model ‘TRADES’ is the same model as in
the penultimate line of Table 5.
Defense Model Robust Accuracy Arob(f)
Madry 97.43% [0.0078484] (Natural)
TRADES 97.63% [0.0075324] (Natural)
Madry 97.38% [0.0084962] (Ours)
TRADES 97.66% [0.0073532] (Madry)
Table 7. Comparisons of TRADES with prior defenses under black-
box FGSM20attack on the CIFAR10 dataset. The models inside
parentheses are source models which provide gradients to adver-
sarial attackers. We provide the average cross-entropy loss value
of each defense model in the bracket. The defense model ‘Madry’
is implemented based on ( Madry et al. ,2018 ), and the defense
model ‘TRADES’ is the same model as in the 11th line of Table 5.
Defense Model Robust Accuracy Arob(f)
Madry 84.39% [0.0519784] (Natural)
TRADES 87.60% [0.0380258] (Natural)
Madry 66.00% [0.1252672] (Ours)
TRADES 70.14 % [0.0885364] (Madry)1st	(TRADES)2.2562nd2.0253rd1.6374th1.5855th1.4766th1.4012.2562.0251.6371.5851.4761.401
00.511.522.5
1st	(TRADES)2nd3rd4th5th6thFigure 3. Top-6 results (out of ~2,000 submissions) in the NeurIPS
2018 Adversarial Vision Challenge. The vertical axis represents
the mean `2perturbation distance that makes robust models fail to
output correct labels.
Experiments on real datasets and adversarial competition
demonstrate the effectiveness of our proposed algorithms.
It would be interesting to combine our methods with other
related line of research on adversarial defenses, e.g., feature
denoising technique ( Xie et al. ,2018 ) and network archi-
tecture design ( Cisse et al. ,2017 ), to achieve more robust
learning systems.
Acknowledgements. We thank Maria-Florina Balcan and
Avrim Blum for valuable discussions. Part of this work
was done while H. Z. was visiting Simons Institute for the
Theory of Computing, and Y. Y. was an intern at Petuum.Theoretically Principled Trade-off between Robustness and Accuracy
References
Alaifari, R., Alberti, G. S., and Gauksson, T. ADef: an
iterative algorithm to construct adversarial deformations.
InInternational Conference on Learning Representations ,
2019.
Athalye, A., Carlini, N., and Wagner, D. Obfuscated gra-
dients give a false sense of security: Circumventing de-
fenses to adversarial examples. In International Confer-
ence on Machine Learning , 2018.
Barthe, F. Extremal properties of central half-spaces for
product measures. Journal of Functional Analysis , 182
(1):81–107, 2001.
Bartlett, P. L., Jordan, M. I., and McAuliffe, J. D. Convexity,
classiﬁcation, and risk bounds. Journal of the American
Statistical Association , 101(473):138–156, 2006.
Brendel, W., Rauber, J., and Bethge, M. Decision-based
adversarial attacks: Reliable attacks against black-box
machine learning models. In International Conference
on Learning Representations , 2018.
Brown, T. B., Carlini, N., Zhang, C., Olsson, C., Christiano,
P., and Goodfellow, I. Unrestricted adversarial examples.
arXiv preprint arXiv:1809.08352 , 2018.
Bubeck, S., Lee, Y. T., Price, E., and Razenshteyn, I. Ad-
versarial examples from cryptographic pseudo-random
generators. arXiv preprint arXiv:1811.06418 , 2018a.
Bubeck, S., Price, E., and Razenshteyn, I. Adversarial
examples from computational constraints. arXiv preprint
arXiv:1805.10204 , 2018b.
Buckman, J., Roy, A., Raffel, C., and Goodfellow, I. Ther-
mometer encoding: One hot way to resist adversarial
examples. In International Conference on Learning Rep-
resentations , 2018.
Carlini, N. and Wagner, D. Towards evaluating the robust-
ness of neural networks. In IEEE Symposium on Security
and Privacy , pp. 39–57, 2017.
Cisse, M., Bojanowski, P., Grave, E., Dauphin, Y., and
Usunier, N. Parseval networks: Improving robustness
to adversarial examples. In International Conference on
Machine Learning , 2017.
Cullina, D., Bhagoji, A. N., and Mittal, P. PAC-learning
in the presence of adversaries. In Advances in Neural
Information Processing Systems , pp. 228–239, 2018.
Dhillon, G. S., Azizzadenesheli, K., Lipton, Z. C., Bern-
stein, J., Kossaiﬁ, J., Khanna, A., and Anandkumar, A.
Stochastic activation pruning for robust adversarial de-
fense. arXiv preprint arXiv:1803.01442 , 2018.Dong, Y., Liao, F., Pang, T., Su, H., Zhu, J., Hu, X., and Li,
J. Boosting adversarial attacks with momentum. In IEEE
Conference on Computer Vision and Pattern Recognition ,
pp. 9185–9193, 2018.
Engstrom, L., Tran, B., Tsipras, D., Schmidt, L., and
Madry, A. A rotation and a translation sufﬁce: Fool-
ing CNNs with simple transformations. arXiv preprint
arXiv:1712.02779 , 2017.
Engstrom, L., Ilyas, A., and Athalye, A. Evaluating and
understanding the robustness of adversarial logit pairing.
arXiv preprint arXiv:1807.10272 , 2018.
Fawzi, A., Fawzi, H., and Fawzi, O. Adversarial vulnerabil-
ity for any classiﬁer. In Advances in Neural Information
Processing Systems , pp. 1186–1195, 2018.
Gilmer, J., Adams, R. P., Goodfellow, I., Andersen, D., and
Dahl, G. E. Motivating the rules of the game for adversar-
ial example research. arXiv preprint arXiv:1807.06732 ,
2018.
Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining
and harnessing adversarial examples. In International
Conference on Learning Representations , 2015.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual
learning for image recognition. In IEEE conference on
computer vision and pattern recognition , pp. 770–778,
2016.
He, W., Wei, J., Chen, X., Carlini, N., and Song, D. Adver-
sarial example defenses: Ensembles of weak defenses are
not strong. arXiv preprint arXiv:1706.04701 , 2017.
Huang, R., Xu, B., Schuurmans, D., and Szepesvári,
C. Learning with a strong adversary. arXiv preprint
arXiv:1511.03034 , 2015.
Huang, S., Papernot, N., Goodfellow, I., Duan, Y., and
Abbeel, P. Adversarial attacks on neural network policies.
arXiv preprint arXiv:1702.02284 , 2017.
Jia, R. and Liang, P. Adversarial examples for evaluating
reading comprehension systems. In Empirical Methods
in Natural Language Processing , 2017.
Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial
logit pairing. arXiv preprint arXiv:1803.06373 , 2018.
Kolter, J. Z. and Wong, E. Provable defenses against adver-
sarial examples via the convex outer adversarial polytope.
InInternational Conference on Machine Learning , 2018.
Kurakin, A., Goodfellow, I., and Bengio, S. Adversarial
machine learning at scale. In International Conference
on Learning Representations , 2017.Theoretically Principled Trade-off between Robustness and Accuracy
Ma, X., Li, B., Wang, Y., Erfani, S. M., Wijewickrema, S.,
Houle, M. E., Schoenebeck, G., Song, D., and Bailey, J.
Characterizing adversarial subspaces using local intrinsic
dimensionality. arXiv preprint arXiv:1801.02613 , 2018.
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and
Vladu, A. Towards deep learning models resistant to
adversarial attacks. In International Conference on Learn-
ing Representations , 2018.
Meng, D. and Chen, H. Magnet: a two-pronged defense
against adversarial examples. In ACM SIGSAC Confer-
ence on Computer and Communications Security , pp.
135–147, 2017.
Moosavi-Dezfooli, S.-M., Fawzi, A., and Frossard, P. Deep-
fool: a simple and accurate method to fool deep neural
networks. In IEEE Conference on Computer Vision and
Pattern Recognition , pp. 2574–2582, 2016.
Na, T., Ko, J. H., and Mukhopadhyay, S. Cascade adver-
sarial machine learning regularized with a uniﬁed embed-
ding. arXiv preprint arXiv:1708.02582 , 2017.
Pires, B. Á. and Szepesvári, C. Multiclass classiﬁcation
calibration functions. arXiv preprint arXiv:1609.06385 ,
2016.
Raghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed
defenses against adversarial examples. In International
Conference on Learning Representations , 2018a.
Raghunathan, A., Steinhardt, J., and Liang, P. S. Semidef-
inite relaxations for certifying robustness to adversarial
examples. In Advances in Neural Information Processing
Systems , pp. 10899–10909, 2018b.
Rauber, J., Brendel, W., and Bethge, M. Foolbox v0. 8.0: A
python toolbox to benchmark the robustness of machine
learning models. arXiv preprint arXiv:1707.04131 , 2017.
Ross, A. S. and Doshi-Velez, F. Improving the adversarial
robustness and interpretability of deep neural networks
by regularizing their input gradients. arXiv preprint
arXiv:1711.09404 , 2017.
Samangouei, P., Kabkab, M., and Chellappa, R. Defense-
gan: Protecting classiﬁers against adversarial attacks us-
ing generative models. arXiv preprint arXiv:1805.06605 ,
2018.
Schmidt, L., Santurkar, S., Tsipras, D., Talwar, K., and
M ˛ adry, A. Adversarially robust generalization requires
more data. In Advances in Neural Information Processing
Systems 31 , pp. 5019–5031, 2018.
Shaham, U., Yamada, Y., and Negahban, S. Understanding
adversarial training: Increasing local stability of neu-
ral nets through robust optimization. arXiv preprint
arXiv:1511.05432 , 2015.Sinha, A., Namkoong, H., and Duchi, J. Certiﬁable distribu-
tional robustness with principled adversarial training. In
International Conference on Learning Representations ,
2018.
Song, Y ., Kim, T., Nowozin, S., Ermon, S., and Kushman, N.
Pixeldefend: Leveraging generative models to understand
and defend against adversarial examples. In International
Conference on Learning Representations , 2018.
Su, D., Zhang, H., Chen, H., Yi, J., Chen, P.-Y., and Gao, Y.
Is robustness the cost of accuracy? — a comprehensive
study on the robustness of 18 deep image classiﬁcation
models. In European Conference on Computer Vision ,
2018.
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,
D., Goodfellow, I., and Fergus, R. Intriguing properties of
neural networks. arXiv preprint arXiv:1312.6199 , 2013.
Tabacof, P. and Valle, E. Exploring the space of adversarial
images. In International Joint Conference on Neural
Networks , pp. 426–433, 2016.
Tramèr, F., Kurakin, A., Papernot, N., Goodfellow, I.,
Boneh, D., and McDaniel, P. Ensemble adversarial train-
ing: Attacks and defenses. In International Conference
on Learning Representations , 2018.
Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and
Madry, A. Robustness may be at odds with accuracy. In
International Conference on Learning Representations ,
2019.
Uesato, J., O’Donoghue, B., Kohli, P., and van den Oord,
A. Adversarial risk and the dangers of evaluating against
weak attacks. In International Conference on Machine
Learning , pp. 5025–5034, 2018.
Volpi, R., Namkoong, H., Sener, O., Duchi, J. C., Murino,
V., and Savarese, S. Generalizing to unseen domains via
adversarial data augmentation. In Advances in Neural
Information Processing Systems , pp. 5339–5349, 2018.
Wong, E., Schmidt, F., Metzen, J., and Kolter, J. Scaling
provable adversarial defenses. In Advances in Neural
Information Processing Systems , 2018.
Xiao, C., Zhu, J.-Y., Li, B., He, W., Liu, M., and Song, D.
Spatially transformed adversarial examples. In Interna-
tional Conference on Learning Representations , 2018.
Xie, C., Wang, J., Zhang, Z., Zhou, Y ., Xie, L., and Yuille, A.
Adversarial examples for semantic segmentation and ob-
ject detection. In International Conference on Computer
Vision , 2017.Theoretically Principled Trade-off between Robustness and Accuracy
Xie, C., Wu, Y., van der Maaten, L., Yuille, A., and He, K.
Feature denoising for improving adversarial robustness.
arXiv preprint arXiv:1812.03411 , 2018.
Yin, D., Ramchandran, K., and Bartlett, P. Rademacher
complexity for adversarially robust generalization. arXiv
preprint arXiv:1810.11914 , 2018.
Zagoruyko, S. and Komodakis, N. Wide residual networks.
InBritish Machine Vision Conference , 2016.
Zhang, H., Xu, S., Jiao, J., Xie, P., Salakhutdinov, R., and
Xing, E. P. Stackelberg GAN: Towards provable mini-
max equilibrium via multi-generator architectures. arXiv
preprint arXiv:1811.08010 , 2018.
Zhang, H., Chen, H., Song, Z., Boning, D., Dhillon, I. S.,
and Hsieh, C.-J. The limitations of adversarial training
and the blind-spot attack. In International Conference on
Learning Representations , 2019a.
Zhang, H., Shao, J., and Salakhutdinov, R. Deep neural
networks with multi-branch architectures are intrinsically
less non-convex. In International Conference on Artiﬁcial
Intelligence and Statistics , pp. 1099–1109, 2019b.
Zhang, T. Covering number bounds of certain regularized
linear function classes. Journal of Machine Learning
Research , 2:527–550, 2002.
Zheng, S., Song, Y., Leung, T., and Goodfellow, I. Improv-
ing the robustness of deep neural networks via stability
training. In IEEE Conference on Computer Vision and
Pattern Recognition , pp. 4480–4488, 2016.