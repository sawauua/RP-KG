Guaranteed Rank Minimization via Singular Value
Projection
Prateek Jain
Microsoft Research Bangalore
Bangalore, India
prajain@microsoft.comRaghu Meka
UT Austin Dept. of Computer Sciences
Austin, TX, USA
raghu@cs.utexas.edu
Inderjit Dhillon
UT Austin Dept. of Computer Sciences
Austin, TX, USA
inderjit@cs.utexas.edu
Abstract
Minimizing the rank of a matrix subject to afﬁne constraints is a fundamental
problem with many important applications in machine learning and statistics. In
this paper we propose a simple and fast algorithm SVP (Singular Value Projec-
tion) for rank minimization under afﬁne constraints ( ARMP ) and show that SVP
recovers the minimum rank solution for afﬁne constraints that satisfy a restricted
isometry property (RIP). Our method guarantees geometric convergence rate even
in the presence of noise and requires strictly weaker assumptions on the RIPcon-
stants than the existing methods. We also introduce a Newton-step for our SVP
framework to speed-up the convergence with substantial empirical gains. Next,
we address a practically important application of ARMP - the problem of low-
rank matrix completion, for which the deﬁning afﬁne constraints do not directly
obey RIP, hence the guarantees of SVP do not hold. However, we provide partial
progress towards a proof of exact recovery for our algorithm by showing a more
restricted isometry property and observe empirically that our algorithm recovers
low-rank incoherent matrices from an almost optimal number of uniformly sam-
pled entries. We also demonstrate empirically that our algorithms outperform ex-
isting methods, such as those of [5, 18, 14], for ARMP and the matrix completion
problem by an order of magnitude and are also more robust to noise and sampling
schemes. In particular, results show that our SVP-Newton method is signiﬁcantly
robust to noise and performs impressively on a more realistic power-law sampling
scheme for the matrix completion problem.
1 Introduction
In this paper we study the general afﬁne rank minimization problem (ARMP),
minrank (X)s.tA(X) =b, X∈Rm×n, b∈Rd, (ARMP)
whereAis an afﬁne transformation from Rm×ntoRd.
The afﬁne rank minimization problem above is of considerable practical interest and many important
machine learning problems such as matrix completion, low-dimensional metric embedding, low-
rank kernel learning can be viewed as instances of the above problem. Unfortunately, ARMP is
NP-hard in general and is also NP-hard to approximate ([22]).
Until recently, most known methods for ARMP were heuristic in nature with few known rigorous
guarantees. In a recent breakthrough, Recht et al. [24] gave the ﬁrst nontrivial results for the
1problem obtaining guaranteed rank minimization for afﬁne transformations Athat satisfy a restricted
isometry property (RIP). Deﬁne the isometry constant of A,δkto be the smallest number such that
for allX∈Rm×nof rank at most k,
(1−δk)∥X∥2
F≤∥A (X)∥2
2≤(1 +δk)∥X∥2
F. (1)
The above RIPcondition is a direct generalization of the RIPcondition used in the compressive
sensing context. Moreover, RIPholds for many important practical applications of ARMP such
as image compression, linear time-invariant systems. In particular, Recht et al. show that for most
natural families of random measurements, RIPis satisﬁed even for only O(nklogn)measurements.
Also, Recht et al. show that for ARMP with isometry constant δ5k<1/10, the minimum rank
solution can be recovered by the minimum trace-norm solution.
In this paper we propose a simple and efﬁcient algorithm SVP (Singular Value Projection) based
on the projected gradient algorithm. We present a simple analysis showing that SVP recovers the
minimum rank solution for noisy afﬁne constraints that satisfy RIPand prove the following guar-
antees. (Independent of our work, Goldfarb and Ma [12] proposed an algorithm similar to SVP.
However, their analysis and formulation is different from ours. They also require stronger isometry
assumptions, δ3k<1/√
30, than our analysis.)
Theorem 1.1 Suppose the isometry constant of Asatisﬁesδ2k<1/3and letb=A(X∗)for a
rank-kmatrixX∗. Then, SVP (Algorithm 1) with step-size ηt= 1/(1 +δ2k)converges to X∗.
Furthermore, SVP outputs a matrix Xof rank at most ksuch that∥A(X)−b∥2
2≤ϵand∥X−
X∗∥2
F≤ϵ/(1−δ2k)in at most⌈
1
log((1 −δ2k)/2δ2k)log∥b∥2
2ϵ⌉
iterations.
Theorem 1.2 (Main) Suppose the isometry constant of Asatisﬁesδ2k<1/3and letb=A(X∗)+e
for a rankkmatrixX∗and an error vector e∈Rd. Then, SVP with step-size ηt= 1/(1 +δ2k)
outputs a matrix Xof rank at most ksuch that∥A(X)−b∥2
2≤C∥e∥2+ϵand∥X−X∗∥2
F≤
C∥e∥2+ϵ
1−δ2k,ϵ≥0, in at most⌈
1
log(1/D)log∥b∥2
2(C∥e∥2+ϵ)⌉
iterations for universal constants C,D .
As our SVP algorithm is based on projected gradient descent, it behaves as a ﬁrst order methods
and may require a relatively large number of iterations to achieve high accuracy, even after iden-
tifying the correct row and column subspaces. To this end, we introduce a Newton-type step in
our framework ( SVP-Newton ) rather than using a simple gradient-descent step. Guarantees sim-
ilar to Theorems 1.1, 1.2 follow easily for SVP-Newton using the proofs for SVP. In practice,
SVP-Newton performs better than SVP in terms of accuracy and number of iterations.
We next consider an important application of ARMP : the low-rank matrix completion problem
(MCP )— given a small number of entries from an unknown low-rank matrix, the task is to complete
the missing entries. Note that RIPdoes not hold directly for this problem. Recently, Candes and
Recht [6], Candes and Tao [7] and Keshavan et al. [14] gave the ﬁrst theoretical guarantees for the
problem obtaining exact recovery from an almost optimal number of uniformly sampled entries.
While RIPdoes not hold for MCP, we show that a similar property holds for incoherent matrices
[6]. Given our reﬁned RIPand a hypothesis bounding the incoherence of the iterates arising in SVP,
an analysis similar to that of Theorem 1.1 immediately implies that SVP optimally solves MCP.
We provide strong empirical evidence for our hypothesis and show that that both of our algorithms
recover a low-rank matrix from an almost optimal number of uniformly sampled entries.
In summary, our main contributions are:
•Motivated by [11], we propose a projected gradient based algorithm, SVP, forARMP and show
that our method recovers the optimal rank solution when the afﬁne constraints satisfy RIP. To the
best of our knowledge, our isometry constant requirements are least stringent: we only require
δ2k<1/3as opposed to δ5k<1/10by Recht et al., δ3k<1/4√
3by Lee and Bresler [18] and
δ4k<0.04by Lee and Bresler [17].
•We introduce a Newton-type step in the SVP method which is useful if high precision is criti-
cally. SVP-Newton has similar guarantees to that of SVP, is more stable and has better empirical
performance in terms of accuracy. For instance, on the Movie-lens dataset [1] and rank k= 3,
SVP-Newton achieves an RMSE of 0.89, while SVT method [5] achieves an RMSE of 0.98.
•As observed in [23], most trace-norm based methods perform poorly for matrix completion when
entries are sampled from more realistic power-law distributions. Our method SVP-Newton is
relatively robust to sampling techniques and performs signiﬁcantly better than the methods of
[5, 14, 23] even for power-law distributed samples.
2•We show that the afﬁne constraints in the low-rank matrix completion problem satisfy a weaker
restricted isometry property and as supported by empirical evidence, conjecture that SVP (as
well as SVP-Newton ) recovers the underlying matrix from an almost optimal number of uni-
formly random samples.
•We evaluate our method on a variety of synthetic and real-world datasets and show that our
methods consistently outperform, both in accuracy and time, various existing methods [5, 14].
2 Method
In this section, we ﬁrst introduce our Singular Value Projection ( SVP) algorithm for ARMP and
present a proof of its optimality for afﬁne constraints satisfying RIP(1). We then specialize our
algorithm for the problem of matrix completion and prove a more restricted isometry property for
the same. Finally, we introduce a Newton-type step in our SVP algorithm and prove its convergence.
2.1 Singular Value Decomposition (SVP)
Consider the following more robust formulation of ARMP (RARMP ),
min
Xψ(X) =1
2∥A(X)−b∥2
2s.t X∈C(k) ={X:rank (X)≤k}. (RARMP)
The hardness of the above problem mainly comes from the non-convexity of the set of low-rank
matricesC(k). However, the Euclidean projection onto C(k)can be computed efﬁciently using
singular value decomposition (SVD). Our algorithm uses this observation along with the projected
gradient method for efﬁciently minimizing the objective function speciﬁed in (RARMP).
LetPk:Rm×n→Rm×ndenote the orthogonal projection on to the set C(k). That is,Pk(X) =
argminY{∥Y−X∥F:Y∈C(k)}. It is well known that Pk(X)can be computed efﬁciently by
computing the top ksingular values and vectors of X.
InSVP, a candidate solution to ARMP is computed iteratively by starting from the all-zero ma-
trix and adapting the classical projected gradient descent update as follows (note that ∇ψ(X) =
AT(A(X)−b)):
Xt+1←Pk(
Xt−ηt∇ψ(Xt))
=Pk(
Xt−ηtAT(A(Xt)−b))
. (1)
Figure 1 presents SVP in more detail. Note that the iterates Xtare always low-rank, facilitating
faster computation of the SVD. See Section 3 for a more detailed discussion of computational issues.
Algorithm 1 Singular Value Projection ( SVP) Algorithm
Require:A,b,toleranceε,ηtfort= 0,1,2,...
1:Initialize:X0= 0andt= 0
2:repeat
3:Yt+1←Xt−ηtAT(A(Xt)−b)
4: Compute top ksingular vectors of Yt+1:Uk,Σk,Vk
5:Xt+1←UkΣkVT
k
6:t←t+ 1
7:until∥A(Xt+1)−b∥2
2≤ε
Analysis for Constraints Satisfying RIP
Theorem 1.1 shows that SVP converges to an ϵ-approximate solution of RARMP in O(log∥b∥2
ϵ)
steps. Theorem 1.2 shows a similar result for the noisy case. The theorems follow from the following
lemma that bounds the objective function after each iteration.
Lemma 2.1 LetX∗be an optimal solution of (RARMP) and letXtbe the iterate obtained by SVP
att-th iteration. Then, ψ(Xt+1)≤ψ(X∗) +δ2k
(1−δ2k)∥A(X∗−Xt)∥2
2,whereδ2kis the rank 2k
isometry constant of A.
The lemma follows from elementary linear algebra, optimality of SVD (Eckart-Young theorem) and
two simple applications of RIP. We refer to the supplementary material (Appendix A) for a detailed
proof. We now prove Theorem 1.1. Theorem 1.2 can also be proved similarly; see supplementary
material (Appendix A) for a detailed proof.
Proof of Theorem 1.1 Using Lemma 2.1 and the fact that ψ(X∗) = 0 , it follows that
ψ(Xt+1)≤δ2k
(1−δ2k)∥A(X∗−Xt)∥2
2=2δ2k
(1−δ2k)ψ(Xt).
3Also, note that for δ2k<1/3,2δ2k
(1−δ2k)<1. Hence, ψ(Xτ)≤ϵwhereτ=⌈
1
log((1 −δ2k)/2δ2k)logψ(X0)
ϵ⌉
. Further, using RIP for the rank at most 2kmatrixXτ−X∗we
get:∥Xτ−X∗∥≤ψ(Xτ)/(1−δ2k)≤ϵ/(1−δ2k).Now, the SVP algorithm is initialized using
X0= 0, i.e.,ψ(X0) =∥b∥2
2. Hence,τ=⌈
1
log((1 −δ2k)/2δ2k)log∥b∥2
2ϵ⌉
.
2.2 Matrix Completion
We ﬁrst describe the low-rank matrix completion problem formally. For Ω⊆[m]×[n], letPΩ:
Rm×n→Rm×ndenote the projection onto the index set Ω. That is, (PΩ(X))ij=Xijfor(i,j)∈
Ωand(PΩ(X))ij= 0 otherwise. Then, the low-rank matrix completion problem ( MCP ) can be
formulated as follows,
min
Xrank(X)s.tPΩ(X) =PΩ(X∗), X∈Rm×n. (MCP)
Observe that MCP is a special case of ARMP , so we can apply SVP for matrix completion. We
use step-size ηt= 1/(1 +δ)p, wherepis the density of sampled entries and δis a parameter which
we will explain later in this section. Using the given step-size and update (1), we get the following
update for matrix-completion:
Xt+1←Pk(
Xt−1
(1 +δ)p(PΩ(Xt)−P Ω(X∗)))
. (2)
Although matrix completion is a special case of ARMP , the afﬁne constraints that deﬁne MCP ,PΩ,
do not satisfy RIPin general. Thus Theorems 1.1, 1.2 above and the results of Recht et al. [24] do
not directly apply to MCP . However, we show that the matrix completion afﬁne constraints satisfy
RIPfor low-rank incoherent matrices.
Deﬁnition 2.1 (Incoherence) A matrixX∈Rm×nwith singular value decomposition X=
UΣVTisµ-incoherent if maxi,j|Uij|≤√µ√m,maxi,j|Vij|≤√µ√n.
The above notion of incoherence is similar to that introduced by Candes and Recht [6] and also used
by [7, 14]. Intuitively, high incoherence (i.e., µis small) implies that the non-zero entries of X
are not concentrated in a small number of entries. Hence, a random sampling of the matrix should
provide enough global information to satisfy RIP.
Using the above deﬁnition, we prove the following reﬁned restricted isometry property.
Theorem 2.2 There exists a constant C≥0such that the following holds for all 0< δ < 1,
µ≥1,n≥m≥3: For Ω⊆[m]×[n]chosen according to the Bernoulli model with density
p≥Cµ2k2logn/δ2m, with probability at least 1−exp(−nlogn), the following restricted isometry
property holds for all µ-incoherent matrices Xof rank at most k:
(1−δ)p∥X∥2
F≤ ∥P Ω(X)∥2
F≤(1 +δ)p∥X∥2
F. (3)
Roughly, our proof combines a Chernoff bound estimate for ∥PΩ(X)∥2
Fwith a union bound over
low-rank incoherent matrices. A proof sketch is presented in Section 2.2.1.
Given the above reﬁned RIP, if the iterates arising in SVP are shown to be incoherent, the arguments
of Theorem 1.1 can be used to show that SVP achieves exact recovery for low-rank incoherent
matrices from uniformly sampled entries. As supported by empirical evidence, we hypothesize that
the iteratesXtarising in SVP remain incoherent when the underlying matrix X∗is incoherent.
Figure 1 (d) plots the maximum incoherence maxtµ(Xt) =√nmaxt,i,j|Ut
ij|, whereUtare the
left singular vectors of the intermediate iterates Xtcomputed by SVP. The ﬁgure clearly shows
that the incoherence µ(Xt)of the iterates is bounded by a constant independent of the matrix size
nand densitypthroughout the execution of SVP. Figure 2 (c) plots the threshold sampling density
pbeyond which matrix completion for randomly generated matrices is solved exactly by SVP for
ﬁxedkand varying matrix sizes n. Note that the density threshold matches the optimal information-
theoretic bound [14] of Θ(klogn/n).
Motivated by Theorem 2.2 and supported by empirical evidence (Figures 2 (c), (d)) we hypothesize
thatSVP achieves exact recovery from an almost optimal number of samples for incoherent matrices.
Conjecture 2.3 Fixµ,k andδ≤1/3. Then, there exists a constant Csuch that for a µ-
incoherent matrix X∗of rank at most kandΩsampled from the Bernoulli model with density
p= Ωµ,k((logn)/m),SVP with step-size ηt= 1/(1 +δ)pconverges to X∗with high probability.
Moreover, SVP outputs a matrix Xof rank at most ksuch that∥PΩ(X)−P Ω(X∗)∥2
F≤ϵafter
Oµ,k(⌈
log(1
ϵ)⌉)
iterations.
42.2.1 RIPfor Matrix Completion on Incoherent Matrices
We now prove the restricted isometry property of Theorem 2.2 for the afﬁne constraints that result
from the projection operator PΩ. To prove Theorem 2.2 we ﬁrst show the theorem for a discrete
collection of matrices using Chernoff type large-deviation bounds and use standard quantization
arguments to generalize to the continuous case. We ﬁrst introduce some notation and provide useful
lemmas for our main proof1. First, we introduce the notion of α-regularity.
Deﬁnition 2.2 A matrixX∈Rm×nisα-regular if maxi,j|Xij|≤α√mn·∥X∥F.
Lemma 2.4 below relates the notion of regularity to incoherence and Lemma 2.5 proves (3) for a
ﬁxed regular matrix when the samples Ωare selected independently.
Lemma 2.4 LetX∈Rm×nbe aµ-incoherent matrix of rank at most k. ThenXisµ√
k-regular.
Lemma 2.5 Fix aα-regularX∈Rm×nand0<δ< 1. Then, for Ω⊆[m]×[n]chosen according
to the Bernoulli model, with each pair (i,j)∈Ωchosen independently with probability p,
Pr[∥PΩ(X)∥2
F−p∥X∥2
F≥δp∥X∥2
F]
≤2 exp(
−δ2pmn
3α2)
.
While the above lemma shows Equation (3) for a ﬁxed rank k,µ-incoherentX(i.e., (µ√
k)-regular
Xusing Lemma 2.4), we need to show Equation (3) for allsuch rankkincoherent matrices. To
handle this problem, we discretize the space of low-rank incoherent matrices so as to be able to
use the above lemma and a union bound. We now show the existence of a small set of matrices
S(µ,ϵ)⊆Rm×nsuch that every low-rank µ-incoherent matrix is close to an appropriately regular
matrix from the set S(µ,ϵ).
Lemma 2.6 For all 0<ϵ< 1/2,µ≥1,m,n≥3andk≥1, there exists a set S(µ,ϵ)⊆Rm×n
with|S(µ,ϵ)|≤(mnk/ϵ )3 (m+n)ksuch that the following holds. For any µ-incoherentX∈Rm×n
of rankkwith∥X∥2= 1, there exists Y∈S(µ,ϵ)s.t.∥Y−X∥F<ϵandYis(4µ√
k)-regular.
We now prove Theorem 2.2 by combining Lemmas 2.5, 2.6 and applying a union bound. We present
a sketch of the proof but defer the details to the supplementary material (Appendix B).
Proof Sketch of Theorem 2.2 LetS′(µ,ϵ) ={Y:Y∈S(µ,ϵ),Yis4µ√
k-regular}, where
S(µ,ϵ)is as in Lemma 2.6 for ϵ=δ/9mnk . Letm≤n. Then, by Lemma 2.5 and union bound,
for anyY∈S′(µ,ϵ),
Pr[∥PΩ(Y)∥2
F−p∥Y∥2
F≥δp∥Y∥2
F]
≤2(mnk/ϵ )3(m+n)kexp(−δ2pmn
16µ2k)
≤exp(C1nklogn)·exp(−δ2pmn
16µ2k)
,
whereC1≥0is a constant independent of m,n,k . Thus, ifp > Cµ2k2logn/δ2m, whereC=
16(C1+ 1), with probability at least 1−exp(−nlogn), the following holds
∀Y∈S′(µ,ϵ),|∥PΩ(Y)∥2
F−p∥Y∥2
F|≤δp∥Y∥2
F. (4)
As the statement of the theorem is invariant under scaling, it is enough to show the statement for all
µ-incoherent matrices Xof rank at most kand∥X∥2= 1. Fix such aXand suppose that (4) holds.
Now, by Lemma 2.6 there exists Y∈S′(µ,ϵ)such that∥Y−X∥F≤ϵ. Moreover,
∥Y∥2
F≤(∥X∥F+ϵ)2≤∥X∥2
F+ 2ϵ∥X∥F+ϵ2≤∥X∥2
F+ 3ϵk.
Proceeding similarly, we can show that
|∥X∥2
F−∥Y∥2
F|≤3ϵk,|∥PΩ(Y)∥2
F−∥P Ω(X)∥2
F|≤3ϵk. (5)
Combining inequalities (4), (5) above, with probability at least 1−exp(−nlogn)we have,
|∥PΩ(X)∥2
F−p∥X∥2
F|≤|∥P Ω(X)∥2
F−∥P Ω(Y)∥2
F|+p|∥X∥2
F−∥Y∥2
F|+|∥PΩ(Y)∥2
F−p∥Y∥2
F|≤2δp∥X∥2
F.
The theorem follows using the above inequality.
2.3 SVP-Newton
In this section we introduce a Newton-type step in our SVP method to speed up its convergence.
Recall that each iteration of SVP (Equation (1)) takes a step along the gradient of the objective
function and then projects the iterate to the set of low rank matrices using SVD. Now, the top k
singular vectors ( Uk,Vk) ofYt+1=Xt−ηtAT(A(Xt)−b)determine the range-space and column-
space of the next iterate in SVP. Then, Σkis given by Σk=Diag (UT
k(Xt−ηtAT(A(Xt)−b))Vk).
1Detailed proofs of all the lemmas in this section are provided in Appendix B of the supplementary material.
5Hence, Σkcan be seen as a product of gradient-descent step for a quadratic objective function, i.e.,
Σk= argminSψ(UkSVT
k). This leads us to the following variant of SVP we call SVP-Newton :2
Compute top k-singular vectors Uk,VkofYt+1=Xt−ηtAT(A(Xt)−b)
Xt+1=UkΣkVk,Σk= argmin
SΨ(UkSVT
k) = argmin
S∥A(UkΣkVT
k)−b∥2.
Note that asAis an afﬁne transformation, Σkcan be computed by solving a least squares problem
onk×kvariables. Also, for a single iteration, given the same starting point, SVP-Newton decreases
the objective function more than SVP. This observation along with straightforward modiﬁcations of
the proofs of Theorems 1.1, 1.2 show that similar guarantees hold for SVP-Newton as well3.
Note that the least squares problem for computing Σkhask2variables. This makes SVP-Newton
computationally expensive for problems with large rank, particularly for situations with a large
number of constraints as is the case for matrix completion. To overcome this issue, we also consider
the alternative where we restrict Σkto be a diagonal matrix, leading to the update
Σk= argmin
S,s.t.,S ij=0fori̸=j∥A(UkSVT
k)−b∥2(6)
We call the above method SVP-NewtonD (for SVP-Newton Diagonal). As for SVP-Newton, guar-
antees similar to SVP follow for SVP-NewtonD by observing that for each iteration, SVP-NewtonD
decreases the objective function more than SVP.
3 Related Work and Computational Issues
The general rank minimization problem with afﬁne constraints is NP-hard and is also NP-hard to
approximate [22]. Most methods for ARMP either relax the rank constraint to a convex function
such as the trace-norm [8], [9], or assume a factorization and optimize the resulting non-convex
problem by alternating minimization [4, 3, 15].
The results of Recht et al. [24] were later extended to noisy measurements and isometry constants
up toδ3k<1/4√
3by Fazel et al. [10] and Lee and Bresler [18]. However, even the best existing
optimization algorithms for the trace-norm relaxation are relatively inefﬁcient in practice. Recently,
Lee and Bresler [17] proposed an algorithm (ADMiRA) motivated by the orthogonal matching pur-
suitline of work in compressed sensing and show that for afﬁne constraints with isometry constant
δ4k≤0.04,their algorithm recovers the optimal solution. However, their method is not very efﬁ-
cient for large datasets and when the rank of the optimal solution is relatively large.
For the matrix-completion problem until the recent works of [6], [7] and [14], there were few meth-
ods with rigorous guarantees. The alternating least squares minimization heuristic and its variants
[3, 15] perform the best in practice, but are notoriously hard to analyze. Candes and Recht [6],
Candes and Tao [7] show that if X∗isµ-incoherent and the known entries are sampled uniformly
at random with|Ω|≥C(µ)k2nlog2n, ﬁnding the minimum trace-norm solution recovers the min-
imum rank solution. Keshavan et.al obtained similar results independently for exact recovery from
uniformly sampled Ωwith|Ω|≥C(µ,k)nlogn.
Minimizing the trace-norm of a matrix subject to afﬁne constraints can be cast as a semi-deﬁnite
program (SDP). However, algorithms for semi-deﬁnite programming, as used by most methods for
minimizing trace-norm, are prohibitively expensive even for moderately large datasets. Recently,
a variety of methods based mostly on iterative soft-thresholding have been proposed to solve the
trace-norm minimization problem more efﬁciently. For instance, Cai et al. [5] proposed a Singular
Value Thresholding (SVT) algorithm which is based on Uzawa’s algorithm [2]. A related approach
based on linearized Bregman iterations was proposed by Ma et al. [20], Toh and Yun [25], while Ji
and Ye [13] use Nesterov’s gradient descent methods for optimizing the trace-norm.
While the soft-thresholding based methods for trace-norm minimization are signiﬁcantly faster than
SDP based approaches, they suffer from slow convergence (see Figure 2 (d)). Also, noisy measure-
ments pose considerable computational challenges for trace-norm optimization as the rank of the
intermediate iterates can become very large (see Figure 3(b)).
2We call our method SVP-Newton as the Newton method when applied to a quadratic objective function
leads to the exact solution by solving the resulting least squares problem.
3As a side note, we can show a stronger result for SVP-Newton when applied to the special case of
compressed-sensing, i.e., when the matrix Xis restricted to be diagonal. Speciﬁcally, we can show that under
certain assumptions SVP-Newton converges to the optimal solution in O(logk), improving upon the result of
Maleki [21]. We give the precise statement of the theorem and proof in the supplementary material.
6406080100120140160100102104
n (Size of Matrix)  ARMP: Random Instances
SVP
SVT
6008001000120014001600024681012ARMP: MIT Logo
Number of ConstraintsError (Frobenius Norm)
  
SVP
SVT
1000 2000 3000 4000 50000.020.040.060.080.1
n (Size of the matrix)SVP Density Threshold
  k = 10, threshold p
k=10, Cklog(n)/n
1000 2000 3000 4000 50003.544.555.5Incoherence (SVP)
n (Size of the Matrix)µ
  
p=.05
p=.15
p=.25
p=.35(a) (b) (c) (d)
Figure 1: (a)Time taken by SVP and SVT for random instances of the Afﬁne Rank Minimization
Problem (ARMP) with optimal rank k= 5.(b)Reconstruction error for the MIT logo. (c)Empirical
estimates of the sampling density threshold required for exact matrix completion by SVP (here
C= 1.28). Note that the empirical bounds match the information theoretically optimal bound
Θ(klogn/n).(d)Maximum incoherence maxtµ(Xt)over the iterates of SVP for varying densities
pand sizesn. Note that the incoherence is bounded by a constant, supporting Conjecture 2.3.
1000 2000 3000 4000 5000−10123
n (Size of Matrix)  
SVP−NewtonD
SVP
SVT
ALS
ADMiRA
OPT
1000200030004000500001234x 10−3
n (Size of Matrix)RMSE
  
SVP−NewtonD
SVP
SVT
ALS
ADMiRA
OPT
2 4 6 810100101102103
k (Rank of Matrix)Time Taken (secs)
  
SVP−NewtonD
SVP
SVT
ALS
ADMiRA
OPT
10002000300040005000050100150200
n (Size of Matrix)Number of Iterations
  
SVP−NewtonD
SVP
SVT
(a) (b) (c) (d)
Figure 2: (a), (b) Running time ( on log scale ) and RMSE of various methods for matrix completion
problem with sampling density p=.1and optimal rank k= 2.(c)Running time ( on log scale ) of
various methods for matrix completion with sampling density p=.1andn= 1000 .(d)Number of
iterations needed to get RMSE 0.001.
For the case of matrix completion, SVP has an important property facilitating fast computation
of the main update in equation (2); each iteration of SVP involves computing the singular value
decomposition (SVD) of the matrix Y=Xt+PΩ(Xt−X∗), whereXtis a matrix of rank at
mostkwhose SVD is known and PΩ(Xt−X∗)is a sparse matrix. Thus, matrix-vector products
of the formYvcan be computed in time O((m+n)k+|Ω|). This facilitates the use of fast SVD
computing packages such as PROPACK [16] and ARPACK [19] that only require subroutines for
computing matrix-vector products.
4 Experimental Results
In this section, we empirically evaluate our methods for the afﬁne rank minimization problem and
low-rank matrix completion. For both problems we present empirical results on synthetic as well
as real-world datasets. For ARMP we compare our method against the trace-norm based singular
value thresholding (SVT) method [5]. Note that although Cai et al. present the SVT algorithm in the
context of MCP, it can be easily adapted for ARMP . For MCP we compare against SVT, ADMiRA
[17], the OptSpace (OPT) method of Keshavan et al. [14], and regularized alternating least squares
minimization (ALS). We use our own implementation of SVT for ARMP and ALS, while for matrix
completion we use the code provided by the respective authors for SVT, ADMiRA and OPT. We
report results averaged over 20runs. All the methods are implemented in Matlab and use mex ﬁles.
4.1 Afﬁne Rank Minimization
We ﬁrst compare our method against SVT on random instances of ARMP . We generate random
matricesX∈Rn×nof different sizes nand ﬁxed rank k= 5. We then generate d= 6knrandom
afﬁne constraint matrices Aiand compute b=A(X). Figure 1(a) compares the computational time
required by SVP and SVT (in log-scale) for achieving a relative error ( ∥A(X)−b∥2/∥b∥2) of10−3,
and shows that our method requires many fewer iterations and is signiﬁcantly faster than SVT.
Next we evaluate our method for the problem of matrix reconstruction from random measurements.
As in Recht et al. [24], we use the MIT logo as the test image for reconstruction. The MIT logo
we use is a 38×73image and has rank four. For reconstruction, we generate random measurement
matricesAiand measure bi=Tr(AiX). We let both SVP and SVT converge and then compute the
reconstruction error for the original image. Figure 1 (b) shows that our method incurs signiﬁcantly
smaller reconstruction error than SVT for the same number of measurements.
Matrix Completion: Synthetic Datasets (Uniform Sampling)
We now evaluate our method against various matrix completion methods for random low-rank ma-
7k SVP-NewtonD SVP ALS SVT
2 0.90 1.15 0.88 1.06
3 0.89 1.14 0.87 0.98
5 0.89 1.09 0.86 0.95
7 0.89 1.08 0.86 0.93
10 0.90 1.07 0.87 0.91
12 0.91 1.08 0.88 0.90
(a)
10002000300040005000100101102103
n (Size of Matrix)Time Taken (secs)
  
SVP−NewtonD
SVP
SVT
ALS
5001000 1500 200000.511.522.5
n (Size of Matrix)RMSE
  
ICMC
ALS
SVT
SVP
SVP NewtonD
5001000 1500 200001234
n (Size of Matrix)RMSE
  
ICMC
ALS
SVT
SVP
SVP NewtonD(b) (c) (d)
Figure 3: (a): RMSE incurred by various methods for matrix completion with different rank ( k)
solutions on Movie-Lens Dataset. (b): Time( on log scale ) required by various methods for matrix
completion with p=.1,k= 2 and10% Gaussian noise. Note that all the four methods achieve
similar RMSE. (c): RMSE incurred by various methods for matrix completion with p= 0.1,k= 10
when the sampling distribution follows Power-law distribution (Chung-Lu-Vu Model). (d): RMSE
incurred for the same problem setting as plot (c) but with added Gaussian noise.
trices and uniform samples. We generate a random rank kmatrixX∈Rn×nand generate random
Bernoulli samples with probability p. Figure 2 (a) compares the time required by various methods
(inlog-scale) to obtain a root mean square error (RMSE) of 10−3on the sampled entries for ﬁxed
k= 2. Clearly, SVP is substantially faster than the other methods. Next, we evaluate our method
for increasing k. Figure 2 (b) compares the overall RMSE obtained by various methods. Note that
SVP-Newton is signiﬁcantly more accurate than both SVP and SVT. Figure 2 (c) compares the time
required by various methods to obtain a root mean square error (RMSE) of 10−3on the sampled
entries for ﬁxed n= 1000 and increasing k. Note that our algorithms scale well with increasing k
and are faster than other methods. Next, we analyze reasons for better performance of our methods.
To this end, we plot the number of iterations required by our methods as compared to SVT (Fig-
ure 2 (d)). Note that even though each iteration of SVT is almost as expensive as our methods’, our
methods converge in signiﬁcantly fewer iterations.
Finally, we study the behavior of our method in presence of noise. For this experiment, we generate
random matrices of different size and add approximately 10% Gaussian noise. Figure 2 (c) plots
time required by various methods as nincreases from 1000 to5000 . Note that SVT is particularly
sensitive to noise. One of the reason for this is that due to noise, the rank of the intermediate iterates
arising in SVT can be fairly large.
Matrix Completion: Synthetic Dataset (Power-law Sampling) We now evaluate our methods
against existing matrix-completion methods under more realistic power-law distributed samples.
As before, we generate a random rank- k= 10 matrixX∈Rn×nand sample the entries of X
using a graph generated using Chung-Lu-Vu model with power-law distributed degrees (see [23])
for details. Figure 3 (c) plots the RMSE obtained by various methods for varying nand ﬁxed
sampling density p= 0.1. Note that SVP-NewtonD performs signiﬁcantly better than SVT as well
as SVP. Figure 3 (d) plots the RMSE obtained by various methods when each sampled entry is
corrupted with around 1%Gaussian noise. Note that here again SVP-NewtonD performs similar to
ALS and is signiﬁcantly better than the other methods including the ICMC method [23] which is
specially designed for power-law sampling but is quite sensitive to noise.
Matrix Completion: Movie-Lens Dataset
Finally, we evaluate our method on the Movie-Lens dataset [1], which contains 1 million ratings for
3900 movies by 6040 users. Figure 3 (a) shows the RMSE obtained by each method with varying k.
ForSVP and SVP-Newton, we ﬁx step size to be η= 1/p√
(t), wheretis the number of iterations.
For SVT, we ﬁx δ=.2pusing cross-validation. Since, rank cannot be ﬁxed in SVT, we try various
values for the parameter τto obtain the desired rank solution. Note that SVP-Newton incurs a
RMSE of 0.89fork= 3. In contrast, SVT achieves a RMSE of 0.98for the same rank. We remark
that SVT was able to achieve RMSE up to 0.89but required rank 17solution and was signiﬁcantly
slower in convergence because many intermediate iterates had large rank (up to around 150). We
attribute the relatively poor performance of SVP and SVT as compared with ALS and SVP-Newton
to the fact that the ratings matrix is not sampled uniformly, thus violating the crucial assumption of
uniformly distributed samples.
Acknowledgements: This research was supported in part by NSF grant CCF-0728879.
8References
[1]Movie lens dataset. Public dataset. URL http://www.grouplens.org/taxonomy/term/14 .
[2]K. Arrow, L. Hurwicz, and H. Uzawa. Studies in Linear and Nonlinear Programming . Stanford University
Press, Stanford, 1958.
[3]Robert Bell and Yehuda Koren. Scalable collaborative ﬁltering with jointly derived neighborhood inter-
polation weights. In ICDM , pages 43–52, 2007. doi: 10.1109/ICDM.2007.90.
[4]Matthew Brand. Fast online SVD revisions for lightweight recommender systems. In SIAM International
Conference on Data Mining , 2003.
[5]Jian-Feng Cai, Emmanuel J. Cand `es, and Zuowei Shen. A singular value thresholding algorithm for
matrix completion. SIAM Journal on Optimization , 20(4):1956–1982, 2010.
[6]Emmanuel J. Cand `es and Benjamin Recht. Exact matrix completion via convex optimization. Foundations
of Computational Mathematics , 9(6):717–772, December 2009.
[7]Emmanuel J. Cand `es and Terence Tao. The power of convex relaxation: Near-optimal matrix completion.
IEEE Trans. Inform. Theory , 56(5):2053–2080, 2009.
[8]M. Fazel, H. Hindi, and S. Boyd. A rank minimization heuristic with application to minimum order
system approximation. In American Control Conference, Arlington, Virginia , 2001.
[9]M. Fazel, H. Hindi, and S. Boyd. Log-det heuristic for matrix rank minimization with applications to
Hankel and Euclidean distance matrices. In American Control Conference , 2003.
[10] M. Fazel, E. Candes, B. Recht, and P. Parrilo. Compressed sensing and robust recovery of low rank
matrices. In Signals, Systems and Computers, 2008 42nd Asilomar Conference on , pages 1043–1047,
Oct. 2008. doi: 10.1109/ACSSC.2008.5074571.
[11] Rahul Garg and Rohit Khandekar. Gradient descent with sparsiﬁcation: an iterative algorithm for sparse
recovery with restricted isometry property. In ICML , 2009.
[12] Donald Goldfarb and Shiqian Ma. Convergence of ﬁxed point continuation algorithms for matrix rank
minimization, 2009. Submitted.
[13] Shuiwang Ji and Jieping Ye. An accelerated gradient method for trace norm minimization. In ICML ,
2009.
[14] Raghunandan H. Keshavan, Sewoong Oh, and Andrea Montanari. Matrix completion from a few entries.
InISIT’09: Proceedings of the 2009 IEEE international conference on Symposium on Information Theory ,
pages 324–328, Piscataway, NJ, USA, 2009. IEEE Press. ISBN 978-1-4244-4312-3.
[15] Yehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative ﬁltering model. In
KDD , pages 426–434, 2008. doi: 10.1145/1401890.1401944.
[16] R.M. Larsen. Propack: a software for large and sparse SVD calculations. Available online. URL http:
//sun.stanford.edu/rmunk/PROPACK/ .
[17] Kiryung Lee and Yoram Bresler. Admira: Atomic decomposition for minimum rank approximation, 2009.
[18] Kiryung Lee and Yoram Bresler. Guaranteed minimum rank approximation from linear observations by
nuclear norm minimization with an ellipsoidal constraint, 2009.
[19] Richard B. Lehoucq, Danny C. Sorensen, and Chao Yang. ARPACK Users’ Guide: Solution of Large-
Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods . SIAM, Philadelphia, 1998.
[20] S. Ma, D. Goldfarb, and L. Chen. Fixed point and bregman iterative methods for matrix rank minimiza-
tion. To appear, Mathematical Programming Series A , 2010.
[21] Arian Maleki. Coherence analysis of iterative thresholding algorithms. CoRR , abs/0904.1193, 2009.
[22] Raghu Meka, Prateek Jain, Constantine Caramanis, and Inderjit S. Dhillon. Rank minimization via online
learning. In ICML , pages 656–663, 2008. doi: 10.1145/1390156.1390239.
[23] Raghu Meka, Prateek Jain, and Inderjit S. Dhillon. Matrix completion from power-law distributed sam-
ples. In NIPS , 2009.
[24] Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization, 2007. To appear in SIAM Review.
[25] K.C. Toh and S. Yun. An accelerated proximal gradient algorithm for nuclear norm regularized least
squares problems. Preprint, 2009. URL http://www.math.nus.edu.sg/ ˜matys/apg.pdf .
9