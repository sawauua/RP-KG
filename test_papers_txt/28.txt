Learning and Generalization in Overparameterized
Neural Networks, Going Beyond Two Layers
Zeyuan Allen-Zhu
Microsoft Research AI
zeyuan@csail.mit.eduYuanzhi Li
Carnegie Mellon University
yuanzhil@andrew.cmu.eduYingyu Liang
University of Wisconsin-Madison
yliang@cs.wisc.edu
Abstract
The fundamental learning theory behind neural networks remains largely open.
What classes of functions can neural networks actually learn? Why doesn‚Äôt the
trained network overÔ¨Åt when it is overparameterized?
In this work, we prove that overparameterized neural networks can learn some
notable concept classes, including two and three-layer networks with fewer pa-
rameters and smooth activations. Moreover, the learning can be simply done by
SGD (stochastic gradient descent) or its variants in polynomial time using poly-
nomially many samples. The sample complexity can also be almost independent
of the number of parameters in the network.
On the technique side, our analysis goes beyond the so-called NTK (neural tan-
gent kernel) linearization of neural networks in prior works. We establish a new
notion of quadratic approximation of the neural network, and connect it to the
SGD theory of escaping saddle points.
1 Introduction
Neural network learning has become a key machine learning approach and has achieved remarkable
success in a wide range of real-world domains, such as computer vision, speech recognition, and
game playing [25, 26, 30, 41]. In contrast to the widely accepted empirical success, much less
theory is known. Despite a recent boost of theoretical studies, many questions remain largely open,
including fundamental ones about the optimization and generalization in learning neural networks.
One key challenge in analyzing neural networks is that the corresponding optimization is non-convex
and is theoretically hard in the general case [40, 55]. This is in sharp contrast to the fact that simple
optimization algorithms like stochastic gradient descent (SGD) and its variants usually produce good
solutions in practice even on both training and test data. Therefore,
what functions can neural networks provably learn?
Another key challenge is that, in practice, neural networks are heavily overparameterized (e.g., [53]):
the number of learnable parameters is much larger than the number of the training samples. It
is observed that overparameterization empirically improves both optimization and generalization,
appearing to contradict traditional learning theory.2Therefore,
why do overparameterized networks (found by those training algorithms) generalize?
Full version and future updates can be found on https://arxiv.org/abs/1811.04918 .
2For example, Livni et al. [36] observed that on synthetic data generated from a target network, SGD con-
verges faster when the learned network has more parameters than the target. Perhaps more interestingly, Arora
et al. [6] found that overparameterized networks learned in practice can often be compressed to simpler ones
with much fewer parameters, without hurting their ability to generalize; however, directly learning such simpler
networks runs into worse results due to the optimization difÔ¨Åculty. We also have experiments in Figure 1(a).
33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.1.1 What Can Neural Networks Provably Learn?
Most existing works analyzing the learnability of neural networks [9, 12, 13, 20, 21, 28, 33, 34, 42,
43, 47, 49, 50, 56] make unrealistic assumptions about the data distribution (such as being random
Gaussian), and/or make strong assumptions about the network (such as using linear activations).
Li and Liang [32] show that two-layer ReLU networks can learn classiÔ¨Åcation tasks when the data
come from mixtures of arbitrary but well-separated distributions.
A theorem without distributional assumptions on data is often more desirable. Indeed, how to obtain
a result that does not depend on the data distribution, but only on the concept class itself, lies in the
center of PAC-learning which is one of the foundations of machine learning theory [48]. Also,
studying non-linear activations is critical because otherwise one can only learn linear functions,
which can also be easily learned via linear models without neural networks.
Brutzkus et al. [14] prove that two-layer networks with ReLU activations can learn linearly-separable
data (and thus the class of linear functions) using just SGD. This is an (improper) PAC-learning
type of result because it makes no assumption on the data distribution. Andoni et al. [5] proves that
two-layer networks can learn polynomial functions of degree roverd-dimensional inputs in sample
complexityO(dr). Their learner networks use exponential activation functions, where in practice the
rectiÔ¨Åed linear unit (ReLU) activation has been the dominant choice across vastly different domains.
On a separate note, if one treats all but the last layer of neural networks as generating a random
feature map, then training only the last layer is a convex task, so one can learn the class of linear
functions in this implicit feature space [15, 16]. This result implies low-degree polynomials and
compositional kernels can be learned by neural networks in polynomial time. Empirically, training
last layer greatly weakens the power of neural networks (see Figure 1).
Our Result. We prove that an important concept class that contains three-layer (resp. two-layer)
neural networks equipped with smooth activations can be efÔ¨Åciently learned by three-layer (resp.
two-layer) ReLU neural networks via SGD or its variants.
SpeciÔ¨Åcally, suppose in aforementioned class the best network (called the target function or target
network) achieves a population risk OPT with respect to some convex loss function. We show that
one can learn up to population risk OPT +", using three-layer (resp. two-layer) ReLU networks of
size greater than a Ô¨Åxed polynomial in the size of the target network, in 1=", and in the ‚Äúcomplexity‚Äù
of the activation function used in the target network. Furthermore, the sample complexity is also
polynomial in these parameters, and only poly-logarithmic in the size of the learner ReLU network.
We stress here that this is agnostic PAC-learning because we allow the target function to have er-
ror (e.g., OPT can be positive for regression), and is improper learning because the concept class
consists of smaller neural networks comparing to the networks being trained.
Our Contributions. We believe our result gives further insights to the fundamental questions about
the learning theory of neural networks.
To the best of our knowledge, this is the Ô¨Årst result showing that using hidden layers of neural
networks one can provably learn the concept class containing two (or even three) layer neural
networks with non-trivial activation functions.3
Our three-layer result gives the Ô¨Årst theoretical proof that learning neural networks, even with
non-convex interactions across layers, can still be plausible. In contrast, in the two-layer case
the optimization landscape with overparameterization is almost convex [17, 32]; and in previ-
ous studies on the multi-layer case, researchers have weakened the network by applying the so-
called NTK (neural tangent kernel) linearization to remove all non-convex interactions [4, 27].
To some extent we explain the reason why overparameterization improves testing accuracy:
with larger overparameterization, one can hope to learn better target functions with possibly
larger size, more complex activations, smaller risk OPT , and to a smaller error ".
We establish new tools to tackle the learning process of neural networks in general, which can
be useful for studying other network architectures and learning tasks. (E.g., the new tools here
3In contrast, Daniely [15] focuses on training essentially only the last layer (and the hidden-layer movement
is negligible). After this paper has appeared online, Arora et al. [8] showed that neural networks can provably
learn two-layer networks with a slightly weaker class of smooth activation functions. Namely, the activation
functions that are either linear functions or even functions.
2have allowed researchers to study also the learning of recurrent neural networks [2].)
Other Related Works. We acknowledge a different line of research using kernels as improper
learners to learn the concept class of neural networks [22, 23, 36, 54]. This is very different from us
because we use ‚Äúneural networks‚Äù as learners. In other words, we study the question of ‚Äúwhat can
neural networks learn‚Äù but they study ‚Äúwhat alternative methods can replace neural networks.‚Äù
There is also a line of work studying the relationship between neural networks and NTKs (neural
tangent kernels) [3, 4, 7, 27, 31, 51]. These works study neural networks by considering their
‚Äúlinearized approximations.‚Äù There is a known performance gap between the power of real neural
networks and the power of their linearized approximations. For instance, ResNet achieves 96%
test error on the CIFAR-10 data set but NTK (even with inÔ¨Ånite width) achieves 77% [7]. We also
illustrate this in Figure 1.
1.2 Why Do Overparameterized Networks Generalize?
Our result above assumes that the learner network is sufÔ¨Åciently overparameterized. So, why does it
generalize to the population risk and give small test error? More importantly, why does it generalize
with a number of samples that is (almost) independent of the number of parameters?
This question cannot be studied under the traditional VC-dimension learning theory since the VC
dimension grows with the number of parameters. Several works [6, 11, 24, 39] explain generaliza-
tion by studying some other ‚Äúcomplexity‚Äù of the learned networks. Most related to the discussion
here is [11] where the authors prove a generalization bound in the norms (of weight matrices) of
each layer, as opposed to the number of parameters. There are two main concerns with those results.
Learnability =Trainability +Generalization. It is not clear from those results how a network
with both low ‚Äúcomplexity‚Äù and small training loss can be found by the training method.
Therefore, they do not directly imply PAC-learnability for non-trivial concept classes (at least
for those concept classes studied by this paper).
Their norms are ‚Äúsparsity induced norms‚Äù: for the norm not to scale with the number of hidden
neuronsm, essentially, it requires the number of neurons with non-zero weights notto scale
withm. This more or less reduces the problem to the non-overparameterized case.
At a high level, our generalization is made possible with the following sequence of conceptual steps.
Good networks with small risks are plentiful : thanks to overparameterization, with high prob-
ability over random initialization, there exists a good network in the close neighborhood of
anypoint on the SGD training trajectory. (This corresponds to Section 6.2 and 6.3.)
The optimization in overparameterized neural networks has benign properties: essentially
along the training trajectory, there is no second-order critical points for learning three-layer
networks, and no Ô¨Årst-order critical points for two-layer. (This corresponds to Section 6.4.)
In the learned networks, information is also evenly distributed among neurons, by utilizing
either implicit or explicit regularization. This structure allows a new generalization bound that
is (almost) independent of the number of neurons. (This corresponds to Section 6.5 and 6.6,
and we also empirically verify it in Section 7.1.)
Since practical neural networks are typically overparameterized, we genuinely hope that our results
can provide theoretical insights to networks used in various applications.
1.3 Roadmap
In the main body of this paper, we introduce notations in Section 2, present our main results and
contributions for two and three-layer networks in Section 3 and 4, and conclude in Section 5.
For readers interested in our novel techniques, we present in Section 6 an 8-paged proof sketch of our
three-layer result. For readers more interested in the practical relevance, we give more experiments
in Section 7. In the appendix, we begin with mathematical preliminaries in Appendix A. Our full
three-layer proof is in Appendix C. Our two-layer proof is much easier and in Appendix B.
30.0630.251.4.16.64.Test error
m = number of hidden neurons3layer
2layer
3layer(last)
2layer(last)
3layer(NTK)(a)N= 1000 and varym
0.0010.0040.0160.0630.251.4.16.64.Test error
N = number of samples3layer
2layer
3layer(last)
2layer(last)
3layer(NTK) (b)m= 2000 and varyN
Figure 1: Performance comparison. 3layer/2layer stands for training (hidden weights) in three and
two-layer neural networks. (last) stands for conjugate kernel [15], meaning training only the
output layer. (NTK) stands for neural tangent kernel [27] with Ô¨Ånite width. We also implemented
other direct kernels such as [54] but they perform much worse.
Setup. We consider `2regression task on synthetic data where feature vectors x2R4
are generated as normalized random Gaussian, and label is generated by target function
F(x) = (sin(3 x1) + sin(3x2) + sin(3x3) 2)2cos(7x4). We useNtraining samples,
and SGD with mini-batch size 50 and best tune learning rates and weight decay parameters. See
Appendix 7 for our experiment setup, how we choose such target function, and more experiments.
2 Notations
()denotes the ReLU function (x) = maxfx;0g. Givenf:R!Rand a vector x2Rm,f(x)
denotesf(x) = (f(x1);:::;f (xm)). For a vector w,kwkpdenote itsp-th norm, and when clear
from the context, abbreviate kwk=kwk2. For a matrix W2Rmd, useWior sometimes wito
denote thei-th row ofW. The row`pnorm iskWk2;p:=P
i2[m]kWikp
21=p
;the spectral norm is
kWk2, and the Frobenius norm is kWkF=kWk2;2. We sayf:Rd!RisL-Lipschitz continuous
ifjf(x) f(y)jLkx yk2; isL-Lipschitz smooth if krf(x) rf(y)k2Lkx yk2.
Function complexity. The following notion measures the complexity of any smooth activation
function(z). Suppose(z) =P1
i=0cizi. Given a non-negative R, the complexity
C"(;R) :=P1
i=0
(CR)i+ p
log(1=")p
iCRi
jcij;Cs(;R) :=CP1
i=0(i+ 1)1:75Rijcij
whereCis a sufÔ¨Åciently large constant (e.g., 104). Intuitively, Csmeasures the sample complexity:
how many samples are required to learn correctly; while C"bounds the network size: how much
over-parameterization is needed for the algorithm to efÔ¨Åciently learn up to"error. It is always
true that Cs(;R)C"(;R)Cs(;O(R))poly(1=").4While for sinz;exp(z)or low degree
polynomials, Cs(;O(R))andC"(;R)only differ by o(1=").
Example 2.1. If(z) =ecz 1,(z) = sin(cz),(z) = cos(cz)for constant cor(z)is
low degree polynomial, then C"(;1) =o(1=")andCs(;1) =O(1). If(z) = sigmoid( z)or
tanh(z), we can truncate their Taylor series at degree (log1
")to get"approximation. One can
verify this gives C"(;1)poly(1=")andCs(;1)O(1).
3 Result for Two-Layer Networks
We consider learning some unknown distribution Dof data points z= (x;y)2RdY, wherex
is the input point and yis the label. Without loss of generality, assume kxk2= 1 andxd=1
2.5
Consider a loss function L:RkR!Y such that for every y2Y, the function L(;y)is non-
negative, convex, 1-Lipschitz continuous and 1-Lipschitz smooth and L(0;y)2[0;1]. This includes
both the cross-entropy loss and the `2-regression loss (for bounded Y).
4Recall p
log(1=")p
iCieO(log(1="))=1
poly(")for everyi1.
51
2can always be padded to the last coordinate, and kxk2= 1 can always be ensured from kxk21by
paddingp
1 kxk2
2. This assumption is for simplifying the presentation.
4Concept class and target function F(x).Consider target functions F:Rd!Rkof
F= (f
1;:::;f
k)andf
r(x) =pX
i=1a
r;ii(hw
1;i;xi)hw
2;i;xi (3.1)
where eachi:R!Ris inÔ¨Ånite-order smooth and the weights w
1;i2Rd;w
2;i2Rdanda
r;i2R.
We assume for simplicity kw
1;ik2=kw
2;ik2= 1andja
r;ij1.6We denote by
C"(;R) := maxj2[p]fC"(j;R)gand Cs(;R) := maxj2[p]fCs(j;R)g
the complexity of Fand assume they are bounded.
In the agnostic PAC-learning language , our concept class consists of all functions Fin the form
of (3.1) with complexity bounded by threshold Cand parameter pbounded by threshold p0. Let
OPT =E[L(F(x);y)]be the population risk achieved by the best target function in this concept
class. Then, our goal is to learn this concept class with population risk OPT +"using sample and
time complexity polynomial inC,p0and1=". In the remainder of this paper, to simplify notations,
we do not explicitly deÔ¨Åne this concept class parameterized by Candp. Instead, we equivalently
state our theorem with respect to any (unknown) target function Fwith speciÔ¨Åc parameters Cand
psatisfying OPT =E[L(F(x);y)]. We assume OPT2[0;1]for simplicity.
Remark. Standard two-layer networks f
r(x) =Pp
i=1a
r;i(hw
1;i;xi)are special cases of (3.1) (by
settingw
2;i= (0;:::; 0;1)andi=). Our formulation (3.1) additionally captures combinations
of correlations between non-linear and linear measurements of different directions of x.
Learner network F(x;W).Using a data setZ=fz1;:::;zNgofNi.i.d. samples from D, we
train a network F= (f1;;fk):Rd!Rkwith
fr(x) :=mX
i=1ar;i(hwi;xi+bi) =a>
r(Wx+b) (3.2)
whereis the ReLU activation, W= (w1;:::;wm)2Rmdis the hidden weight matrix, b2Rm
is the bias vector, and ar2Rmis the output weight vector. To simplify analysis, we only update W
and keepbandarat initialization values. For such reason, we write the learner network as fr(x;W)
andF(x;W). We sometimes use b(0)=banda(0)
r=arto emphasize they are randomly initialized.
Our goal is to learn a weight matrix Wwith population risk E
L(F(x;W);y)
OPT +":
Learning Process. LetW(0)denote the initial value of the hidden weight matrix, and let W(0)+Wt
denote the value at time t. (Note that Wtis the matrix of increments .) The weights are initialized
with Gaussians and then Wis updated by the vanilla SGD. More precisely,
entries ofW(0)andb(0)are i.i.d. random Gaussians from N(0;1=m),
entries of each a(0)
rare i.i.d. random Gaussians from N(0;"2
a)for some Ô¨Åxed "a2(0;1].7
At timet, SGD samples z= (x;y)Z and updates Wt+1=Wt rL(F(x;W(0)+Wt);y).
3.1 Main Theorem
For notation simplicity, with high probability (or w.h.p.) means with probability 1 e clog2mfor a
sufÔ¨Åciently large constant c, andeOhides factors of polylog (m).
Theorem 1 (two-layer) .For every"2 
0;1
pkCs(;1)
, there exists
M0=poly(C"(;1);1=")andN0=poly(Cs(;1);1=")
such that for every mM0and everyNe
(N0), choosing"a="=e(1) for the initialization,
choosing learning rate =e 1
"km
and
T=e(Cs(;1))2k3p2
"2
;
6For generalkw
1;ik2B;kw
2;ik2B,ja
r;ij B, the scaling factor Bcan be absorbed into the
activation function 0(x) =(Bx). Our results then hold by replacing the complexity of with0.
7We shall choose "a=e(")in the proof due to technical reason. As we shall see in the three-layer case, if
weight decay is used, one can relax this to "a= 1.
5with high probability over the random initialization, SGD after Titeration satisÔ¨Åes
Esgdh
1
TPT 1
t=0E(x;y)DL(F(x;W(0)+Wt);y)i
OPT +":
Example 3.1. For functions such as (z) =ez;sinz;sigmoid(z);tanh(z)or low degree polynomi-
als, using Example 2.1, our theorem indicates that for target networks with such activation functions,
we can learn them using two-layer ReLU networks with
sizem=poly(k;p)
poly(")and sample complexity minfN;Tg=poly(k;p;logm)
"2
We note sample complexity Tis (almost) independent of m, the amount of overparametrization.
3.2 Our Interpretations
Overparameterization improves generalization. By increasing m, Theorem 1 supports more
target functions with possibly larger size, more complex activations, and smaller population risk
OPT . In other words, when mis Ô¨Åxed, among the class of target functions whose complexities
are captured by m, SGD can learn the best function approximator of the data, with the smallest
population risk. This gives intuition how overparameterization improves test error, see Figure 1(a).
Large margin non-linear classiÔ¨Åer. Theorem 1 is a nonlinear analogue of the margin theory for
linear classiÔ¨Åers. The target function with a small population risk (and of bounded norm) can be
viewed as a ‚Äúlarge margin non-linear classiÔ¨Åer.‚Äù In this view, Theorem 1 shows that assuming the
existence of such large-margin classiÔ¨Åer, SGD Ô¨Ånds a good solution with sample complexity mostly
determined by the margin, instead of the dimension of the data.
Inductive bias. Recent works (e.g., [4, 32]) show that when the network is heavily overparame-
terized (that is, mis polynomial in the number of training samples) and no two training samples are
identical, then SGD can Ô¨Ånd a global optimum with 0classiÔ¨Åcation error (or Ô¨Ånd a solution with
"training loss) in polynomial time. This does not come with generalization, since it can even Ô¨Åt
random labels. Our theorem, combined with [4], conÔ¨Årms the inductive bias of SGD for two-layer
networks: when the labels are random, SGD Ô¨Ånds a network that memorizes the training data; when
the labels are (even only approximately) realizable by some target network, then SGD learns and
generalizes. This gives an explanation towards the well-known empirical observations of such in-
ductive bias (e.g., [53]) in the two-layer setting, and is more general than Brutzkus et al. [14] in
which the target network is only linear.
4 Result for Three-Layer Networks
Concept class and target function F(x).This time we consider more powerful target functions
F= (f
1;;f
k)of the form
f
r(x) :=X
i2[p1]a
r;ii0
@X
j2[p2]v
1;i;j1;j(hw
1;j;xi)1
A0
@X
j2[p2]v
2;i;j2;j(hw
2;j;xi)1
A (4.1)
where each 1;j;2;j;i:R!Ris inÔ¨Ånite-order smooth, and the weights w
1;i;w
2;i2Rd,
v
1;i;v
2;i2Rp2anda
r;i2Rsatisfykw
1;jk2=kw
2;jk2=kv
1;ik2=kv
2;ik2= 1 andja
r;ij1.
Let
C"(;R) = maxj2[p2];s2[1;2]fC"(s;j;R)g; C"(;R) = maxj2[p1]fC"(j;R)g
Cs(;R) = maxj2[p2];s2[1;2]fCs(s;j;R)g; Cs(;R) = maxj2[p1]fCs(j;R)g
to denote the complexity of the two layers, and assume they are bounded.
Our concept class contains measures of correlations between composite non-linear functions and
non-linear functions of the input, there are plenty of functions in this new concept class that may not
necessarily have small-complexity representation in the previous formulation (3.1), and as we shall
see in Figure 1(a), this is the critical advantage of using three-layer networks compared to two-
layer ones or their NTKs. The learnability of this correlation is due to the non-convex interactions
between hidden layers. As a comparison, [15] studies the regime where the changes in hidden layers
are negligible thus can not show how to learn this concept class with a three-layer network.
6Remark 4.1.Standard three-layer networks
f
r(x) =P
i2[p1]a
r;iiP
j2[p2]v
i;jj(hw
j;xi)
are only special cases of (4.1). Also, even in the special case of i(z) =z, the target
f
r(x) =P
i2[p1]a
r;iP
j2[p2]v
1;i;j1(hw
1;j;xi)P
j2[p2]v
2;i;j2(hw
2;j;xi)
captures combinations of correlations of non-linear measurements in different directions of x.
Learner network F(x;W;V ).Our learners are three-layer networks F= (f1;:::;fk)with
fr(x) =X
i2[m2]ar;i(ni(x) +b2;i)where eachni(x) =X
j2[m1]vi;j(hwj;xi+b1;j)
The Ô¨Årst and second layers have m1andm2hidden neurons. Let W2Rm1dandV2Rm2m1
represent the weights of the Ô¨Årst and second hidden layers respectively, and b12Rm1andb22Rm2
represent the corresponding bias vectors, ar2Rm2represent the output weight vector.
4.1 Learning Process
Again for simplicity, we only update WandV. The weights are randomly initialized as:
entries ofW(0)andb1=b(0)
1are i.i.d. fromN(0;1=m1),
entries ofV(0)andb2=b(0)
2are i.i.d. fromN(0;1=m2),
entries of each ar=a(0)
rare i.i.d. fromN(0;"2
a)for"a= 1.
As for the optimization algorithm, we use SGD with weight decay and an explicit regularizer.
For some2(0;1], we will use F(x;W;V )as the learner network, i.e., linearly scale Fdown
by. This is equivalent to replacing W,Vwithp
W,p
V, since a ReLU network is positive
homogenous. The SGD will start with = 1and slowly decrease it, similar to weight decay.8
We also use an explicit regularizer for some w;v>0with9
R(p
W;p
V) :=vkp
Vk2
F+wkp
Wk4
2;4:
Now, in each round t= 1;2;:::;T , we use (noisy) SGD to minimize the following stochastic
objective for some Ô¨Åxed t 1:
L2(t 1;W0;V0) :=L
t 1F 
x;W(0)+W+W0;V(0)+V+V0
+R(p
t 1W0;p
t 1V0) (4.2)
Above, the objective is stochastic because (1) zZ is a random sample from the training set, (2)
WandVare two small perturbation random matrices with entries i.i.d. drawn from N(0;2
w)
andN(0;2
v)respectively, and (3) 2Rm1m1is a random diagonal matrix with diagonals i.i.d.
uniformly drawn from f+1; 1g. We note that the use of WandVis standard for Gaussian
smoothing on the objective (and not needed in practice).10The use of may be reminiscent of the
Dropout technique [46] in practice which randomly masks out neurons, and can also be removed.11
8We illustrate the technical necessity of adding weight decay. During training, it is easy to add new infor-
mation to the current network, but hard to forget ‚Äúfalse‚Äù information that is already in the network. Such false
information can be accumulated from randomness of SGD, non-convex landscapes, and so on. Thus, by scaling
down the network we can effectively forget false information.
9Thiskk 2;4norm onWencourages weights to be more evenly distributed across neurons. It can be
replaced withkpt 1Wt 1k2+

2;2+
for any constant 
>0for our theoretical purpose. We choose 
= 2 for
simplicity, and observe that in practice, weights are automatically spread out due to data randomness, so this
explicit regularization may not be needed. See Section 7.1 for an experiment.
10Similar to known non-convex literature [19] or smooth analysis, we introduce Gaussian perturbation W
andVfor theoretical purpose and it is not needed in practice. Also, we apply noisy SGD which is the vanilla
SGD plus Gaussian perturbation, which again is needed in theory but believed unnecessary for practice [19].
11In the full paper we study two variants of SGD. This present version is the ‚Äúsecond variant,‚Äù and the Ô¨Årst
variantL1(t 1;W0;V0)is the same as (4.2) by removing . Due to technical difÔ¨Åculty, the best sample
complexity we can prove for L1is a bit higher.
7Algorithm 1 SGD for three-layer networks (second variant (4.2))
Input: Data setZ, initialization W(0);V(0), step size, number of inner steps Tw,w;v;w;v.
1:W0= 0;V0= 0;1= 1,T=  
 1loglog(m1m2)
"0
.
2:fort= 1;2;:::;T do
3: Apply noisy SGD with step size on the stochastic objective L2(t 1;W;V )forTwsteps; the starting
point isW=Wt 1;V=Vt 1and suppose it reaches Wt;Vt. see Lemma A.9
4:t+1= (1 )t. weight decay
5:end for
6: Randomly sample bwith diagonal entries i.i.d. uniform on f1; 1g
7: Randomly sample e(1="2
0)many noise matrices
W;j;V;j	
. Let
j= arg minjn
Ez2ZL
TF 
x;W(0)+W;j+bWT;V(0)+V;j+VTbo
8: OutputW(out)
T =W(0)+W;j+bWT,V(out)
T =V(0)+V;j+VTb.
Algorithm 1 presents the details. SpeciÔ¨Åcally, in each round t, Algorithm 1 starts with weight ma-
tricesWt 1;Vt 1and performs Twiterations. In each iteration it goes in the negative direction of
the stochastic gradient rW0;V0L2(t;W0;V0). Let the Ô¨Ånal matrices be Wt;Vt. At the end of this
roundt, Algorithm 1 performs weight decay by setting t= (1 )t 1for some>0.
4.2 Main Theorems
For notation simplicity, with high probability (or w.h.p.) means with probability 1 e clog2(m1m2)
andeOhides factors of polylog (m1;m2).
Theorem 2 (three-layer, second variant) .Consider Algorithm 1. For every constant
2(0;1=4],
every"02(0;1=100], every"="0
kp1p2
2Cs(;p2Cs(;1))Cs(;1)2, there exists
M=poly
C"(;pp2C"(;1));1
"
such that for every m2=m1=mM, and properly set w;v;w;vin Table 1, as long as
Ne
C"(;pp2C"(;1))C"(;1)pp2p1k2
"02
there is a choice = 1=poly(m1;m2)andT=poly(m1;m2)such that with probability 99=100,
E(x;y)DL(TF(x;W(out)
T;V(out)
T);y)(1 +
)OPT +"0:
4.3 Our Contributions
Our sample complexity Nscales polynomially with the complexity of the target network, and is
(almost) independent ofm, the amount of overparameterization. This itself can be quite surprising,
because recent results on neural network generalization [6, 11, 24, 39] require Nto be polynomial
inm. Furthermore, Theorem 2 shows three-layer networks can efÔ¨Åciently learn a bigger concept
class (4.1) comparing to what we know about two-layer networks (3.1).
From a practical standpoint, one can construct target functions of the form (4.1) that cannot be
(efÔ¨Åciently) approximated by any two-layer target function in (3.1). If data is generated according
to such functions, then it may be necessary to use three-layer networks as learners (see Figure 1).
From a theoretical standpoint, even in the special case of (z) =z, our target function can cap-
ture correlations between non-linear measurements of the data (recall Remark 4.1). This means
C"(;C"(;1)pp2)O(pp2C"(;1)), so learning it is essentially in the same complexity as
learning each s;j. For example, a three-layer network can learn cos(100hw
1;xi)e100hw
2;xiup to
accuracy"in complexity poly(1="), while it is unclear how to do so using two-layer networks.
Technical Contributions. We highlight some technical contributions in the proof of Theorem 2.
In recent results on the training convergence of neural networks for more than two layers [3, 4], the
optimization process stays in a close neighborhood of the initialization so that, with heavy overpa-
rameterization, the network becomes ‚Äúlinearized‚Äù and the interactions across layers are negligible.
In our three-layer case, this means that the matrix Wnever interacts with V. They then argue that
8SGD simulates a neural tangent kernel so the learning process is almost convex [27]. In our analysis,
we directly tackle non-convex interactions between WandV, by studying a ‚Äúquadratic approxima-
tion‚Äù of the network. (See Remark 6.1 for a mathematical comparison.) Our new proofs techniques
that could be useful for future theoretical applications.
Also, for the results [3, 4] and our two-layer Theorem 1 to hold, it sufÔ¨Åces to analyze a regime where
the ‚Äúsign pattern‚Äù of ReLUs can be replaced with that of the random initialization. (Recall (x) =
Ix0xand we call Ix0the ‚Äúsign pattern.‚Äù) In our three-layer analysis, the optimization process
has moved sufÔ¨Åciently away from initialization , so that the sign pattern change can signiÔ¨Åcantly
affect output. This brings in additional technical challenge because we have to tackle non-convex
interactions between WandVtogether with changing sign patterns.12
Comparison to Daniely [15]. Daniely [15] studies the learnability of multi-layer networks when
(essentially) only the output layer is trained, which reduces to a convex task. He shows that multi-
layer networks can learn a compositional kernel space, which implies two/three-layer networks can
efÔ¨Åciently learn low-degree polynomials. He did not derive the general sample/time complexity
bounds for more complex functions such as those in our concept classes (3.1) and (4.1), but showed
that they are Ô¨Ånite.
In contrast, our learnability result of concept class (4.1) is due to the non-convex interaction between
hidden layers. Since Daniely [15] studies the regime when the changes in hidden layers are negli-
gible, if three layer networks are used, to the best of our knowledge, their theorem cannot lead to
similar sample complexity bounds comparing to Theorem 2 by only training the last layer of a three-
layer network. Empirically, one can also observe that training hidden layers is better than training
the last layer (see Figure 1).
5 Conclusion and Discussion
We show by training the hidden layers of two-layer (resp. three-layer) overparameterized neu-
ral networks, one can efÔ¨Åciently learn some important concept classes including two-layer (resp.
three-layer) networks equipped with smooth activation functions. Our result is in the agnostic PAC-
learning language thus is distribution-free . We believe our work opens up a new direction in both
algorithmic and generalization perspectives of overparameterized neural networks, and pushing for-
ward can possibly lead to more understanding about deep learning.
Our results apply to other more structured neural networks. As a concrete example, consider con-
volutional neural networks (CNN). Suppose the input is a two dimensional matrix x2Rdswhich
can be viewed as d-dimensional vectors in schannels , then a convolutional layer on top of xis
deÔ¨Åned as follows. There are d0Ô¨Åxed subsetsfS1;S2;:::;Sd0gof[d]each of size k0. The output
of the convolution layer is a matrix of size d0m, whose (i;j)-th entry is(hwj;xSii), where
xSi2Rk0sis the submatrix of xwith rows indexed by Si;wj2Rk0sis the weight matrix of the
j-th channel; and is the activation function. Overparameterization then means a larger number of
channelsmin our learned network comparing to the target. Our analysis can be adapted to show a
similar result for this type of networks.
One can also combine this paper with properties of recurrent neural networks (RNNs) [3] to derive
PAC-learning results for RNNs [2], or use the existential tools of this paper to derive PAC-learning
results for three-layer residual networks (ResNet) [1]. The latter gives a provable separation between
neural networks and kernels in the efÔ¨Åcient PAC-learning regime.
Acknowledgements
This work was supported in part by FA9550-18-1-0166. Y . Liang would also like to acknowl-
edge that support for this research was provided by the OfÔ¨Åce of the Vice Chancellor for Research
and Graduate Education at the University of Wisconsin-Madison with funding from the Wisconsin
Alumni Research Foundation.
12For instance, the number of sign changes can be m0:999for the second hidden layer (see Lemma 6.5).
In this region, the network output can be affected by m0:499since each neuron is of value roughly m 1=2.
Therefore, if after training we replace the sign pattern with random initialization, the output will be meaningless.
9References
[1] Zeyuan Allen-Zhu and Yuanzhi Li. What Can ResNet Learn EfÔ¨Åciently, Going Beyond Ker-
nels? In NeurIPS , 2019. Full version available at http://arxiv.org/abs/1905.10337 .
[2] Zeyuan Allen-Zhu and Yuanzhi Li. Can SGD Learn Recurrent Neural Networks with Provable
Generalization? In NeurIPS , 2019. Full version available at http://arxiv.org/abs/1902.
01028 .
[3] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent
neural networks. In NeurIPS , 2019. Full version available at http://arxiv.org/abs/1810.
12065 .
[4] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via
over-parameterization. In ICML , 2019. Full version available at http://arxiv.org/abs/
1811.03962 .
[5] Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with
neural networks. In International Conference on Machine Learning , pages 1908‚Äì1916, 2014.
[6] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds
for deep nets via a compression approach. arXiv preprint arXiv:1802.05296 , 2018.
[7] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.
On exact computation with an inÔ¨Ånitely wide neural net. arXiv preprint arXiv:1904.11955 ,
2019.
[8] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. arXiv
preprint arXiv:1901.08584 , 2019.
[9] Ainesh Bakshi, Rajesh Jayaram, and David P Woodruff. Learning two layer rectiÔ¨Åed neural
networks in polynomial time. arXiv preprint arXiv:1811.01885 , 2018.
[10] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds
and structural results. Journal of Machine Learning Research , 3(Nov):463‚Äì482, 2002.
[11] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds
for neural networks. In Advances in Neural Information Processing Systems , pages 6241‚Äì6250,
2017.
[12] Digvijay Boob and Guanghui Lan. Theoretical properties of the global optimizer of two layer
neural network. arXiv preprint arXiv:1710.11241 , 2017.
[13] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with
gaussian inputs. arXiv preprint arXiv:1702.07966 , 2017.
[14] Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. In International
Conference on Learning Representations , 2018.
[15] Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural
Information Processing Systems , pages 2422‚Äì2430, 2017.
[16] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural net-
works: The power of initialization and a dual view on expressivity. In Advances in Neural
Information Processing Systems (NIPS) , pages 2253‚Äì2261, 2016.
[17] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably opti-
mizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054 , 2018.
[18] Ronen Eldan, Dan Mikulincer, and Alex Zhai. The clt in high dimensions: quantitative bounds
via martingale embedding. arXiv preprint arXiv:1806.09087 , 2018.
10[19] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochas-
tic gradient for tensor decomposition. In Conference on Learning Theory , pages 797‚Äì842,
2015.
[20] Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with
landscape design. arXiv preprint arXiv:1711.00501 , 2017.
[21] Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang. Learning two-layer neural networks
with symmetric inputs. In International Conference on Learning Representations , 2019.
[22] Surbhi Goel and Adam Klivans. Learning neural networks with two nonlinear layers in poly-
nomial time. arXiv preprint arXiv:1709.06010v4 , 2018.
[23] Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in
polynomial time. In Conference on Learning Theory , pages 1004‚Äì1042, 2017.
[24] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity
of neural networks. In Proceedings of the Conference on Learning Theory , 2018.
[25] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep
recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee
international conference on , pages 6645‚Äì6649. IEEE, 2013.
[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recogni-
tion, pages 770‚Äì778, 2016.
[27] Arthur Jacot, Franck Gabriel, and Cl ¬¥ement Hongler. Neural tangent kernel: Convergence
and generalization in neural networks. In Advances in neural information processing systems ,
pages 8571‚Äì8580, 2018.
[28] Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Informa-
tion Processing Systems , pages 586‚Äì594, 2016.
[29] Robert Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does sgd escape
local minima? arXiv preprint arXiv:1802.06175 , 2018.
[30] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiÔ¨Åcation with deep
convolutional neural networks. In Advances in neural information processing systems , pages
1097‚Äì1105, 2012.
[31] Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. arXiv preprint arXiv:1902.06720 , 2019.
[32] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic
gradient descent on structured data. In Advances in Neural Information Processing Systems ,
2018.
[33] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu
activation. In Advances in Neural Information Processing Systems , pages 597‚Äì607, 2017.
[34] Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-
parameterized matrix recovery. arXiv preprint arXiv:1712.09203 , 2017.
[35] Percy Liang. CS229T/STAT231: Statistical Learning Theory (Winter 2016). https://web.
stanford.edu/class/cs229t/notes.pdf , April 2016. accessed January 2019.
[36] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efÔ¨Åciency of training
neural networks. In Advances in Neural Information Processing Systems , pages 855‚Äì863,
2014.
[37] Martin J. Wainwright. Basic tail and concentration bounds. https://www.stat.berkeley.
edu/ ~mjwain/stat210b/Chap2_TailBounds_Jan22_2015.pdf , 2015. Online; accessed
Oct 2018.
11[38] Andreas Maurer. A vector-contraction inequality for rademacher complexities. In International
Conference on Algorithmic Learning Theory , pages 3‚Äì17. Springer, 2016.
[39] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-
bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint
arXiv:1707.09564 , 2017.
[40] Ohad Shamir. Distribution-speciÔ¨Åc hardness of learning neural networks. Journal of Machine
Learning Research , 19(32), 2018.
[41] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van
Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc-
tot, et al. Mastering the game of go with deep neural networks and tree search. nature , 529
(7587):484, 2016.
[42] Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the
optimization landscape of over-parameterized shallow neural networks. arXiv preprint
arXiv:1707.04926 , 2017.
[43] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guar-
antees for multilayer neural networks. arXiv preprint arXiv:1605.08361 , 2016.
[44] Daniel Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex
algorithm usually takes polynomial time. In Proceedings of the thirty-third annual ACM sym-
posium on Theory of computing , pages 296‚Äì305. ACM, 2001.
[45] Karthik Sridharan. Machine Learning Theory (CS 6783). http://www.cs.cornell.edu/
courses/cs6783/2014fa/lec7.pdf , 2014. accessed January 2019.
[46] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. Dropout: a simple way to prevent neural networks from overÔ¨Åtting. The Journal of
Machine Learning Research , 15(1):1929‚Äì1958, 2014.
[47] Yuandong Tian. An analytical formula of population gradient for two-layered relu network and
its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560 ,
2017.
[48] Leslie Valiant. A theory of the learnable. Communications of the ACM , 1984.
[49] Santosh Vempala and John Wilmes. Polynomial convergence of gradient descent for training
one-hidden-layer neural networks. arXiv preprint arXiv:1805.02677 , 2018.
[50] Bo Xie, Yingyu Liang, and Le Song. Diversity leads to generalization in neural networks.
arXiv preprint Arxiv:1611.03131 , 2016.
[51] Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian pro-
cess behavior, gradient independence, and neural tangent kernel derivation. arXiv preprint
arXiv:1902.04760 , 2019.
[52] Alex Zhai. A high-dimensional CLT in W2distance with near optimal convergence rate. Prob-
ability Theory and Related Fields , 170(3-4):821‚Äì845, 2018.
[53] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In ICLR , 2017. arXiv 1611.03530.
[54] Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improp-
erly learnable in polynomial time. In International Conference on Machine Learning , pages
993‚Äì1001, 2016.
[55] Yuchen Zhang, Jason Lee, Martin Wainwright, and Michael Jordan. On the learnability of
fully-connected neural networks. In ArtiÔ¨Åcial Intelligence and Statistics , pages 83‚Äì91, 2017.
[56] Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guar-
antees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175 , 2017.
12