A Dual Coordinate Descent Method for Large-scale Linear SVM

Cho-Jui Hsieh b92085@csie.ntu.edu.tw
Kai-Wei Chang b92084@csie.ntu.edu.tw
Chih-Jen Lin cjlin@csie.ntu.edu.tw
Department of Computer Science, National Taiwan University, Taipei 106, Taiwan
S. Sathiya Keerthi selvarak@yahoo-inc.com
Yahoo! Research, Santa Clara, USA
S. Sundararajan ssrajan@yahoo-inc.com
Yahoo! Labs, Bangalore, India
Abstract
In many applications, data appear with a
huge number of instances as well as features.
Linear Support Vector Machines (SVM) is
one of the most popular tools to deal with
such large-scale sparse data. This paper
presents a novel dual coordinate descent
method for linear SVM with L1- and L2-
loss functions. The proposed method is sim-
ple and reaches an -accurate solution in
O(log(1=)) iterations. Experiments indicate
that our method is much faster than state
of the art solvers such as Pegasos ,TRON ,
SVMperf, and a recent primal coordinate de-
scent implementation.
1. Introduction
Support vector machines (SVM) (Boser et al., 1992)
are useful for data classication. Given a set of
instance-label pairs ( xi;yi);i= 1;:::;l;xi2Rn; yi2
f 1;+1g, SVM requires the solution of the following
unconstrained optimization problem:
min
w1
2wTw+ClX
i=1(w;xi;yi); (1)
where(w;xi;yi) is a loss function, and C0 is a
penalty parameter. Two common loss functions are:
max(1 yiwTxi;0) and max(1 yiwTxi;0)2:(2)
The former is called L1-SVM, while the latter is L2-
SVM. In some applications, an SVM problem appears
Appearing in Proceedings of the 25thInternational Confer-
ence on Machine Learning , Helsinki, Finland, 2008. Copy-
right 2008 by the author(s)/owner(s).with a bias term b. One often deal with this term by
appending each instance with an additional dimension:
xT
i [xT
i;1]wT [wT;b]: (3)
Problem (1) is often referred to as the primal form of
SVM. One may instead solve its dual problem:
min

f(
) =1
2
TQ
 eT

subject to 0
iU;8i; (4)
where Q=Q+D,Dis a diagonal matrix, and Qij=
yiyjxT
ixj. For L1-SVM, U=CandDii= 0;8i. For
L2-SVM,U=1andDii= 1=(2C);8i.
An SVM usually maps training vectors into a high-
dimensional space via a nonlinear function (x). Due
to the high dimensionality of the vector variable w,
one solves the dual problem (4) by the kernel trick
(i.e., using a closed form of (xi)T(xj)). We call
such a problem as a nonlinear SVM. In some applica-
tions, data appear in a rich dimensional feature space,
the performances are similar with/without nonlinear
mapping. If data are not mapped, we can often train
much larger data sets. We indicate such cases as linear
SVM; these are often encountered in applications such
as document classication. In this paper, we aim at
solving very large linear SVM problems.
Recently, many methods have been proposed for lin-
ear SVM in large-scale scenarios. For L1-SVM, Zhang
(2004), Shalev-Shwartz et al. (2007), Bottou (2007)
propose various stochastic gradient descent methods.
Collins et al. (2008) apply an exponentiated gradi-
ent method. SVMperf(Joachims, 2006) uses a cutting
plane technique. Smola et al. (2008) apply bundle
methods, and view SVMperfas a special case. For
L2-SVM, Keerthi and DeCoste (2005) propose mod-
ied Newton methods. A trust region Newton method
(TRON ) (Lin et al., 2008) is proposed for logistic re-A Dual Coordinate Descent Method for Large-scale Linear SVM
gression and L2-SVM. These algorithms focus on dif-
ferent aspects of the training speed. Some aim at
quickly obtaining a usable model, but some achieve
fast nal convergence of solving the optimization prob-
lem in (1) or (4). Moreover, among these methods,
Joachims (2006), Smola et al. (2008) and Collins et al.
(2008) solve SVM via the dual (4). Others consider the
primal form (1). The decision of using primal or dual
is of course related to the algorithm design.
Very recently, Chang et al. (2007) propose using co-
ordinate descent methods for solving primal L2-SVM.
Experiments show that their approach more quickly
obtains a useful model than some of the above meth-
ods. Coordinate descent, a popular optimization tech-
nique, updates one variable at a time by minimizing a
single-variable sub-problem. If one can eciently solve
this sub-problem, then it can be a competitive opti-
mization method. Due to the non-di
erentiability of
the primal L1-SVM, Chang et al's work is restricted to
L2-SVM. Moreover, as primal L2-SVM is di
erentiable
but not twice di
erentiable, certain considerations are
needed in solving the single-variable sub-problem.
While the dual form (4) involves bound constraints
0
iU, its objective function is twice di
erentiable
for both L1- and L2-SVM. In this paper, we investi-
gate coordinate descent methods for the dual problem
(4). We prove that an -optimal solution is obtained
inO(log(1=)) iterations. We propose an implemen-
tation using a random order of sub-problems at each
iteration, which leads to very fast training. Experi-
ments indicate that our method is more ecient than
the primal coordinate descent method. As Chang et al.
(2007) solve the primal, they require the easy access
of a feature's corresponding data values. However, in
practice one often has an easier access of values per in-
stance. Solving the dual takes this advantage, so our
implementation is simpler than Chang et al. (2007).
Early SVM papers (Mangasarian & Musicant, 1999;
Friess et al., 1998) have discussed coordinate descent
methods for the SVM dual form. However, they do not
focus on large data using the linear kernel. Crammer
and Singer (2003) proposed an online setting for multi-
class SVM without considering large sparse data. Re-
cently, Bordes et al. (2007) applied a coordinate de-
scent method to multi-class SVM, but they focus on
nonlinear kernels. In this paper, we point out that
dual coordinate descent methods make crucial advan-
tage of the linear kernel and outperform other solvers
when the numbers of data and features are both large.
Coordinate descent methods for (4) are related to the
popular decomposition methods for training nonlinear
SVM. In this paper, we show their key di
erences andexplain why earlier studies on decomposition meth-
ods failed to modify their algorithms in an ecient
way like ours for large-scale linear SVM. We also dis-
cuss the connection to other linear SVM works such as
(Crammer & Singer, 2003; Collins et al., 2008; Shalev-
Shwartz et al., 2007).
This paper is organized as follows. In Section 2, we de-
scribe our proposed algorithm. Implementation issues
are investigated in Section 3. Section 4 discusses the
connection to other methods. In Section 5, we compare
our method with state of the art implementations for
large linear SVM. Results show that the new method
is more ecient. Proofs can be found at http://www.
csie.ntu.edu.tw/ ~cjlin/papers/cddual.pdf .
2. A Dual Coordinate Descent Method
In this section, we describe our coordinate descent
method for L1- and L2-SVM. The optimization pro-
cess starts from an initial point 
02Rland generates
a sequence of vectors f
kg1
k=0. We refer to the process
from
kto
k+1as an outer iteration. In each outer
iteration we have linner iterations, so that sequen-
tially
1;
2;:::;
lare updated. Each outer iteration
thus generates vectors 
k;i2Rl,i= 1;:::;l + 1, such
that
k;1=
k,
k;l+1=
k+1, and

k;i= [
k+1
1;:::;
k+1
i 1;
k
i;:::;
k
l]T;8i= 2;:::;l:
For updating 
k;ito
k;i+1, we solve the following
one-variable sub-problem:
min
df(
k;i+dei) subject to 0
k
i+dU;(5)
whereei= [0;:::; 0;1;0;:::; 0]T. The objective func-
tion of (5) is a simple quadratic function of d:
f(
k;i+dei) =1
2Qiid2+rif(
k;i)d+ constant;(6)
whererifis theith component of the gradient rf.
One can easily see that (5) has an optimum at d= 0
(i.e., no need to update 
i) if and only if
rP
if(
k;i) = 0; (7)
whererPf(
) means the projected gradient
rP
if(
) =8
><
>:rif(
) if 0 <
i<U;
min(0;rif(
)) if
i= 0;
max(0;rif(
)) if
i=U:(8)
If (7) holds, we move to the index i+1 without updat-
ing
k;i
i. Otherwise, we must nd the optimal solution
of (5). If Qii>0, easily the solution is:

k;i+1
i = min
max

k;i
i rif(
k;i)
Qii;0
;U
:(9)A Dual Coordinate Descent Method for Large-scale Linear SVM
Algorithm 1 A dual coordinate descent method for
Linear SVM
Given
and the corresponding w=P
iyi
ixi.
While
is not optimal
Fori= 1;:::;l
(a) 
i 
i
(b)G=yiwTxi 1 +Dii
i
(c)
PG=8
><
>:min(G;0) if
i= 0;
max(G;0) if
i=U;
G if 0<
i<U
(d) IfjPGj6= 0,

i min(max(
i G=Qii;0);U)
w w+ (
i 
i)yixi
We thus need to calculate Qiiandrif(
k;i). First,
Qii=xT
ixi+Diican be precomputed and stored in
the memory. Second, to evaluate rif(
k;i), we use
rif(
) = ( Q
)i 1 =Xl
j=1Qij
j 1: (10)
Qmay be too large to be stored, so one calculates Q's
ith row when doing (10). If  nis the average number
of nonzero elements per instance, and O(n) is needed
for each kernel evaluation, then calculating the ith row
of the kernel matrix takes O(ln). Such operations are
expensive. However, for a linear SVM, we can dene
w=Xl
j=1yj
jxj; (11)
so (10) becomes
rif(
) =yiwTxi 1 +Dii
i: (12)
To evaluate (12), the main cost is O(n) for calculating
wTxi. This is much smaller than O(ln). To apply
(12),wmust be maintained throughout the coordinate
descent procedure. Calculating wby (11) takes O(ln)
operations, which are too expensive. Fortunately, if

iis the current value and 
iis the value after the
updating, we can maintain wby
w w+ (
i 
i)yixi: (13)
The number of operations is only O(n). To have the
rstw, one can use 
0=0sow=0. In the end, we
obtain the optimal wof the primal problem (1) as the
primal-dual relationship implies (11).
IfQii= 0, we have Dii= 0,Qii=xT
ixi= 0, and
hencexi=0. This occurs only in L1-SVM without
the bias term by (3). From (12), if xi=0, thenrif(
k;i) = 1:AsU=C <1for L1-SVM, the
solution of (5) makes the new 
k;i+1
i =U. We can
easily include this case in (9) by setting 1 =Qii=1.
Brie
y, our algorithm uses (12) to compute rif(
k;i),
checks the optimality of the sub-problem (5) by (7),
updates
iby (9), and then maintains wby (13). A
description is in Algorithm 1. The cost per iteration
(i.e., from
kto
k+1) isO(ln). The main memory
requirement is on storing x1;:::;xl. For the conver-
gence, we prove the following theorem using techniques
in (Luo & Tseng, 1992):
Theorem 1 For L1-SVM and L2-SVM, f
k;iggen-
erated by Algorithm 1 globally converges to an optimal
solution
. The convergence rate is at least linear:
there are 0<< 1and an iteration k0such that
f(
k+1) f(
)(f(
k) f(
));8kk0:(14)
The global convergence result is quite remarkable.
Usually for a convex but not strictly convex problem
(e.g., L1-SVM), one can only obtain that any limit
point is optimal. We dene an -accurate solution 

iff(
)f(
) +. By (14), our algorithm obtains
an-accurate solution in O(log(1=)) iterations.
3. Implementation Issues
3.1. Random Permutation of Sub-problems
In Algorithm 1, the coordinate descent algorithm
solves the one-variable sub-problems in the order of

1;:::;
l. Past results such as (Chang et al., 2007)
show that solving sub-problems in an arbitrary order
may give faster convergence. This inspires us to ran-
domly permute the sub-problems at each outer itera-
tion. Formally, at the kth outer iteration, we permute
f1;:::;lgtof(1);:::; (l)g, and solve sub-problems
in the order of 
(1);
(2);:::;
(l). Similar to Al-
gorithm 1, the algorithm generates a sequence f
k;ig
such that
k;1=
k,
k;l+1=
k+1;1and

k;i
t=(

k+1
t if 1
k(t)<i;

k
t if 1
k(t)i:
The update from 
k;ito
k;i+1is by

k;i+1
t =
k;i
t+arg min
0
k;i
t+dUf(
k;i+det) if 1
k(t) =i:
We prove that Theorem 1 is still valid. Hence, the new
setting obtains an -accurate solution in O(log(1=)) it-
erations. A simple experiment reveals that this setting
of permuting sub-problems is much faster than Algo-
rithm 1. The improvement is also bigger than that
observed in (Chang et al., 2007) for primal coordinate
descent methods.A Dual Coordinate Descent Method for Large-scale Linear SVM
Algorithm 2 Coordinate descent algorithm with ran-
domly selecting one instance at a time
Given
and the corresponding w=P
iyi
ixi.
While
is not optimal
{Randomly choose i2f1;:::;lg.
{Do steps (a)-(d) of Algorithm 1 to update 
i.
3.2. Shrinking
Eq. (4) contains constraints 0 
iU. If an

iis 0 orUfor many iterations, it may remain the
same. To speed up decomposition methods for non-
linear SVM (discussed in Section 4.1), the shrinking
technique (Joachims, 1998) reduces the size of the op-
timization problem without considering some bounded
variables. Below we show it is much easier to apply this
technique to linear SVM than the nonlinear case.
IfAis the subset after removing some elements and
A=f1;:::;lgnA, then the new problem is
min

A1
2
T
AQAA
A+ (QAA
A eA)T
A
subject to 0
iU;i2A; (15)
where QAA;QAAare sub-matrices of Q, and
Ais
considered as a constant vector. Solving this smaller
problem consumes less time and memory. Once (15) is
solved, we must check if the vector 
is optimal for (4).
This check needs the whole gradient rf(
). Since
rif(
) =Qi;A
A+Qi;A
A 1;
ifi2A, and one stores Qi;A
Abefore solving (15), we
already haverif(
). However, for all i =2A, we must
calculate the corresponding rows of Q. This step, re-
ferred to as the reconstruction of gradients in training
nonlinear SVM, is very time consuming. It may cost
up toO(l2n) if each kernel evaluation is O(n).
For linear SVM, in solving the smaller problem (15),
we still have the vector
w=X
i2Ayi
ixi+X
i2Ayi
ixi
though only the rst partP
i2Ayi
ixiis updated.
Therefore, using (12), rf(
) is easily available. Below
we demonstrate a shrinking implementation so that re-
constructing the whole rf(
) is never needed.
Our method is related to what LIBSVM (Chang & Lin,
2001) uses. From the optimality condition of bound-
constrained problems, 
is optimal for (4) if and only if
rPf(
) =0, whererPf(
) is the projected gradient
dened in (8). We then prove the following result:
Theorem 2 Let
be the convergent point of f
k;ig.1. If

i= 0 andrif(
)>0, then9kisuch that
8kki;8s;
k;s
i= 0.
2. If

i=Uandrif(
)<0, then9kisuch that
8kki;8s; 
k;s
i=U.
3.lim
k!1max
jrP
jf(
k;j)= lim
k!1min
jrP
jf(
k;j)=0:
During the optimization procedure, rPf(
k)6=0, so
in general max jrP
jf(
k)>0 and min jrP
jf(
k)<0.
These two values measure how the current solution vi-
olates the optimality condition. In our iterative proce-
dure, what we have are rif(
k;i); i= 1;:::;l . Hence,
at the (k 1)st iteration, we obtain
Mk 1max
jrP
jf(
k 1;j);mk 1min
jrP
jf(
k 1;j):
Then at each inner step of the kth iteration, before
updating
k;i
ito
k;i+1
i, this element is shrunken if
one of the following two conditions holds:

k;i
i= 0 andrif(
k;i)>Mk 1;

k;i
i=Uandrif(
k;i)<mk 1;(16)
whereMk 1=(
Mk 1ifMk 1>0;
1 otherwise,
mk 1=(
mk 1ifmk 1<0
 1 otherwise.
In (16), Mk 1must be strictly positive, so we set it be
1ifMk 1= 0. From Theorem 2, elements satisfying
the \if condition" of properties 1 and 2 meet (16) after
certain iterations, and are then correctly removed for
optimization. To have a more aggressive shrinking,
one may multiply both Mk 1and mk 1in (16) by a
threshold smaller than one.
Property 3 of Theorem 2 indicates that with a toler-
ance,
Mk mk< (17)
is satised after a nite number of iterations. Hence
(17) is a valid stopping condition. We also use it for
smaller problems (15). If at the kth iteration, (17)
for (15) is reached, we enlarge Atof1;:::;lg, set
Mk=1;mk= 1 (so no shrinking at the ( k+ 1)st
iteration), and continue regular iterations. Thus, we
do shrinking without reconstructing gradients.
3.3. An Online Setting
In some applications, the number of instances is huge,
so going over all 
1;:::;
lcauses an expensive outer
iteration. Instead, one can randomly choose an index
ikat a time, and update only 
ikat thekth outer
iteration. A description is in Algorithm 2. The setting
is related to (Crammer & Singer, 2003; Collins et al.,
2008). See also the discussion in Section 4.2.A Dual Coordinate Descent Method for Large-scale Linear SVM
Table 1. A comparison between decomposition methods
(Decomp. ) and dual coordinate descent ( DCD ). For both
methods, we consider that one 
iis updated at a time. We
assume Decomp. maintains gradients, but DCD does not.
The average number of nonzeros per instance is  n.
Nonlinear SVM Linear SVM
Decomp. DCD Decomp. DCD
Update
i O(1)O(ln)O(1)O(n)
Maintainrf(
)O(ln) NA O(ln) NA
4. Relations with Other Methods
4.1. Decomposition Methods for Nonlinear
SVM
Decomposition methods are one of the most popular
approaches for training nonlinear SVM. As the kernel
matrix is dense and cannot be stored in the computer
memory, decomposition methods solve a sub-problem
of few variables at each iteration. Only a small num-
ber of corresponding kernel columns are needed, so the
memory problem is resolved. If the number of vari-
ables is restricted to one, a decomposition method is
like the online coordinate descent in Section 3.3, but
it di
ers in the way it selects variables for updating.
It has been shown (Keerthi & DeCoste, 2005) that,
for linear SVM decomposition methods are inecient.
On the other hand, here we are pointing out that dual
coordinate descent is ecient for linear SVM. There-
fore, it is important to discuss the relationship between
decomposition methods and our method.
In early decomposition methods that were rst pro-
posed (Osuna et al., 1997; Platt, 1998), variables min-
imized at an iteration are selected by certain heuristics.
However, subsequent developments (Joachims, 1998;
Chang & Lin, 2001; Keerthi et al., 2001) all use gra-
dient information to conduct the selection. The main
reason is that maintaining the whole gradient does not
introduce extra cost. Here we explain the detail by as-
suming that one variable of 
is chosen and updated at
a time1. To set-up and solve the sub-problem (6), one
uses (10) to calculate rif(
). IfO(n) e
ort is needed
for each kernel evaluation, obtaining the ith row of
the kernel matrix takes O(ln) e
ort. If instead one
maintains the whole gradient, then rif(
) is directly
available. After updating 
k;i
ito
k;i+1
i, we obtain Q's
ith column (same as the ith row due to the symmetry
ofQ), and calculate the new whole gradient:
rf(
k;i+1) =rf(
k;i) +Q:;i(
k;i+1
i 
k;i
i);(18)
where Q:;iis theith column of Q. The cost is O(ln)
forQ:;iandO(l) for (18). Therefore, maintaining the
1Solvers like LIBSVM update at least two variables due
to a linear constraint in their dual problems. Here (4) has
no such a constraint, so selecting one variable is possible.whole gradient does not cost more. As using the whole
gradient implies fewer iterations (i.e., faster conver-
gence due to the ability to choose for updating the vari-
able that violates optimality most), one should take
this advantage. However, the situation for linear SVM
is very di
erent. With the di
erent way (12) to calcu-
laterif(
), the cost to update one 
iis onlyO(n). If
we still maintain the whole gradient, evaluating (12) l
times takes O(ln) e
ort. We gather this comparison of
di
erent situations in Table 1. Clearly, for nonlinear
SVM, one should use decomposition methods by main-
taining the whole gradient. However, for linear SVM,
iflis large, the cost per iteration without maintaining
gradients is much smaller than that with. Hence, the
coordinate descent method can be faster than the de-
composition method by using many cheap iterations.
An earlier attempt to speed up decomposition methods
for linear SVM is (Kao et al., 2004). However, it failed
to derive our method here because it does not give up
maintaining gradients.
4.2. Existing Linear SVM Methods
We discussed in Section 1 and other places the dif-
ference between our method and a primal coordinate
descent method (Chang et al., 2007). Below we de-
scribe the relations with other linear SVM methods.
We mentioned in Section 3.3 that our Algorithm 2 is
related to the online mode in (Collins et al., 2008).
They aim at solving multi-class and structured prob-
lems. At each iteration an instance is used; then a
sub-problem of several variables is solved. They ap-
proximately minimize the sub-problem, but for two-
class case, one can exactly solve it by (9). For the
batch setting, our approach is di
erent from theirs.
The algorithm for multi-class problems in (Crammer &
Singer, 2003) is also similar to our online setting. For
the two-class case, it solves (1) with the loss function
max( yiwTxi;0), which is di
erent from (2). They
do not study data with a large number of features.
Next, we discuss the connection to stochastic gradient
descent (Shalev-Shwartz et al., 2007; Bottou, 2007).
The most important step of this method is the follow-
ing update of w:
w w rw(yi;xi); (19)
whererw(yi;xi) is the sub-gradient of the approxi-
mate objective function:
wTw=2 +Cmax(1 yiwTxi;0);
andis the learning rate (or the step size). While our
method is dual-based, throughout the iterations weA Dual Coordinate Descent Method for Large-scale Linear SVM
Table 2. On the right training time for a solver to reduce the primal objective value to within 1% of the optimal value;
see (20). Time is in seconds. The method with the shortest running time is boldfaced. Listed on the left are the statistics
of data sets: lis the number of instances and nis the number of features.
Data setData statistics Linear L1-SVM Linear L2-SVM
l n # nonzeros DCDL1 Pegasos SVMperfDCDL2 PCD TRON
a9a 32,561 123 451,592 0.2 1.1 6.0 0.4 0.1 0.1
astro-physic 62,369 99,757 4,834,550 0.2 2.8 2.6 0.2 0.5 1.2
real-sim 72,309 20,958 3,709,083 0.2 2.4 2.4 0.1 0.2 0.9
news20 19,996 1,355,191 9,097,916 0.5 10.3 20.0 0.2 2.4 5.2
yahoo-japan 176,203 832,026 23,506,415 1.1 12.7 69.4 1.0 2.9 38.2
rcv1 677,399 47,236 49,556,258 2.6 21.9 72.0 2.7 5.1 18.6
yahoo-korea 460,554 3,052,939 156,436,656 8.3 79.7 656.8 7.1 18.4 286.1
maintainwby (13). Both (13) and (19) use one single
instancexi, but they take di
erent directions yixiand
rw(yi;xi). The selection of the learning rate may be
the subtlest thing in stochastic gradient descent, but
for our method this is never a concern. The step size
(
i 
i) in (13) is governed by solving a sub-problem
from the dual.
5. Experiments
In this section, we analyze the performance of our dual
coordinate descent algorithm for L1- and L2-SVM. We
compare our implementation with state of the art lin-
ear SVM solvers. We also investigate how the shrink-
ing technique improves our algorithm.
Table 2 lists the statistics of data sets. Four of them
(a9a,real-sim ,news20 ,rcv1) are at http://www.csie.
ntu.edu.tw/ ~cjlin/libsvmtools/datasets . The
set astro-physic is available upon request from
Thorsten Joachims. Except a9a, all others are from
document classication. Past results show that lin-
ear SVM performs as well as kernelized ones for such
data. To estimate the testing accuracy, we use a strat-
ied selection to split each set to 4 =5 training and 1 =5
testing. We brie
y describe each set below. Details
can be found in (Joachims, 2006) ( astro-physic ) and
(Lin et al., 2008) (others). a9ais from the UCI \adult"
data set. real-sim includes Usenet articles. astro-physic
includes documents from Physics Arxiv. news20 is a
collection of news documents. yahoo-japan andyahoo-
korea are obtained from Yahoo!. rcv1 is an archive of
manually categorized newswire stories from Reuters.
We compare six implementations of linear SVM. Three
solve L1-SVM, and three solve L2-SVM.
DCDL1 and DCDL2 : the dual coordinate descent
method with sub-problems permuted at each outer it-
eration (see Section 3.1). DCDL1 solves L1-SVM while
DCDL2 solves L2-SVM. We omit the shrinking setting .
Pegasos : the primal estimated sub-gradient solver
(Shalev-Shwartz et al., 2007) for L1-SVM. The sourceis at http://ttic.uchicago.edu/ ~shai/code .
SVMperf(Joachims, 2006): a cutting plane method for
L1-SVM. We use the latest version 2.1. The source is
athttp://svmlight.joachims.org/svm_perf.html .
TRON : a trust region Newton method (Lin et al., 2008)
for L2-SVM. We use the software LIBLINEAR version
1.22 with option -s 2 (http://www.csie.ntu.edu.
tw/~cjlin/liblinear ).
PCD: a primal coordinate descent method for L2-SVM
(Chang et al., 2007).
Since (Bottou, 2007) is related to Pegasos , we do not
present its results. We do not compare with another
online method Vowpal Wabbit (Langford et al., 2007)
either as it has been made available only very recently.
Though a code for the bundle method (Smola et al.,
2008) is available, we do not include it for comparison
due to its closeness to SVMperf. All sources used for
our comparisons are available at http://csie.ntu.
edu.tw/ ~cjlin/liblinear/exp.html .
We set the penalty parameter C= 1 for comparison2.
For all data sets, the testing accuracy does not increase
afterC4. All the above methods are implemented
in C/C++ with double precision. Some implementa-
tions such as (Bottou, 2007) use single precision to
reduce training time, but numerical inaccuracy may
occur. We do not include the bias term by (3).
To compare these solvers, we consider the CPU time
of reducing the relative di
erence between the primal
objective value and the optimum to within 0.01:
jfP(w) fP(w)j=jfP(w)j0:01; (20)
wherefPis the objective function of (1), and fP(w)
is the optimal value. Note that for consistency, we use
primal objective values even for dual solvers. The ref-
erence solutions of L1- and L2-SVM are respectively
obtained by solving DCDL1 and DCDL2 until the du-
ality gaps are less than 10 6. Table 2 lists the re-
sults. Clearly, our dual coordinate descent method
2The equivalent setting for Pegasos is= 1=(Cl). For
SVMperf, its penalty parameter is Cperf= 0:01Cl.A Dual Coordinate Descent Method for Large-scale Linear SVM
(a) L1-SVM: astro-physic
 (b) L2-SVM: astro-physic
(c) L1-SVM: news20
 (d) L2-SVM: news20
(e) L1-SVM: rcv1
 (f) L2-SVM: rcv1
Figure 1. Time versus the relative error (20). DCDL1-S ,
DCDL2-S areDCDL1 ,DCDL2 with shrinking. The dotted
line indicates the relative error 0.01. Time is in seconds.
for both L1- and L2-SVM is signicantly faster than
other solvers. To check details, we choose astro-physic ,
news20 ,rcv1, and show the relative error along time
in Figure 1. In Section 3.2, we pointed out that the
shrinking technique is very suitable for DCD. In Fig-
ure 1, we also include them ( DCDL1-S and DCDL2-S )
for comparison. Like in Table 2, our solvers are e-
cient for both L1- and L2-SVM. With shrinking, its
performance is even better.
Another evaluation is to consider how fast a solver ob-
tains a model with reasonable testing accuracy. Using
the optimal solutions from the above experiment, we
generate the reference models for L1- and L2-SVM. We
evaluate the testing accuracy di
erence between the
current model and the reference model along the train-
ing time. Figure 2 shows the results. Overall, DCDL1
andDCDL2 are more ecient than other solvers. Note
that we omit DCDL1-S and DCDL2-S in Figure 2, as
the performances with/without shrinking are similar.
Among L1-SVM solvers, SVMperfis competitive with
Pegasos for small data. But in the case of a huge num-
ber of instances, Pegasos outperforms SVMperf. How-
ever, Pegasos has slower convergence than DCDL1 . As
discussed in Section 4.2, the learning rate of stochas-
tic gradient descent may be the cause, but for DCDL1
we exactly solve sub-problems to obtain the step size
(a) L1-SVM: astro-physic
 (b) L2-SVM: astro-physic
(c) L1-SVM: news20
 (d) L2-SVM: news20
(e) L1-SVM: rcv1
 (f) L2-SVM: rcv1
Figure 2. Time versus the di
erence of testing accuracy be-
tween the current model and the reference model (obtained
using strict stopping conditions). Time is in seconds.
in updatingw. Also, Pegasos has a jumpy test set
performance while DCDL1 gives a stable behavior.
In the comparison of L2-SVM solvers, DCDL2 andPCD
are both coordinate descent methods. The former one
is applied to the dual, but the latter one to the pri-
mal. DCDL2 has a closed form solution for each sub-
problem, but PCD has not. The cost per PCD outer
iteration is thus higher than that of DCDL2 . There-
fore, while PCD is very competitive (only second to
DCDL1 /DCDL2 in Table 2), DCDL2 is even better.
Regarding TRON , as a Newton method, it possesses
fast nal convergence. However, since it takes signi-
cant e
ort at each iteration, it hardly generates a rea-
sonable model quickly. From the experiment results,
DCDL2 converges as fast as TRON , but also performs
well in early iterations.
Due to the space limitation, we give the following ob-
servations without details. First, Figure 1 indicates
that our coordinate descent method converges faster
for L2-SVM than L1-SVM. As L2-SVM has the diag-
onal matrix DwithDii= 1=(2C), we suspect that
itsQis better conditioned, and hence leads to faster
convergence. Second, all methods have slower conver-
gence when Cis large. However, small C's are usually
enough as the accuracy is stable after a threshold. In
practice, one thus should try from a small C. More-A Dual Coordinate Descent Method for Large-scale Linear SVM
over, ifnlandCis too large, then our DCDL2 is
slower than TRON orPCD (see problem a9ain Table
2, where the accuracy does not change after C0:25).
Ifnl, clearly one should solve the primal, whose
number of variables is just n. Such data are not our fo-
cus. Indeed, with a small number of features, one usu-
ally maps data to a higher space and train a nonlinear
SVM. Third, we have checked the online Algorithm 2.
Its performance is similar to DCDL1 andDCDL2 (i.e.,
batch setting without shrinking). Fourth, we have in-
vestigated real document classication which involves
many two-class problems. Using the proposed method
as the solver is more ecient than using others.
6. Discussion and Conclusions
We can apply the proposed method to solve regular-
ized least square problems, which have the loss func-
tion (1 yiwTxi)2in (1). The dual is simply (4) with-
out constraints, so the implementation is simpler.
In summary, we present and analyze an ecient dual
coordinate decent method for large linear SVM. It is
very simple to implement, and possesses sound op-
timization properties. Experiments show that our
method is faster than state of the art implementations.
References
Bordes, A., Bottou, L., Gallinari, P., & Weston, J.
(2007). Solving multiclass support vector machines
with LaRank. ICML .
Boser, B. E., Guyon, I., & Vapnik, V. (1992). A train-
ing algorithm for optimal margin classiers. COLT .
Bottou, L. (2007). Stochastic gradient descent exam-
ples. http://leon.bottou.org/projects/sgd .
Chang, C.-C., & Lin, C.-J. (2001). LIBSVM: a library
for support vector machines . Software available at
http://www.csie.ntu.edu.tw/ ~cjlin/libsvm .
Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2007). Coor-
dinate descent method for large-scale L2-loss linear
SVM (Technical Report). http://www.csie.ntu.
edu.tw/ ~cjlin/papers/cdl2.pdf .
Collins, M., Globerson, A., Koo, T., Carreras, X.,
& Bartlett, P. (2008). Exponentiated gradient al-
gorithms for conditional random elds and max-
margin markov networks. JMLR . To appear.
Crammer, K., & Singer, Y. (2003). Ultraconservative
online algorithms for multiclass problems. JMLR ,
3, 951{991.Friess, T.-T., Cristianini, N., & Campbell, C. (1998).
The kernel adatron algorithm: a fast and sim-
ple learning procedure for support vector machines.
ICML .
Joachims, T. (1998). Making large-scale SVM learning
practical. Advances in Kernel Methods - Support
Vector Learning . Cambridge, MA: MIT Press.
Joachims, T. (2006). Training linear SVMs in linear
time. ACM KDD .
Kao, W.-C., Chung, K.-M., Sun, C.-L., & Lin, C.-J.
(2004). Decomposition methods for linear support
vector machines. Neural Comput. ,16, 1689{1704.
Keerthi, S. S., & DeCoste, D. (2005). A modied nite
Newton method for fast solution of large scale linear
SVMs. JMLR ,6, 341{361.
Keerthi, S. S., Shevade, S. K., Bhattacharyya, C., &
Murthy, K. R. K. (2001). Improvements to Platt's
SMO algorithm for SVM classier design. Neural
Comput. ,13, 637{649.
Langford, J., Li, L., & Strehl, A. (2007). Vowpal Wab-
bit.http://hunch.net/ ~vw.
Lin, C.-J., Weng, R. C., & Keerthi, S. S. (2008). Trust
region Newton method for large-scale logistic regres-
sion. JMLR ,9, 623{646.
Luo, Z.-Q., & Tseng, P. (1992). On the convergence of
coordinate descent method for convex di
erentiable
minimization. J. Optim. Theory Appl. ,72, 7{35.
Mangasarian, O. L., & Musicant, D. R. (1999). Suc-
cessive overrelaxation for support vector machines.
IEEE Trans. Neural Networks ,10, 1032{1037.
Osuna, E., Freund, R., & Girosi, F. (1997). Train-
ing support vector machines: An application to face
detection. CVPR .
Platt, J. C. (1998). Fast training of support vector ma-
chines using sequential minimal optimization. Ad-
vances in Kernel Methods - Support Vector Learn-
ing. Cambridge, MA: MIT Press.
Shalev-Shwartz, S., Singer, Y., & Srebro, N. (2007).
Pegasos: primal estimated sub-gradient solver for
SVM. ICML .
Smola, A. J., Vishwanathan, S. V. N., & Le, Q. (2008).
Bundle methods for machine learning. NIPS .
Zhang, T. (2004). Solving large scale linear predic-
tion problems using stochastic gradient descent al-
gorithms. ICML .