Certified Adversarial Robustness via Randomized Smoothing

Jeremy Cohen1Elan Rosenfeld1J. Zico Kolter1 2
Abstract
We show how to turn any classiÔ¨Åer that classi-
Ô¨Åes well under Gaussian noise into a new classi-
Ô¨Åer that is certiÔ¨Åably robust to adversarial per-
turbations under the `2norm. This ‚Äúrandom-
ized smoothing‚Äù technique has been proposed re-
cently in the literature, but existing guarantees are
loose. We prove a tight robustness guarantee in
`2norm for smoothing with Gaussian noise. We
use randomized smoothing to obtain an ImageNet
classiÔ¨Åer with e.g. a certiÔ¨Åed top-1 accuracy of
49% under adversarial perturbations with `2norm
less than 0.5 (=127/255). No certiÔ¨Åed defense
has been shown feasible on ImageNet except for
smoothing. On smaller-scale datasets where com-
peting approaches to certiÔ¨Åed `2robustness are
viable, smoothing delivers higher certiÔ¨Åed accu-
racies. Our strong empirical results suggest that
randomized smoothing is a promising direction
for future research into adversarially robust classi-
Ô¨Åcation. Code and models are available at http:
//github.com/locuslab/smoothing .
1. Introduction
Modern image classiÔ¨Åers achieve high accuracy on i.i.d.
test sets but are not robust to small, adversarially-chosen
perturbations of their inputs (Szegedy et al., 2014; Biggio
et al., 2013). Given an image xcorrectly classiÔ¨Åed by, say,
a neural network, an adversary can usually engineer an ad-
versarial perturbation so small that x+looks just like
xto the human eye, yet the network classiÔ¨Åes x+as a
different, incorrect class. Many works have proposed heuris-
tic methods for training classiÔ¨Åers intended to be robust to
adversarial perturbations. However, most of these heuristics
have been subsequently shown to fail against suitably pow-
erful adversaries (Carlini & Wagner, 2017; Athalye et al.,
2018; Uesato et al., 2018). In response, a line of work on
1Carnegie Mellon University2Bosch Center for AI. Correspon-
dence to: Jeremy Cohen <jeremycohen@cmu.edu >.
Proceedings of the 36thInternational Conference on Machine
Learning , Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).
xpA
pB
Figure 1. Evaluating the smoothed classiÔ¨Åer at an input x.Left:
the decision regions of the base classiÔ¨Åer fare drawn in differ-
ent colors. The dotted lines are the level sets of the distribution
N(x;2I).Right : the distribution f(N(x;2I)). As discussed
below,pAis a lower bound on the probability of the top class and
pBis an upper bound on the probability of each other class. Here,
g(x)is ‚Äúblue.‚Äù
.
certiÔ¨Åable robustness studies classiÔ¨Åers whose prediction at
any pointxis veriÔ¨Åably constant within some set around x
(Wong & Kolter, 2018; Raghunathan et al., 2018a, e.g.). In
most of these works, the robust classiÔ¨Åer takes the form of a
neural network. Unfortunately, all existing approaches for
certifying the robustness of neural networks have trouble
scaling to networks that are large and expressive enough to
solve problems like ImageNet.
One workaround is to look for robust classiÔ¨Åers that are not
neural networks. Recently, two papers (Lecuyer et al., 2019;
Li et al., 2018) showed that an operation we call randomized
smoothing1can transform any arbitrary base classiÔ¨Åer finto
a new ‚Äúsmoothed classiÔ¨Åer‚Äù gthat is certiÔ¨Åably robust in
`2norm. Letfbe an arbitrary classiÔ¨Åer which maps inputs
Rdto classesY. For any input x, the smoothed classiÔ¨Åer‚Äôs
predictiong(x)is deÔ¨Åned to be the class which fis most
likely to classify the random variable N(x;2I)as. That is,
g(x)returns the most probable prediction by fof random
Gaussian corruptions of x.
If the base classiÔ¨Åer fis most likely to classify N(x;2I)
asx‚Äôs correct class, then the smoothed classiÔ¨Åer gwill be
1Smoothing was proposed under the name ‚ÄúPixelDP‚Äù (for dif-
ferential privacy). We use a different name since our improved
analysis does not involve differential privacy.arXiv:1902.02918v2  [cs.LG]  15 Jun 2019CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
correct atx. But the smoothed classiÔ¨Åer gwill also possess
a desirable property that the base classiÔ¨Åer may lack: one
can verify that g‚Äôs prediction is constant within an `2ball
around any input x, simply by estimating the probabilities
with whichfclassiÔ¨ÅesN(x;2I)as each class. The higher
the probability with which fclassiÔ¨ÅesN(x;2I)as the
most probable class, the larger the `2radius around xin
whichgprovably returns that class.
Lecuyer et al. (2019) proposed randomized smoothing as
a provable adversarial defense, and used it to train the Ô¨Årst
certiÔ¨Åably robust classiÔ¨Åer for ImageNet. Subsequently, Li
et al. (2018) proved a stronger robustness guarantee. How-
ever, both of these guarantees are loose, in the sense that
the smoothed classiÔ¨Åer gisprovably always more robust
than the guarantee indicates. In this paper, we prove the
Ô¨Årst tight robustness guarantee for randomized smoothing.
Our analysis reveals that smoothing with Gaussian noise
naturally induces certiÔ¨Åable robustness under the `2norm.
We suspect that other, as-yet-unknown noise distributions
might induce robustness to other perturbation sets such as
general`pnorm balls.
Randomized smoothing has one major drawback. If fis
a neural network, it is not possible to exactly compute the
probabilities with which fclassiÔ¨ÅesN(x;2I)as each
class. Therefore, it is not possible to exactly evaluate g‚Äôs
prediction at any input x, or to exactly compute the radius
in which this prediction is certiÔ¨Åably robust. Instead, we
present Monte Carlo algorithms for both tasks that are guar-
anteed to succeed with arbitrarily high probability.
Despite this drawback, randomized smoothing enjoys sev-
eral compelling advantages over other certiÔ¨Åably robust
classiÔ¨Åers proposed in the literature: it makes no assump-
tions about the base classiÔ¨Åer‚Äôs architecture, it is simple to
implement and understand, and, most importantly, it per-
mits the use of arbitrarily large neural networks as the base
classiÔ¨Åer. In contrast, other certiÔ¨Åed defenses do not cur-
Table 1. Approximate certiÔ¨Åed accuracy on ImageNet. Each row
shows a radius r, the best hyperparameter for that radius, the
approximate certiÔ¨Åed accuracy at radius rof the corresponding
smoothed classiÔ¨Åer, and the standard accuracy of the corresponding
smoothed classiÔ¨Åer. To give a sense of scale, a perturbation with
`2radius 1.0 could change one pixel by 255, ten pixels by 80, 100
pixels by 25, or 1000 pixels by 8. Random guessing on ImageNet
would attain 0.1% accuracy.
`2RADIUS BEST  CERT. ACC(%) S TD. ACC(%)
0.5 0.25 49 67
1.0 0.50 37 57
2.0 0.50 19 57
3.0 1.00 12 44
Figure 2. The smoothed classiÔ¨Åer‚Äôs prediction at an input x(left)
is deÔ¨Åned as the most likely prediction by the base classiÔ¨Åer on
random Gaussian corruptions of x(right;= 0:5). Note that this
Gaussian noise is much larger in magnitude than the adversarial
perturbations to which gis provably robust. One interpretation of
randomized smoothing is that these large random perturbations
‚Äúdrown out‚Äù small adversarial perturbations.
rently scale to large networks. Indeed, smoothing is the only
certiÔ¨Åed adversarial defense which has been shown feasible
on the full-resolution ImageNet classiÔ¨Åcation task.
We use randomized smoothing to train state-of-the-art certi-
Ô¨Åably`2-robust ImageNet classiÔ¨Åers; for example, one of
them achieves 49% provable top-1 accuracy under adver-
sarial perturbations with `2norm less than 127/255 (Table
1). We also demonstrate that on smaller-scale datasets like
CIFAR-10 and SHVN, where competing approaches to cer-
tiÔ¨Åed`2robustness are feasible, randomized smoothing can
deliver better certiÔ¨Åed accuracies, both because it enables
the use of larger networks and because it does not constrain
the expressivity of the base classiÔ¨Åer.
2. Related Work
Many works have proposed classiÔ¨Åers intended to be ro-
bust to adversarial perturbations. These approaches can
be broadly divided into empirical defenses, which empiri-
cally seem robust to known adversarial attacks, and certiÔ¨Åed
defenses, which are provably robust to certain kinds of ad-
versarial perturbations.
Empirical defenses The most successful empirical de-
fense to date is adversarial training (Goodfellow et al.,
2015; Kurakin et al., 2017; Madry et al., 2018), in which
adversarial examples are found during training (often using
projected gradient descent) and added to the training set.
Unfortunately, it is typically impossible to tell whether a
prediction by an empirically robust classiÔ¨Åer is truly robust
to adversarial perturbations; the most that can be said is that
a speciÔ¨Åc attack was unable to Ô¨Ånd any. In fact, many heuris-
tic defenses proposed in the literature were later ‚Äúbroken‚Äù
by stronger adversaries (Carlini & Wagner, 2017; Athalye
et al., 2018; Uesato et al., 2018; Athalye & Carlini, 2018).CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
Aiming to escape this cat-and-mouse game, a growing body
of work has focused on defenses with formal guarantees.
CertiÔ¨Åed defenses A classiÔ¨Åer is said to be certiÔ¨Åably ro-
bustif for any input x, one can easily obtain a guarantee that
the classiÔ¨Åer‚Äôs prediction is constant within some set around
x, often an`2or`1ball. In most work in this area, the
certiÔ¨Åably robust classiÔ¨Åer is a neural network. Some works
propose algorithms for certifying the robustness of generi-
cally trained networks, while others (Wong & Kolter, 2018;
Raghunathan et al., 2018a) propose both a robust training
method and a complementary certiÔ¨Åcation mechanism.
CertiÔ¨Åcation methods are either exact (a.k.a ‚Äúcomplete‚Äù) or
conservative (a.k.a ‚Äúsound but incomplete‚Äù). In the context
of`pnorm-bounded perturbations, exact methods take a
classiÔ¨Åerg, inputx, and radius r, and report whether or
not there exists a perturbation withinkkrfor which
g(x)6=g(x+). In contrast, conservative methods either
certify that no such perturbation exists or decline to make a
certiÔ¨Åcation; they may decline even when it is true that no
such perturbation exists. Exact methods are usually based
on SatisÔ¨Åability Modulo Theories (Katz et al., 2017; Carlini
et al., 2017; Ehlers, 2017; Huang et al., 2017) or mixed
integer linear programming (Cheng et al., 2017; Lomuscio
& Maganti, 2017; Dutta et al., 2017; Fischetti & Jo, 2018;
Bunel et al., 2018). Unfortunately, no exact methods have
been shown to scale beyond moderate-sized (100,000 acti-
vations) networks (Tjeng et al., 2019), and networks of that
size can only be veriÔ¨Åed when they are trained in a manner
that impairs their expressivity.
Conservative certiÔ¨Åcation is more scalable. Some conser-
vative methods bound the global Lipschitz constant of the
neural network (Gouk et al., 2018; Tsuzuku et al., 2018;
Anil et al., 2019; Cisse et al., 2017), but these approaches
tend to be very loose on expressive networks. Others mea-
sure the local smoothness of the network in the vicinity of a
particular input x. In theory, one could obtain a robustness
guarantee via an upper bound on the local Lipschitz con-
stant of the network (Hein & Andriushchenko, 2017), but
computing this quantity is intractable for general neural net-
works. Instead, a panoply of practical solutions have been
proposed in the literature (Wong & Kolter, 2018; Wang et al.,
2018a;b; Raghunathan et al., 2018a;b; Wong et al., 2018;
Dvijotham et al., 2018b;a; Croce et al., 2019; Gehr et al.,
2018; Mirman et al., 2018; Singh et al., 2018; Gowal et al.,
2018; Weng et al., 2018a; Zhang et al., 2018). Two themes
stand out. Some approaches cast veriÔ¨Åcation as an opti-
mization problem and import tools such as relaxation and
duality from the optimization literature to provide conserva-
tive guarantees (Wong & Kolter, 2018; Wong et al., 2018;
Raghunathan et al., 2018a;b; Dvijotham et al., 2018b;a).
Others step through the network layer by layer, maintaining
at each layer an outer approximation of the set of activationsreachable by a perturbed input (Mirman et al., 2018; Singh
et al., 2018; Gowal et al., 2018; Weng et al., 2018a; Zhang
et al., 2018). None of these local certiÔ¨Åcation methods have
been shown to be feasible on networks that are large and
expressive enough to solve modern machine learning prob-
lems like the ImageNet classiÔ¨Åcation task. Also, all either
assume speciÔ¨Åc network architectures (e.g. ReLU activa-
tions or a layered feedforward structure) or require extensive
customization for new network architectures.
Related work involving noise Prior works have proposed
using a network‚Äôs robustness to Gaussian noise as a proxy
for its robustness to adversarial perturbations (Weng et al.,
2018b; Ford et al., 2019), and have suggested that Gaussian
data augmentation could supplement or replace adversar-
ial training (Zantedeschi et al., 2017; Kannan et al., 2018).
Smilkov et al. (2017) observed that averaging a classiÔ¨Åer‚Äôs
input gradients over Gaussian corruptions of an image yields
very interpretable saliency maps. The robustness of neural
networks to random noise has been analyzed both theo-
retically (Fawzi et al., 2016; Franceschi et al., 2018) and
empirically (Dodge & Karam, 2017). Finally, Webb et al.
(2019) proposed a statistical technique for estimating the
noise robustness of a classiÔ¨Åer more efÔ¨Åciently than naive
Monte Carlo simulation; we did not use this technique since
it appears to lack formal high-probability guarantees. While
these works hypothesized relationships between a neural net-
work‚Äôs robustness to random noise and the same network‚Äôs
robustness to adversarial perturbations, randomized smooth-
ing instead uses a classiÔ¨Åer‚Äôs robustness to random noise to
create a new classiÔ¨Åer robust to adversarial perturbations.
Randomized smoothing Randomized smoothing has
been studied previously for adversarial robustness. Sev-
eral works (Liu et al., 2018; Cao & Gong, 2017) proposed
similar techniques as heuristic defenses, but did not prove
any guarantees. Lecuyer et al. (2019) used inequalities
from the differential privacy literature to prove an `2and
`1robustness guarantee for smoothing with Gaussian and
Laplace noise, respectively. Subsequently, Li et al. (2018)
used tools from information theory to prove a stronger `2ro-
bustness guarantee for Gaussian noise. However, all of these
robustness guarantees are loose. In contrast, we prove a tight
robustness guarantee in `2norm for randomized smoothing
with Gaussian noise.
3. Randomized smoothing
Consider a classiÔ¨Åcation problem from Rdto classesY.
As discussed above, randomized smoothing is a method for
constructing a new, ‚Äúsmoothed‚Äù classiÔ¨Åer gfrom an arbitrary
base classiÔ¨Åer f. When queried at x, the smoothed classiÔ¨Åer
greturns whichever class the base classiÔ¨Åer fis most likelyCertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
to return when xis perturbed by isotropic Gaussian noise:
g(x) = arg max
c2YP(f(x+") =c) (1)
where"N(0;2I)
An equivalent deÔ¨Ånition is that g(x)returns the class c
whose pre-imagefx02Rd:f(x0) =cghas the largest
probability measure under the distribution N(x;2I). The
noise levelis a hyperparameter of the smoothed classiÔ¨Åer
gwhich controls a robustness/accuracy tradeoff; it does not
change with the input x. We leave undeÔ¨Åned the behavior
ofgwhen the argmax is not unique.
We will Ô¨Årst present our robustness guarantee for the
smoothed classiÔ¨Åer g. Then, since it is not possible to
exactly evaluate the prediction of gatxor to certify the ro-
bustness ofgaroundx, we will give Monte Carlo algorithms
for both tasks that succeed with arbitrarily high probability.
3.1. Robustness guarantee
Suppose that when the base classiÔ¨Åer fclassiÔ¨ÅesN(x;2I),
the most probable class cAis returned with probability pA,
and the ‚Äúrunner-up‚Äù class is returned with probability pB.
Our main result is that smoothed classiÔ¨Åer gis robust around
xwithin the`2radiusR=
2( 1(pA)  1(pB)), where
 1is the inverse of the standard Gaussian CDF. This result
also holds if we replace pAwith a lower bound pAand we
replacepBwith an upper bound pB.
Theorem 1. Letf:Rd! Y be any deterministic or
random function, and let "N(0;2I). Letgbe deÔ¨Åned
as in (1). Suppose cA2Y andpA;pB2[0;1]satisfy:
P(f(x+") =cA)pApBmax
c6=cAP(f(x+") =c)(2)
Theng(x+) =cAfor allkk2<R, where
R=
2( 1(pA)  1(pB)) (3)
We now make several observations about Theorem 12
Theorem 1 assumes nothing about f. This is crucial
since it is unclear which well-behavedness assump-
tions, if any, are satisÔ¨Åed by modern deep architectures.
The certiÔ¨Åed radius Ris large when: (1) the noise level
is high, (2) the probability of the top class cAis high,
and (3) the probability of each other class is low.
2After the dissemination of this work, a more general result
was published in Levine et al. (2019); Salman et al. (2019): if h:
Rd![0;1]is a function and ^his the ‚Äúsmoothed‚Äù version ^h(x) =
E"N (0;2I)[h(x+")], then the function x7! 1(^h(x))is
1=-Lipschitz. Theorem 1 can be proved by applying this result to
the functions fc(x) =1[f(x) =c]for each class c.The certiÔ¨Åed radius Rgoes to1aspA!1and
pB!0. This should sound reasonable: the Gaussian
distribution is supported on all of Rd, so the only way
thatf(x+") =cAwith probability 1 is if f=cA
almost everywhere.
Both Lecuyer et al. (2019) and Li et al. (2018) proved `2
robustness guarantees for the same setting as Theorem 1, but
with different, smaller expressions for the certiÔ¨Åed radius.
However, our `2robustness guarantee is tight: if (2) is all
that is known about f, then it is impossible to certify an `2
ball with radius larger than R. In fact, it is impossible to
certify any superset of the `2ball with radius R:
Theorem 2. AssumepA+pB1. For any perturbation
withkk2>R, there exists a base classiÔ¨Åer fconsistent
with the class probabilities (2) for which g(x+)6=cA.
Theorem 2 shows that Gaussian smoothing naturally in-
duces`2robustness: if we make no assumptions on the base
classiÔ¨Åer beyond the class probabilities (2), then the set of
perturbations to which a Gaussian-smoothed classiÔ¨Åer is
provably robust is exactly an`2ball.
The complete proofs of Theorems 1 and 2 are in Appendix
A. We now sketch the proofs in the special case when there
are only two classes.
Theorem 1 (binary case). SupposepA2(1
2;1]satisÔ¨Åes
P(f(x+") =cA)pA. Theng(x+) =cAfor all
kk2< 1(pA).
Proof sketch. Fix a perturbation 2Rd. To guarantee
thatg(x+) =cA, we need to show that fclassiÔ¨Åes the
translated Gaussian N(x+;2I)ascAwith probability
>1
2. However, all we know about fis thatfclassiÔ¨Åes
N(x;2I)ascAwith probabilitypA. This raises the
question: out of all possible base classiÔ¨Åers fwhich classify
N(x;2I)ascAwith probabilitypA, which one f
classiÔ¨ÅesN(x+;2I)ascAwith the smallest probability?
One can show using an argument similar to the Neyman-
Pearson lemma (Neyman & Pearson, 1933) that this ‚Äúworst-
case‚Äùfis a linear classiÔ¨Åer whose decision boundary is
normal to the perturbation (Figure 3):
f(x0) =(
cA ifT(x0 x)kk2 1(pA)
cB otherwise(4)
This ‚Äúworst-case‚Äù fclassiÔ¨ÅesN(x+;2I)ascAwith
probability 
 1(pA) kk2

. Therefore, to ensure that
even the ‚Äúworst-case‚Äù fclassiÔ¨ÅesN(x+;2I)ascAwith
probability>1
2, we solve for those for which

 1(pA) kk2

>1
2
which is equivalent to the condition kk2< 1(pA).CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
x+
x
x+
x
Figure 3. Illustration of fin two dimensions. The concentric
circles are the density contours of N(x;2I)andN(x+;2I).
Out of all base classiÔ¨Åers fwhich classifyN(x;2I)ascA(blue)
with probabilitypA, such as both classiÔ¨Åers depicted above,
the ‚Äúworst-case‚Äù f‚Äî the one which classiÔ¨Åes N(x+;2I)as
cAwith minimal probability ‚Äî is depicted on the right: a linear
classiÔ¨Åer with decision boundary normal to the perturbation .
Theorem 2 is a simple consequence: for any withkk2>
R, the base classiÔ¨Åer fdeÔ¨Åned in (4) is consistent with (2);
yet iffis the base classiÔ¨Åer, then g(x+) =cB.
Figure 5 (left) plots our `2robustness guarantee against
the guarantees derived in prior work. Observe that our
Ris much larger than that of Lecuyer et al. (2019) and
moderately larger than that of Li et al. (2018). Appendix I
derives the other two guarantees using this paper‚Äôs notation.
Linear base classiÔ¨Åer A two-class linear classiÔ¨Åer
f(x) = sign(wTx+b)is already certiÔ¨Åable: the dis-
tance from any input xto the decision boundary is jwTx+
bj=kwk2, and no perturbation with`2norm less than this
distance can possibly change f‚Äôs prediction. In Appendix B
we show that if fis linear, then the smoothed classiÔ¨Åer gis
identical to the base classiÔ¨Åer f. Moreover, we show that our
bound (3) will certify the true robust radius jwTx+bj=kwk,
rather than a smaller, overconservative radius. Therefore,
whenfis linear, there always exists a perturbation just
beyond the certiÔ¨Åed radius which changes g‚Äôs prediction.
Noise level can scale with image resolution Since our
expression (3) for the certiÔ¨Åed radius does not depend ex-
plicitly on the data dimension d, one might worry that ran-
domized smoothing is less effective for images of higher
resolution ‚Äî certifying a Ô¨Åxed `2radius is ‚Äúless impressive‚Äù
for, say, a 224224image than for a 5656image. How-
ever, as illustrated by Figure 4, images in higher resolution
can tolerate higher levels of isotropic Gaussian noise be-
fore their class-distinguishing content gets destroyed. As
a consequence, in high resolution, smoothing can be per-
formed with a larger , leading to larger certiÔ¨Åed radii. See
Appendix G for a more rigorous version of this argument.3.2. Practical algorithms
We now present practical Monte Carlo algorithms for eval-
uatingg(x)and certifying the robustness of garoundx.
More details can be found in Appendix C.
3.2.1. P REDICTION
Evaluating the smoothed classiÔ¨Åer‚Äôs prediction g(x)re-
quires identifying the class cAwith maximal weight in the
categorical distribution f(x+"). The procedure described
in pseudocode as PREDICT drawsnsamples off(x+")
by runningnnoise-corrupted copies of xthrough the base
classiÔ¨Åer. Let ^cAbe the class which appeared the largest
number of times. If ^cAappeared much more often than any
other class, then PREDICT returns ^cA. Otherwise, it abstains
from making a prediction. We use the hypothesis test from
Hung & Fithian (2019) to calibrate the abstention threshold
so as to bound by 
the probability of returning an incorrect
answer. P REDICT satisÔ¨Åes the following guarantee:
Proposition 1. With probability at least 1 
over the
randomness in PREDICT ,PREDICT will either abstain or
returng(x). (Equivalently: the probability that PREDICT
returns a class other than g(x)is at most
.)
The function SAMPLE UNDER NOISE (f,x, num,) in the
pseudocode draws num samples of noise, "1:::" num
N(0;2I), runs eachx+"ithrough the base classiÔ¨Åer f,
and returns a vector of class counts. BINOM PVALUE (nA,
nA+nB,p) returns the p-value of the two-sided hypothesis
test thatnABinomial (nA+nB;p).
Even if the true smoothed classiÔ¨Åer gis robust at radius R,
PREDICT will be vulnerable in a certain sense to adversarial
perturbations with `2norm slightly less than R. By engi-
neering a perturbation for whichf(x++")puts mass
just over1
2on classcAand mass just under1
2on classcB,
an adversary can force PREDICT to abstain at a high rate. If
this scenario is of concern, a variant of Theorem 1 could be
proved to certify a radius in which P(f(x++") =cA)is
larger by some margin than maxc6=cAP(f(x++") =c).
3.2.2. C ERTIFICATION
Evaluating and certifying the robustness of garound an
inputxrequires not only identifying the class cAwith maxi-
mal weight in f(x+"), but also estimating a lower bound
Figure 4. Left to right: clean 56 x 56 image, clean 224 x 224 image,
noisy 56 x 56 image ( = 0:5), noisy 224 x 224 image ( = 0:5).CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
Pseudocode for certiÔ¨Åcation and prediction
# evaluategatx
function PREDICT (f,,x,n,
)
counts SAMPLE UNDER NOISE (f,x,n,)
^cA;^cB top two indices in counts
nA;nB counts [^cA],counts [^cB]
ifBINOM PVALUE (nA,nA+nB, 0.5)
return ^cA
else return ABSTAIN
# certify the robustness of garoundx
function CERTIFY (f,,x,n0,n,
)
counts0 SAMPLE UNDER NOISE(f;x;n 0;)
^cA top index in counts0
counts SAMPLE UNDER NOISE(f;x;n; )
pA LOWER CONFBOUND (counts [^cA],n,1 
)
ifpA>1
2return prediction ^cAand radius 1(pA)
else return ABSTAIN
pAon the probability that f(x+") =cAand an upper
boundpBon the probability that f(x+")equals any other
class. Doing all three of these at the same time in a sta-
tistically correct manner requires some care. One simple
solution is presented in pseudocode as CERTIFY : Ô¨Årst, use
a small number of samples from f(x+")to take a guess
atcA; then use a larger number of samples to estimate pA;
then simply take pB= 1 pA.
Proposition 2. With probability at least 1 
over the
randomness in CERTIFY , ifCERTIFY returns a class ^cA
and a radius R(i.e. does not abstain), then gpredicts ^cA
within radius Raroundx:g(x+) = ^cA8kk2<R.
The function LOWER CONFBOUND (k,n,1 
) in the pseu-
docode returns a one-sided (1 
)lower conÔ¨Ådence in-
terval for the Binomial parameter pgiven a sample k
Binomial (n;p).
Certifying large radii requires many samples Recall
from Theorem 1 that Rapproaches1aspAapproaches 1.
Unfortunately, it turns out that pAapproaches 1 so slowly
withnthatRalso approaches1very slowly with n. Con-
sider the most favorable situation: f(x) =cAeverywhere.
This means that gis robust at radius1. But after observing
nsamples off(x+")which all equal cA, the tightest (to
our knowledge) lower bound would say that with probabil-
ity least 1 
,pA
(1=n). PluggingpA=
(1=n)and
pB= 1 pAinto (3) yields an expression for the certiÔ¨Åed
radius as a function of n:R= 1(
1=n). Figure 5
(right) plots this function for 
= 0:001;= 1. Observe
that certifying a radius of 4with 99.9% conÔ¨Ådence would
require105samples.
0.5 0.6 0.7 0.8 0.9 1.0
pA0123radiusours
(Lecuyer et al, 2018)
(Li et al, 2018)
102
104
106
number of samples012345radiusFigure 5. Left: CertiÔ¨Åed radius Ras a function of pA(withpB=
1 pAand= 1) under all three randomized smoothing bounds.
Right : A plot ofR= 1(
1=n)for
= 0:001and= 1.
The radius we can certify with high probability grows slowly with
the number of samples, even in the best case wheref(x) =cA
everywhere.
3.3. Training the base classiÔ¨Åer
Theorem 1 holds regardless of how the base classiÔ¨Åer fis
trained. However, in order for gto classify the labeled ex-
ample (x;c)correctly and robustly, fneeds to consistently
classifyN(x;2I)asc. In high dimension, the Gaussian
distributionN(x;2I)places almost no mass near its mode
x. As a consequence, when is moderately high, the distri-
bution of natural images has virtually disjoint support from
the distribution of natural images corrupted by N(0;2I);
see Figure 2 for a visual demonstration. Therefore, if the
base classiÔ¨Åer fis trained via standard supervised learning
on the data distribution, it will see no noisy images during
training, and hence will not necessarily learn to classify
N(x;2I)withx‚Äôs true label. Therefore, in this paper we
follow Lecuyer et al. (2019) and train the base classiÔ¨Åer
with Gaussian data augmentation at variance 2. A justiÔ¨Åca-
tion for this procedure is provided in Appendix F. However,
we suspect that there may be room to improve upon this
training scheme, perhaps by training the base classiÔ¨Åer so
as to maximize the smoothed classiÔ¨Åer‚Äôs certiÔ¨Åed accuracy
at some tunable radius r.
4. Experiments
In adversarially robust classiÔ¨Åcation, one metric of interest
is the certiÔ¨Åed test set accuracy at radiusr, deÔ¨Åned as the
fraction of the test set which gclassiÔ¨Åes correctly with a pre-
diction that is certiÔ¨Åably robust within an `2ball of radius r.
However, ifgis a randomized smoothing classiÔ¨Åer, comput-
ing this quantity exactly is not possible, so we instead report
theapproximate certiÔ¨Åed test set accuracy , deÔ¨Åned as the
fraction of the test set which CERTIFY classiÔ¨Åes correctly
(without abstaining) and certiÔ¨Åes robust with a radius Rr.
Appendix D shows how to convert the approximate certiÔ¨Åed
accuracy into a lower bound on the true certiÔ¨Åed accuracy
that holds with high probability over the randomness in
CERTIFY . However Appendix H.2 demonstrates that whenCertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
radius0.00.20.40.60.81.0certified accuracy=0.12
=0.25
=0.50
=1.00
undefended
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
radius0.00.20.40.60.81.0certified accuracy=0.25
=0.50
=1.00
undefended
Figure 6. Approximate certiÔ¨Åed accuracy attained by randomized
smoothing on CIFAR-10 ( top) and ImageNet ( bottom ). The hyper-
parametercontrols a robustness/accuracy tradeoff. The dashed
black line is an upper bound on the empirical robust accuracy of
an undefended classiÔ¨Åer with the base classiÔ¨Åer‚Äôs architecture.

is small, the difference between these two quantities is
negligible. Therefore, in our experiments we omit the step
for simplicity and report approximate certiÔ¨Åed accuracies.
In all experiments, unless otherwise stated, we ran CERTIFY
with
= 0:001, so there was at most a 0.1% chance that
CERTIFY returned a radius in which gwas not truly robust.
Unless otherwise stated, when running CERTIFY we used
n0=100 Monte Carlo samples for selection and n=
100,000 samples for estimation.
In the Ô¨Ågures above that plot certiÔ¨Åed accuracy as a function
of radiusr, the certiÔ¨Åed accuracy always decreases gradually
withruntil reaching some point where it plummets to zero.
This drop occurs because for each noise level and number
of samplesn, there is a hard upper limit to the radius we can
certify with high probability, achieved when all nsamples
are classiÔ¨Åed by fas the same class.
ImageNet and CIFAR-10 results We applied random-
ized smoothing to CIFAR-10 (Krizhevsky, 2009) and Im-
ageNet (Deng et al., 2009). On each dataset we trained
several smoothed classiÔ¨Åers, each with a different . On
CIFAR-10 our base classiÔ¨Åer was a 110-layer residual
network; certifying each example took 15 seconds on an
NVIDIA RTX 2080 Ti. On ImageNet our base classiÔ¨Åer
was a ResNet-50; certifying each example took 110 seconds.
We also trained a neural network with the base classiÔ¨Åer‚Äôs
architecture on clean data, and subjected it to a DeepFool `2
adversarial attack (Moosavi-Dezfooli et al., 2016), in order
0.0 0.5 1.0 1.5 2.0 2.5 3.0
radius0.00.20.40.60.81.0certified accuracysmoothing, large network 
smoothing, small network
(Wong et al, 2018) 1
(Wong et al, 2018) 2
(Wong et al, 2018) 3Figure 7. Comparison betwen randomized smoothing and Wong
et al. (2018). Each green line is a small resnet classiÔ¨Åer trained and
certiÔ¨Åed using the method of Wong et al. (2018) with a different
setting of its hyperparameter . The purple line is our method
using the same small resnet architecture as the base classiÔ¨Åer;
the blue line is our method with a larger neural network as the
base classiÔ¨Åer. Wong et al. (2018) gives deterministic robustness
guarantees, whereas smoothing gives high-probability guaranatees;
therefore, we plot here the certiÔ¨Åed accuracy of Wong et al. (2018)
against the ‚Äúapproximate‚Äù certiÔ¨Åed accuracy of smoothing.
to obtain an empirical upper bound on its robust accuracy.
We certiÔ¨Åed the full CIFAR-10 test set and a subsample of
500 examples from the ImageNet test set.
Figure 6 plots the certiÔ¨Åed accuracy attained by smoothing
with each. The dashed black line is the empirical upper
bound on the robust accuracy of the base classiÔ¨Åer architec-
ture; observe that smoothing improves substantially upon
the robustness of the undefended base classiÔ¨Åer architecture.
We see thatcontrols a robustness/accuracy tradeoff. When
is low, small radii can be certiÔ¨Åed with high accuracy, but
large radii cannot be certiÔ¨Åed. When is high, larger radii
can be certiÔ¨Åed, but smaller radii are certiÔ¨Åed at a lower ac-
curacy. This observation echoes the Ô¨Ånding in Tsipras et al.
(2019) that adversarially trained networks with higher ro-
bust accuracy tend to have lower standard accuracy. Tables
of these results are in Appendix E.
Figure 8 ( left) plots the certiÔ¨Åed accuracy obtained using our
Theorem 1 guarantee alongside the certiÔ¨Åed accuracy ob-
tained using the analogous bounds of Lecuyer et al. (2019)
and Li et al. (2018). Since our expression for the certiÔ¨Åed
radiusRis greater (and, in fact, tight), our bound delivers
higher certiÔ¨Åed accuracies. Figure 8 ( middle ) projects how
the certiÔ¨Åed accuracy would have changed had CERTIFY
used more or fewer samples n(under the assumption that the
relative class proportions in counts would have remained
constant). Finally, Figure 8 ( right ) plots the certiÔ¨Åed accu-
racy as the conÔ¨Ådence parameter 
is varied. Observe that
the certiÔ¨Åed accuracy is not very sensitive to 
.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
0.0 0.2 0.4 0.6 0.8 1.0
radius0.00.20.40.60.81.0certified accuracyours
(Lecuyer et al, 2018)
(Li et al, 2018)
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
radius0.00.20.40.60.81.0certified accuracyn = 1,000
n = 10,000
n = 100,000
n = 1,000,000
n = 10,000,000
0.0 0.2 0.4 0.6 0.8 1.0
radius0.00.20.40.60.81.0certified accuracy99.999% confidence
99.99% confidence
99.9% confidence
99% confidence
Figure 8. Experiments with randomized smoothing on ImageNet with = 0:25.Left: certiÔ¨Åed accuracies obtained using our Theorem 1
versus those obtained using the robustness guarantees derived in prior work. Middle : projections for the certiÔ¨Åed accuracy if the number
of samplesnused by C ERTIFY had been larger or smaller. Right : certiÔ¨Åed accuracy as the failure probability 
of C ERTIFY is varied.
Comparison to baselines We compared randomized
smoothing to three baseline approaches for certiÔ¨Åed `2ro-
bustness: the duality approach from Wong et al. (2018),
the Lipschitz approach from Tsuzuku et al. (2018), and the
approach from Weng et al. (2018a); Zhang et al. (2018).
The strongest baseline was Wong et al. (2018); we defer the
comparison to the other two baselines to Appendix H.
In Figure 7, we compare the largest publicly released model
from Wong et al. (2018), a small resnet, to two randomized
smoothing classiÔ¨Åers: one which used the same small resnet
architecture for its base classiÔ¨Åer, and one which used a
larger 110-layer resnet for its base classiÔ¨Åer. First, observe
that smoothing with the large 110-layer resnet substantially
outperforms the baseline (across all hyperparameter set-
tings) at all radii. Second, observe that smoothing with the
small resnet also outperformed the method of Wong et al.
(2018) at all but the smallest radii. We attribute this latter re-
sult to the fact that neural networks trained using the method
of Wong et al. (2018) are ‚Äútypically overregularized to the
point that many Ô¨Ålters/weights become identically zero,‚Äù per
that paper. In contrast, the base classiÔ¨Åer in randomized
smoothing is a fully expressive neural network.
Prediction It is computationally expensive to certify the
robustness of garound a point x, since the value of nin
CERTIFY must be very large. However, it is far cheaper
to evaluategatxusing PREDICT , sincencan be small.
For example, when we ran PREDICT on ImageNet ( =
0:25) usingn=100, making each prediction only took
0.15 seconds, and we attained a top-1 test accuracy of 65%
(Appendix E).
As discussed earlier, an adversary can potentially force PRE-
DICT to abstain with high probability. However, it is rela-
tively rare for PREDICT to abstain on the actual data dis-
tribution. On ImageNet ( = 0:25),PREDICT with failure
probability
= 0:001abstained 12% of the time when n=
100, 4% when n=1000, and 1% when n=10,000.Empirical tightness of bound Whenfis linear, there al-
ways exists a class-changing perturbation just beyond the
certiÔ¨Åed radius. Since neural networks are not linear, we em-
pirically assessed the tightness of our bound by subjecting
an ImageNet smoothed classiÔ¨Åer ( = 0:25) to a projected
gradient descent-style adversarial attack (Appendix J.3). For
each example, we ran CERTIFY with
= 0:01, and, if the
example was correctly classiÔ¨Åed and certiÔ¨Åed robust at ra-
diusR, we tried Ô¨Ånding an adversarial example for gwithin
radius 1:5Rand within radius 2R. We succeeded 17% of
the time at radius 1:5Rand 53% of the time at radius 2R.
5. Conclusion
Theorem 2 establishes that smoothing with Gaussian noise
naturally confers adversarial robustness in `2norm: if we
have no knowledge about the base classiÔ¨Åer beyond the dis-
tribution off(x+"), then the set of perturbations to which
the smoothed classiÔ¨Åer is provably robust is precisely an `2
ball. We suspect that smoothing with other noise distribu-
tions may lead to similarly natural robustness guarantees for
other perturbation sets such as general `pnorm balls.
Our strong empirical results suggest that randomized
smoothing is a promising direction for future research
into adversarially robust classiÔ¨Åcation. Many empirical
approaches have been ‚Äúbroken,‚Äù and provable approaches
based on certifying neural network classiÔ¨Åers have not been
shown to scale to networks of modern size. It seems to be
computationally infeasible to reason in any sophisticated
way about the decision boundaries of a large, expressive neu-
ral network. Randomized smoothing circumvents this prob-
lem: the smoothed classiÔ¨Åer is not itself a neural network,
though it leverages the discriminative ability of a neural
network base classiÔ¨Åer. To make the smoothed classiÔ¨Åer ro-
bust, one need simply make the base classiÔ¨Åer classify well
under noise. In this way, randomized smoothing reduces the
unsolved problem of adversarially robust classiÔ¨Åcation to
the comparably solved domain of supervised learning.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
6. Acknowledgements
We thank Mateusz Kwa ¬¥snicki for help with Lemma 4 in the
appendix, Aaditya Ramdas for pointing us toward the work
of Hung & Fithian (2019), and Siva Balakrishnan for helpful
discussions regarding the conÔ¨Ådence interval in Appendix
D. We thank Tolani Olarinre, Adarsh Prasad, Ben Cousins,
Ramon Van Handel, Matthias Lecuyer, and Bai Li for useful
conversations. Finally, we are very grateful to Vaishnavh
Nagarajan, Arun Sai Suggala, Shaojie Bai, Mikhail Khodak,
Han Zhao, and Zachary Lipton for reviewing drafts of this
work. Jeremy Cohen is supported by a grant from the Bosch
Center for AI.
References
Anil, C., Lucas, J., and Grosse, R. B. Sorting out lips-
chitz function approximation. In Proceedings of the 36th
International Conference on Machine Learning , 2019.
Athalye, A. and Carlini, N. On the robustness of the cvpr
2018 white-box adversarial example defenses. The Bright
and Dark Sides of Computer Vision: Challenges and
Opportunities for Privacy and Security , 2018.
Athalye, A., Carlini, N., and Wagner, D. Obfuscated gra-
dients give a false sense of security: Circumventing de-
fenses to adversarial examples. In Proceedings of the 35th
International Conference on Machine Learning , 2018.
Biggio, B., Corona, I., Maiorca, D., Nelson, B., rndi, N.,
Laskov, P., Giacinto, G., and Roli, F. Evasion attacks
against machine learning at test time. Joint European
Conference on Machine Learning and Knowledge Dis-
covery in Database , 2013.
Blanchard, G. Lecture Notes, 2007. URL
http://www.math.uni-potsdam.de/
Àúblanchard/lectures/lect_2.pdf .
Bunel, R. R., Turkaslan, I., Torr, P., Kohli, P., and
Mudigonda, P. K. A uniÔ¨Åed view of piecewise linear
neural network veriÔ¨Åcation. In Advances in Neural Infor-
mation Processing Systems 31 . 2018.
Cao, X. and Gong, N. Z. Mitigating evasion attacks to deep
neural networks via region-based classiÔ¨Åcation. 33rd An-
nual Computer Security Applications Conference , 2017.
Carlini, N. and Wagner, D. Adversarial examples are not
easily detected: Bypassing ten detection methods. In
Proceedings of the 10th ACM Workshop on ArtiÔ¨Åcial
Intelligence and Security , 2017.
Carlini, N., Katz, G., Barrett, C., and Dill, D. L. Provably
minimally-distorted adversarial examples. arXiv preprint
arXiv: 1709.10207 , 2017.Cheng, C.-H., Nhrenberg, G., and Ruess, H. Maximum
resilience of artiÔ¨Åcial neural networks. International
Symposium on Automated Technology for VeriÔ¨Åcation
and Analysis , 2017.
Cisse, M., Bojanowski, P., Grave, E., Dauphin, Y ., and
Usunier, N. Parseval networks: Improving robustness to
adversarial examples. In Proceedings of the 34th Interna-
tional Conference on Machine Learning , 2017.
Clopper, C. J. and Pearson, E. S. The use of conÔ¨Ådence
or Ô¨Åducial limits illustrated in the case of the binomial.
Biometrika , 26(4):pp. 404‚Äì413, 1934. ISSN 00063444.
Croce, F., Andriushchenko, M., and Hein, M. Provable
robustness of relu networks via maximization of linear
regions. In Proceedings of the 22nd International Con-
ference on ArtiÔ¨Åcial Intelligence and Statistics , 2019.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-
Fei, L. ImageNet: A Large-Scale Hierarchical Image
Database. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) , 2009.
Dodge, S. and Karam, L. A study and comparison of hu-
man and deep learning recognition performance under
visual distortions. 2017 26th International Conference on
Computer Communication and Networks (ICCCN) , 2017.
Dutta, S., Jha, S., Sanakaranarayanan, S., and Tiwari, A.
Output range analysis for deep neural networks. arXiv
preprint arXiv:1709.09130 , 2017.
Dvijotham, K., Gowal, S., Stanforth, R., Arandjelovic, R.,
O‚ÄôDonoghue, B., Uesato, J., and Kohli, P. Training
veriÔ¨Åed learners with learned veriÔ¨Åers. arXiv preprint
arXiv:1805.10265 , 2018a.
Dvijotham, K., Stanforth, R., Gowal, S., Mann, T., and
Kohli, P. A dual approach to scalable veriÔ¨Åcation of
deep networks. Proceedings of the Thirty-Fourth Con-
ference Annual Conference on Uncertainty in ArtiÔ¨Åcial
Intelligence (UAI-18) , 2018b.
Ehlers, R. Formal veriÔ¨Åcation of piece-wise linear feed-
forward neural networks. In Automated Technology for
VeriÔ¨Åcation and Analysis , 2017.
Fawzi, A., Moosavi-Dezfooli, S.-M., and Frossard, P. Ro-
bustness of classiÔ¨Åers: from adversarial to random noise.
InAdvances in Neural Information Processing Systems
29. 2016.
Fischetti, M. and Jo, J. Deep neural networks and mixed
integer linear optimization. Constraints , 23(3):296‚Äì309,
July 2018.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
Ford, N., Gilmer, J., and Cubuk, E. D. Adversarial ex-
amples are a natural consequence of test error in noise.
InProceedings of the 36th International Conference on
Machine Learning , 2019.
Franceschi, J.-Y ., Fawzi, A., and Fawzi, O. Robustness
of classiÔ¨Åers to uniform `pand gaussian noise. In 21st
International Conference on ArtiÔ¨Åcial Intelligence and
Statistics (AISTATS) . 2018.
Gehr, T., Mirman, M., Drachsler-Cohen, D., Tsankov, P.,
Chaudhuri, S., and Vechev, M. T. AI2: safety and ro-
bustness certiÔ¨Åcation of neural networks with abstract
interpretation. In 2018 IEEE Symposium on Security and
Privacy, SP 2018, Proceedings, 21-23 May 2018, San
Francisco, California, USA , pp. 3‚Äì18, 2018.
Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining
and harnessing adversarial examples. In International
Conference on Learning Representations , 2015.
Gouk, H., Frank, E., Pfahringer, B., and Cree, M. Regulari-
sation of neural networks by enforcing lipschitz continu-
ity.arXiv preprint arXiv:1804.04368 , 2018.
Gowal, S., Dvijotham, K., Stanforth, R., Bunel, R., Qin,
C., Uesato, J., Arandjelovic, R., Mann, T., and Kohli, P.
On the effectiveness of interval bound propagation for
training veriÔ¨Åably robust models, 2018.
Hein, M. and Andriushchenko, M. Formal guarantees on the
robustness of a classiÔ¨Åer against adversarial manipulation.
InAdvances in Neural Information Processing Systems
30. 2017.
Huang, X., Kwiatkowska, M., Wang, S., and Wu, M. Safety
veriÔ¨Åcation of deep neural networks. Computer Aided
VeriÔ¨Åcation , 2017.
Hung, K. and Fithian, W. Rank veriÔ¨Åcation for exponential
families. The Annals of Statistics , (2):758‚Äì782, 04 2019.
Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial
logit pairing. arXiv preprint arXiv:1803.06373 , 2018.
Katz, G., Barrett, C., Dill, D. L., Julian, K., and Kochender-
fer, M. J. Reluplex: An efÔ¨Åcient smt solver for verifying
deep neural networks. Lecture Notes in Computer Sci-
ence, pp. 97117, 2017. ISSN 1611-3349.
Kolter, J. Z. and Madry, A. Adversarial ro-
bustness: Theory and practice. https:
//adversarial-ml-tutorial.org/
adversarial_examples/ , 2018.
Krizhevsky, A. Learning multiple layers of features from
tiny images. Technical report, 2009.Kurakin, A., Goodfellow, I. J., and Bengio, S. Adver-
sarial machine learning at scale. 2017. URL https:
//arxiv.org/abs/1611.01236 .
Lecuyer, M., Atlidakis, V ., Geambasu, R., Hsu, D., and
Jana, S. CertiÔ¨Åed robustness to adversarial examples with
differential privacy. In IEEE Symposium on Security and
Privacy (SP) , 2019.
Levine, A., Singla, S., and Feizi, S. CertiÔ¨Åably ro-
bust interpretation in deep learning. arXiv preprint
arXiv:1905.12105 , 2019.
Li, B., Chen, C., Wang, W., and Carin, L. Second-order ad-
versarial attack and certiÔ¨Åable robustness. arXiv preprint
arXiv:1809.03113 , 2018.
Liu, X., Cheng, M., Zhang, H., and Hsieh, C.-J. Towards
robust neural networks via random self-ensemble. In
The European Conference on Computer Vision (ECCV) ,
September 2018.
Lomuscio, A. and Maganti, L. An approach to reachability
analysis for feed-forward relu neural networks, 2017.
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and
Vladu, A. Towards deep learning models resistant to
adversarial attacks. In International Conference on Learn-
ing Representations , 2018.
Mirman, M., Gehr, T., and Vechev, M. Differentiable ab-
stract interpretation for provably robust neural networks.
InProceedings of the 35th International Conference on
Machine Learning , 2018.
Moosavi-Dezfooli, S.-M., Fawzi, A., and Frossard, P. Deep-
fool: A simple and accurate method to fool deep neural
networks. 2016 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , 2016.
Neyman, J. and Pearson, E. S. On the problem of the most
efÔ¨Åcient tests of statistical hypotheses. Philosophical
Transactions of the Royal Society of London. Series A,
Containing Papers of a Mathematical or Physical Char-
acter , 231:289‚Äì337, 1933.
Raghunathan, A., Steinhardt, J., and Liang, P. CertiÔ¨Åed
defenses against adversarial examples. In International
Conference on Learning Representations , 2018a.
Raghunathan, A., Steinhardt, J., and Liang, P. SemideÔ¨Å-
nite relaxations for certifying robustness to adversarial
examples. In Advances in Neural Information Processing
Systems 31 , 2018b.
Salman, H., Yang, G., Li, J., Zhang, P., Zhang, H., Razen-
shteyn, I., and Bubeck, S. Provably robust deep learn-
ing via adversarially trained smoothed classiÔ¨Åers. arXiv
preprint arXiv:1906.04584 , 2019.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
Singh, G., Gehr, T., Mirman, M., P ¬®uschel, M., and Vechev,
M. Fast and effective robustness certiÔ¨Åcation. In Ad-
vances in Neural Information Processing Systems 31 .
2018.
Smilkov, D., Thorat, N., Kim, B., Vigas, F., and Wattenberg,
M. Smoothgrad: removing noise by adding noise. arXiv
preprint arXiv:1706.03825 , 2017.
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,
D., Goodfellow, I., and Fergus, R. Intriguing proper-
ties of neural networks. In International Conference on
Learning Representations , 2014.
Tjeng, V ., Xiao, K. Y ., and Tedrake, R. Evaluating robust-
ness of neural networks with mixed integer programming.
InInternational Conference on Learning Representations ,
2019. URL https://openreview.net/forum?
id=HyGIdiRqtm .
Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and
Madry, A. Robustness may be at odds with accuracy. In
International Conference on Learning Representations ,
2019. URL https://openreview.net/forum?
id=SyxAb30cY7 .
Tsuzuku, Y ., Sato, I., and Sugiyama, M. Lipschitz-margin
training: Scalable certiÔ¨Åcation of perturbation invariance
for deep neural networks. In Advances in Neural Infor-
mation Processing Systems 31 . 2018.
Uesato, J., O‚ÄôDonoghue, B., Kohli, P., and van den Oord,
A. Adversarial risk and the dangers of evaluating against
weak attacks. In Proceedings of the 35th International
Conference on Machine Learning , 2018.
Wang, S., Chen, Y ., Abdou, A., and Jana, S. Mixtrain: Scal-
able training of formally robust neural networks. arXiv
preprint arXiv:1811.02625 , 2018a.
Wang, S., Pei, K., Whitehouse, J., Yang, J., and Jana, S.
EfÔ¨Åcient formal safety analysis of neural networks. In
Advances in Neural Information Processing Systems 31 .
2018b.
Webb, S., Rainforth, T., Teh, Y . W., and Kumar, M. P.
Statistical veriÔ¨Åcation of neural networks. In In-
ternational Conference on Learning Representations ,
2019. URL https://openreview.net/forum?
id=S1xcx3C5FX .
Weng, L., Zhang, H., Chen, H., Song, Z., Hsieh, C.-J.,
Daniel, L., Boning, D., and Dhillon, I. Towards fast
computation of certiÔ¨Åed robustness for ReLU networks.
InProceedings of the 35th International Conference on
Machine Learning , 2018a.Weng, T.-W., Zhang, H., Chen, P.-Y ., Yi, J., Su, D., Gao, Y .,
Hsieh, C.-J., and Daniel, L. Evaluating the robustness of
neural networks: An extreme value theory approach. In
International Conference on Learning Representations ,
2018b.
Wong, E. and Kolter, J. Z. Provable defenses against adver-
sarial examples via the convex outer adversarial polytope.
InProceedings of the 35th International Conference on
Machine Learning , 2018.
Wong, E., Schmidt, F., Metzen, J. H., and Kolter, J. Z.
Scaling provable adversarial defenses. In Advances in
Neural Information Processing Systems 31 , 2018.
Zantedeschi, V ., Nicolae, M.-I., and Rawat, A. EfÔ¨Åcient de-
fenses against adversarial attacks. Proceedings of the 10th
ACM Workshop on ArtiÔ¨Åcial Intelligence and Security -
AISec 17 , 2017.
Zhang, H., Weng, T.-W., Chen, P.-Y ., Hsieh, C.-J., and
Daniel, L. EfÔ¨Åcient neural network robustness certiÔ¨Åca-
tion with general activation functions. In Advances in
Neural Information Processing Systems 31 . 2018.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
A. Proofs of Theorems 1 and 2
Here we provide the complete proofs for Theorem 1 and Theorem 2. We Ô¨Åst prove the following lemma, which is essentially
a restatement of the Neyman-Pearson lemma (Neyman & Pearson, 1933) from statistical hypothesis testing.
Lemma 3 (Neyman-Pearson ).LetXandYbe random variables in Rdwith densities XandY. Leth:Rd!f0;1g
be a random or deterministic function. Then:
1. IfS=n
z2Rd:Y(z)
X(z)to
for somet>0andP(h(X) = 1)P(X2S), thenP(h(Y) = 1)P(Y2S).
2. IfS=n
z2Rd:Y(z)
X(z)to
for somet>0andP(h(X) = 1)P(X2S), thenP(h(Y) = 1)P(Y2S).
Proof. Without loss of generality, we assume that his random and write h(1jx)for the probability that h(x) = 1 .
First we prove part 1. We denote the complement of SasSc.
P(h(Y) = 1) P(Y2S) =Z
Rdh(1jz)Y(z)dz Z
SY(z)dz
=Z
Sch(1jz)Y(z)dz+Z
Sh(1jz)Y(z)dz
 Z
Sh(1jz)Y(z)dz+Z
Sh(0jz)Y(z)dz
=Z
Sch(1jz)Y(z)dz Z
Sh(0jz)Y(z)dz
tZ
Sch(1jz)X(z)dz Z
Sh(0jz)X(z)
=tZ
Sch(1jz)X(z)dz+Z
Sh(1jz)X(z)dz Z
Sh(1jz)X(z)dz Z
Sh(0jz)X(z)
=tZ
Rdh(1jz)X(z)dz Z
SX(z)dz
=t[P(h(X) = 1) P(X2S)]
0
The inequality in the middle is due to the fact that Y(z)tX(z)8z2SandY(z)>tX(z)8z2Sc. The inequality
at the end is because both terms in the product are non-negative by assumption.
The proof for part 2 is virtually identical, except both ‚Äú ‚Äù become ‚Äú.‚Äù
Remark: connection to statistical hypothesis testing. Part 2 of Lemma 3 is known in the Ô¨Åeld of statistical hypothesis
testing as the Neyman-Pearson Lemma (Neyman & Pearson, 1933). The hypothesis testing problem is this: we are given a
sample that comes from one of two distributions over Rd: either the null distribution Xor the alternative distribution Y.
We would like to identify which distribution the sample came from. It is worse to say ‚Äú Y‚Äù when the true answer is ‚Äú X‚Äù
than to say ‚Äú X‚Äù when the true answer is ‚Äú Y.‚Äù Therefore we seek a (potentially randomized) procedure h:Rd!f0;1g
which returns ‚Äú Y‚Äù when the sample really came from Xwith probability no greater than some failure rate 
. In particular,
out of all such rules h, we would like the uniformly most powerful oneh, i.e. the rule which is most likely to correctly
say ‚ÄúY‚Äù when the sample really came from Y. Neyman & Pearson (1933) showed that his the rule which returns ‚Äú Y‚Äù
deterministically on the set S=fz2Rd:Y(z)
X(z)tgfor whichever tmakes P(X2S) =
. In other words, to state
this in a form that looks like Part 2 of Lemma 3: if his a different rule with P(h(X) = 1)
, thenhis more powerful
thanh, i.e.P(h(Y) = 1)P(Y2S).
Now we state the special case of Lemma 3 for when XandYare isotropic Gaussians.
Lemma 4 (Neyman-Pearson for Gaussians with different means ).LetXN(x;2I)andYN(x+;2I). Let
h:Rd!f0;1gbe any deterministic or random function. Then:
1. IfS=
z2Rd:Tz	
for someandP(h(X) = 1)P(X2S), thenP(h(Y) = 1)P(Y2S)CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
2. IfS=
z2Rd:Tz	
for someandP(h(X) = 1)P(X2S), thenP(h(Y) = 1)P(Y2S)
Proof. This lemma is the special case of Lemma 3 when XandYare isotropic Gaussians with means xandx+.
By Lemma 3 it sufÔ¨Åces to simply show that for any , there is some t>0for which:
fz:Tzg=
z:Y(z)
X(z)t
andfz:Tzg=
z:Y(z)
X(z)t
(5)
The likelihood ratio for this choice of XandYturns out to be:
Y(z)
X(z)=exp
 1
22Pd
i=1(zi (xi+i))2)
exp
 1
22Pd
i=1(zi xi)2
= exp 
1
22dX
i=12zii 2
i 2xii!
= exp(aTz+b)
wherea>0andbare constants w.r.t z, speciÔ¨Åcally a=1
2andb= (2Tx+kk2)
22 .
Therefore, given any we may take t= exp(a+b), noticing that
Tz() exp(aTz+b)t
Tz() exp(aTz+b)t
Finally, we prove Theorem 1 and Theorem 2.
Theorem 1 (restated). Letf:Rd!Y be any deterministic or random function. Let "N (0;2I). Letg(x) =
arg maxcP(f(x+") =c). Suppose that for a speciÔ¨Åc x2Rd, there existcA2Y andpA;pB2[0;1]such that:
P(f(x+") =cA)pApBmax
c6=cAP(f(x+") =c) (6)
Theng(x+) =cAfor allkk2<R, where
R=
2( 1(pA)  1(pB)) (7)
Proof. To show that g(x+) =cA, it follows from the deÔ¨Ånition of gthat we need to show that
P(f(x++") =cA)>max
cB6=cAP(f(x++") =cB)
We will prove that P(f(x++") =cA)>P(f(x++") =cB)for every class cB6=cA. Fix one such class cBwithout
loss of generality.
For brevity, deÔ¨Åne the random variables
X:=x+"=N(x;2I)
Y:=x++"=N(x+;2I)
In this notation, we know from (6) that
P(f(X) =cA)pAandP(f(X) =cB)pB (8)CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
x+
xx+
x
Figure 9. Illustration of the proof of Theorem 1. The solid line concentric circles are the density level sets of X:=x+"; the dashed
line concentric circles are the level sets of Y:=x++". The setAis in blue and the set Bis in red. The Ô¨Ågure on the left depicts
a situation where P(Y2A)>P(Y2B), and hence g(x+)may equalcA. The Ô¨Ågure on the right depicts a situation where
P(Y2A)<P(Y2B)and henceg(x+)6=cA.
and our goal is to show that
P(f(Y) =cA)>P(f(Y) =cB) (9)
DeÔ¨Åne the half-spaces:
A:=fz:T(z x)kk 1(pA)g
B:=fz:T(z x)kk 1(1 pB)g
Algebra (deferred to the end) shows that P(X2A) =pA. Therefore, by (8) we know that P(f(X) =cA)P(X2A).
Hence we may apply Lemma 4 with h(z) :=1[f(z) =cA]to conclude:
P(f(Y) =cA)P(Y2A) (10)
Similarly, algebra shows that P(X2B) =pB. Therefore, by (8) we know that P(f(X) =cB)P(X2B). Hence we
may apply Lemma 4 with h(z) :=1[f(z) =cB]to conclude:
P(f(Y) =cB)P(Y2B) (11)
To guarantee (9), we see from (10, 11) that it sufÔ¨Åces to show that P(Y2A)>P(Y2B), as this step completes the chain
of inequalities
P(f(Y) =cA)P(Y2A)>P(Y2B)P(f(Y) =cB) (12)
We can compute the following:
P(Y2A) = 
 1(pA) kk

(13)
P(Y2B) = 
 1(pB) +kk

(14)
Finally, algebra shows that P(Y2A)>P(Y2B)if and only if:
kk<
2( 1(pA)  1(pB)) (15)
which recovers the theorem statement.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
We now restate and prove Theorem 2, which shows that the bound in Theorem 1 is tight. The assumption below in Theorem
2 thatpA+pB1is mild: given any pAandpBwhich do not satisfy this condition, one could have always redeÔ¨Åned
pB 1 pAto obtain a Theorem 1 guarantee with a larger certiÔ¨Åed radius, so there is no reason to invoke Theorem 1
unlesspA+pB1.
Theorem 2 (restated). AssunepA+pB1. For any perturbation 2Rdwithkk2>R, there exists a base classiÔ¨Åer f
consistent with the observed class probabilities (6) such that if fis the base classiÔ¨Åer for g, theng(x+)6=cA.
Proof. We re-use notation from the preceding proof.
Pick any class cBarbitrarily. DeÔ¨Åne AandBas above, and consider the function
f(x) :=8
><
>:cA ifx2A
cB ifx2B
other classes otherwise
This function is well-deÔ¨Åned, since A\B=;provided that pA+pB1.
By construction, the function fsatisÔ¨Åes (6) with equalities, since
P(f(x+") =cA) =P(X2A) =pA P(f(x+") =cB) =P(X2B) =pB
It follows from (13) and (14) that
P(Y2A)<P(Y2B)() kk2>R
By assumption,kk2>R, soP(Y2A)<P(Y2B), or equivalently,
P(f(x++") =cA)<P(f(x++") =cB)
Therefore, if fis the base classiÔ¨Åer for g, theng(x+)6=cA.
A.0.1. D EFERRED ALGEBRA
Claim. P(X2A) =pA
Proof. Recall thatXN(x;2I)andA=fz:T(z x)kk 1(pA)g.
P(X2A) =P(T(X x)kk 1(pA))
=P(TN(0;2I)kk 1(pA))
=P(kkZkk 1(pA)) (ZN(0;1))
= ( 1(pA))
=pA
Claim. P(X2B) =pB
Proof. Recall thatXN(x;2I)andB=fz:T(z x)kk 1(1 pB)g.
P(X2A) =P(T(X x)kk 1(1 pB))
=P(TN(0;2I)kk 1(1 pB))
=P(kkZkk 1(1 pB)) (ZN(0;1))
=P(Z 1(1 pB))
= 1 ( 1(1 pB))
=pBCertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
Claim. P(Y2A) = 
 1(pA) kk

Proof. Recall thatYN(x+;2I)andA=fz:T(z x)kk 1(pA)g.
P(Y2A) =P(T(Y x)kk 1(pA))
=P(TN(0;2I) +kk2kk 1(pA))
=P(kkZkk 1(pA) kk2) (ZN(0;1))
=P
Z 1(pA) kk

= 
 1(pA) kk

Claim. P(Y2B) = 
 1(pB) +kk

Proof. Recall thatYN(x+;2I)andB=fz:T(z x)kk 1(1 pB)g.
P(Y2B) =P(T(Y x)kk 1(1 pB))
=P(TN(0;2I) +kk2kk 1(1 pB))
=P(kkZ+kk2kk 1(1 pB)) (ZN(0;1))
=P
Z 1(1 pB) kk

=P
Z 1(pB) +kk

= 
 1(pB) +kk
CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
B. Smoothing a two-class linear classiÔ¨Åer
In this appendix, we analyze what happens when the base classiÔ¨Åer fis a two-class linear classiÔ¨Åer f(x) =sign(wTx+b).
To match the deÔ¨Ånition of g, we take sign ()to be undeÔ¨Åned when its argument is zero.
xx
Figure 10. Illustration of Proposition 3. A binary linear classiÔ¨Åer f(x) =sign(wTx+b)partitions Rdinto two half-spaces, drawn here
in blue and red. An isotropic Gaussian N(x;2I)will put more mass on whichever half-space its center xlies in: in the Ô¨Ågure on
the left,xis in the blue half-space and N(x;2I)puts more mass on the blue than on red. In the Ô¨Ågure on the right, xis in the red
half-space andN(x;2I)puts more mass on red than on blue. Since the smoothed classiÔ¨Åer‚Äôs prediction g(x)is deÔ¨Åned to be whichever
half-spaceN(x;2I)puts more mass in, and the base classiÔ¨Åer‚Äôs prediction f(x)is deÔ¨Åned to be whichever half-space xis in, we have
thatg(x) =f(x)for allx.
Our Ô¨Årst result is that when fis a two-class linear classiÔ¨Åer, the smoothed classiÔ¨Åer gis identical to the base classiÔ¨Åer f.
Proposition 3. Iffis a two-class linear classiÔ¨Åer f(x) =sign(wTx+b), andgis the smoothed version of fwith any,
theng(x) =f(x)for anyx(wherefis deÔ¨Åned).
Proof. From the deÔ¨Ånition of g,
g(x) = 1()P"(f(x+") = 1)>1
2("N(0;2I))
()P" 
sign(wT(x+") +b) = 1
>1
2
()P" 
wTx+wT"+b0
>1
2
()P 
kwkZ wTx b
>1
2(ZN(0;1))
()P
ZwTx+b
kwk
>1
2
()wTx+b
kwk>0
()wTx+b>0
()f(x) = 1
A similar calculation shows that g(x) = 1()f(x) = 1.
A two-class linear classiÔ¨Åer f(x) =sign(wTx+b)is already certiÔ¨Åable: the distance from any point xto the decision
boundary is (wTx+b)=kwk2, and no distance with `2norm strictly less than this distance can possibly change f‚Äôs prediction.
Letgbe a smoothed version of f. By Proposition 3, gis identical to f, so it follows that gis truly robust around any
inputxwithin the`2radius (wTx+b)=kwk2. We now show that Theorem 1 will certify this radius, rather than a smaller,
over-conservative radius.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
Proposition 4. Iffis a two-class linear classiÔ¨Åer f(x) =sign(wTx+b), andgis the smoothed version of fwith any
, then invoking Theorem 1 at any x(wherefis deÔ¨Åned) with pA=pAandpB=pBwill yield the certiÔ¨Åed radius
R=jwTx+bj
kwk.
Proof. In binary classiÔ¨Åcation, pA= 1 pB, so Theorem 1 returns R= 1(pA).
We have:
pA=P"(f(x+") =g(x))
=P"(sign(wT(x+") +b) =sign(wTx+b)) (By Proposition 3, g(x) =f(x))
=P"(sign(wTx+kwkZ+b) =sign(wTx+b))
There are two cases: if wTx+b>0, then
pA=P"(wTx+kwkZ+b>0)
=P"
Z > wTx b
kwk
=P"
Z <wTx+b
kwk
= wTx+b
kwk
On the other hand, if wTx+b<0, then
pA=P"(wTx+kwkZ+b<0)
=P"
Z < wTx b
kwk
=  wTx b
kwk
In either case, we have:
pA= jwTx+bj
kwk
Therefore, the bound in Theorem 1 returns a radius of
R= 1(pA)
=jwTx+bj
kwk
The previous two propositions imply that when fis a two-class linear classiÔ¨Åer, the Theorem 1 bound is ‚Äútight‚Äù in the sense
that there always exists a class-changing perturbation just beyond the certiÔ¨Åed radius.3
Proposition 5. Letfbe a two-class linear classiÔ¨Åer f(x) =sign(wTx+b), letgbe the smoothed version of ffor some,
letxbe any point (where fis deÔ¨Åned), and let Rbe the radius certiÔ¨Åed around xby Theorem 1. Then for any radius r>R ,
there exists a perturbation withkk2=rfor whichg(x+)6=g(x).
3Note that this is a different sense of ‚Äútight‚Äù than the sense in which Theorem 2 proves that Theorem 1 is tight. Theorem 2 proves that
for any Ô¨Åxed perturbation outside the radius certiÔ¨Åed by Theorem 1, there exists a base classiÔ¨Åer ffor whichg(x+)6=g(x). In
contrast, Proposition 5 proves that for any Ô¨Åxed binary linear base classiÔ¨Åer f, there exists a perturbation just outside the radius certiÔ¨Åed
by Theorem 1 for which g(x+)6=g(x).CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
Proof. By Proposition 3 it sufÔ¨Åces to show that there exists some perturbation withkk2=rfor whichf(x+)6=f(x).
By Proposition 4, we know that R=jwTx+bj
kwk2.
IfwTx+b>0, consider the perturbation = w
kwk2r. This perturbation satisÔ¨Åes kk2=rand
wT(x+) +b=wTx+b+wT
=wTx+b kwk2r
<wTx+b kwk2R
=wTx+b jwTx+bj
=wTx+b (wTx+b)
= 0
implying that f(x+) = 1.
Likewise, ifwTx+b<0, then consider the perturbation =w
kwk2r. This perturbation satisÔ¨Åes kk2=randf(x+) = 1.
x xx+
Figure 11. Left: Illustration of of Proposition 4. The red/blue half-spaces are the decision regions of both the base classiÔ¨Åer fand the
smoothed classiÔ¨Åer g. (Since the base classiÔ¨Åer is binary linear, g=feverywhere.) The black circle is the robustness radius RcertiÔ¨Åed by
Theorem 1. Right : Illustration of Proposition 5. For any r>R , there exists a perturbation withkk2=rfor whichg(x+)6=g(x).
This special property of two-class linear classiÔ¨Åers is not true in general. In fact, it is possible to construct situations where
g‚Äôs prediction around some point x0is robust at radius1, yet Theorem 1 only certiÔ¨Åes a radius of , whereis arbitrarily
close to zero.
Proposition 6. For any > 0, there exists a base classiÔ¨Åer fand an input x0for which the corresponding smoothed
classiÔ¨Åergis robust around x0at radius1, yet Theorem 1 only certiÔ¨Åes a radius of aroundx0.
Proof. Lett=  1(1
2())and consider the following base classiÔ¨Åer:
f(x) =8
><
>:1 ifx< t
 1if txt
1 ifx>t
Letgbe the smoothed version of fwith= 1. We will show that g(x) = 1 everywhere, implying that g‚Äôs prediction is
robust around x0= 0with radius1. Yet Theorem 1 only certiÔ¨Åes a radius of aroundx0.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
LetZN(0;1). For anyx, we have:
P(f(x+") = 1) =P( tx+"t)
=P[ t xZt x]
P[ tZt] (apply Lemma 5 below with `= t x)
= 1 2( t)
= 1 ()
<1
2:
Therefore,g(x) = 1 for allx.
Meanwhile, at x0= 0, we have:
P(f(x0+") = 1) = P(f(") = 1)
=P(Z < torZ >t )
= 2( t)
= ();
so by Theorem 1, the certiÔ¨Åed radius around x0isR=.
The proof of Proposition 6 employed the following lemma, which formalizes the visually obvious fact that out of all intervals
of some Ô¨Åxed width 2t, the interval with maximal mass under the standard normal distribution Zis the interval [ t;t].
Lemma 5. LetZN(0;1). For any`2R,t>0, we have P(`Z`+ 2t)P( tZt).
Proof. Letbe the PDF of the standard normal distribution. Since is symmetric about the origin (i.e. (x) =( x)8x),
P( tZt) = 2Zt
0(x)dx:
There are two cases to consider:
Case 1: The interval [`;`+ 2t]is entirely positive, i.e. `0, or[`;`+ 2t]is entirely negative, i.e. `+ 2t0.
First, we use the fact that is symmetric about the origin to rewrite P(`Z`+ 2t)as the probability that Zfalls in a
non-negative interval [a;a+ 2t]for somea.
SpeciÔ¨Åcally, if `0, then leta=`. Else, if`+ 2t0, then leta= (`+ 2t). We therefore have:
P(`Z`+ 2t) =P(aZa+ 2t):
Therefore:
P( tZt) P(`Z`+ 2t) =Zt
0(x)dx Za+t
a(x)dx+Zt
0(x)dx Za+2t
a+t(x)dx
=Za+t
a(x a)dx Za+t
a(x)dx+Za+2t
a+t(x a t)dx Za+2t
a+t(x)dx
=Za+t
a[(x a) (x)]dx+Za+2t
a+t[(x a t) (x)]dx
Za+t
a0dx+Za+2t
a+t0dx
= 0CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
where the inequality is because is monotonically decreasing on [0;1).
Case 2:Iis partly positive, partly negative, i.e. `<0<`+ 2t.
First, we use the fact that is symmetric about the origin to rewrite P(`Z`+ 2t)as the sum of the probabilities that
Zfalls in two non-negative intervals [0;a]and[0;b]for somea;b.
SpeciÔ¨Åcally, let a= min( `;`+ 2t)andb= max( `;`+ 2t). We therefore have:
P(`Z`+ 2t) =P(0Za) +P(0Zb):
Note that by construction, a+b= 2t, and 0atandtb2t.
We have:
P( tZt) P(`Z`+ 2t) =Zt
0(x)dx Za
0(x)dx
 "Zb
0(x)dx Zt
0(x)dx!
=Zt
a(x)dx Zb
t(x)dx
=Zt
a(x)dx Z2t a
t(x)dx
=Zt
a(x)dx Zt
a(x a+t)dx
=Zt
a((x) (x a+t))dx
Zt
a0dx
= 0
where the inequality is again because is monotonically decreasing on [0;1).CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
C. Practical algorithms
In this appendix, we elaborate on the prediction and certiÔ¨Åcation algorithms described in Section 3.2. The pseudocode in
Section 3.2 makes use of several helper functions:
SAMPLE UNDER NOISE (f,x, num,) works as follows:
1. Draw num samples of noise, "1:::" numN(0;2I).
2. Run the noisy images through the base classiÔ¨Åer fto obtain the predictions f(x+"1);:::;f (x+"num).
3. Return the counts for each class, where the count for class cis deÔ¨Åned asPnum
i=11[f(x+"i) =c].
BINOM PVALUE (nA,nA+nB,p) returns the p-value of the two-sided hypothesis test that nABinomial (nA+nB;p).
Using scipy.stats.binom test , this can be implemented as: binom test(nA, nA + nB, p) .
LOWER CONFBOUND (k,n,1 
) returns a one-sided (1 
)lower conÔ¨Ådence interval for the Binomial pa-
rameterpgiven thatkBinomial (n;p). In other words, it returns some number pfor whichppwith prob-
ability at least 1 
over the sampling of kBinomial (n;p). Following Lecuyer et al. (2019), we chose to
use the Clopper-Pearson conÔ¨Ådence interval, which inverts the Binomial CDF (Clopper & Pearson, 1934). Using
statsmodels.stats.proportion.proportion confint , this can be implemented as
proportion_confint(k, n, alpha=2 *alpha, method="beta")[0]
C.1. Prediction
The randomized algorithm given in pseudocode as PREDICT leverages the hypothesis test given in Hung & Fithian (2019)
for identifying the top category of a multinomial distribution. PREDICT has one tunable hyperparameter, 
. When
is small,
PREDICT abstains frequently but rarely returns the wrong class. When 
is large, PREDICT usually makes a prediction, but
may often return the wrong class.
We now prove that with high probability, P REDICT will either return g(x)or abstain.
Proposition 1 (restated). With probability at least 1 
over the randomness in PREDICT ,PREDICT will either abstain
or returng(x). (Equivalently: the probability that PREDICT returns a class other than g(x)is at most
.)
Proof. For notational convenience, deÔ¨Åne pc=P(f(x+") =c). LetcA= maxcpc. Notice that by deÔ¨Ånition, g(x) =cA.
We can describe the randomized procedure P REDICT as follows:
1. Sample a vector of class counts fncgc2Yfrom Multinomial (fpcgc2Y;n).
2.Let^cA= arg maxcncbe the class whose count is largest. Let nAandnBbe the largest count and the second-largest
count, respectively.
3.If the p-value of the two-sided hypothesis test that nAis drawn from Binom 
nA+nB;1
2
is less than
, then return
^cA. Else, abstain.
The quantities cAand thepc‚Äôs are Ô¨Åxed but unknown, while the quantities ^cA, thenc‚Äôs,nA, andnBare random.
We‚Äôd like to prove that the probability that PREDICT returns a class other than cAis at most
.PREDICT returns a class
other thancAif and only if (1) ^cA6=cAand (2) P REDICT does not abstain.
We have:
P(PREDICT returns class6=cA) =P(^cA6=cA;PREDICT does not abstain )
=P(^cA6=cA)P(PREDICT does not abstainj^cA6=cA)
P(PREDICT does not abstainj^cA6=cA)CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
Recall that PREDICT does not abstain if and only if the p-value of the two-sided hypothesis test that nAis drawn from
Binom (nA+nB;1
2)is less than
. Theorem 1 in Hung & Fithian (2019) proves that the conditional probability that this
event occurs given that ^cA6=cAis exactly
. That is,
P(PREDICT does not abstainj^cA6=cA) =

Therefore, we have:
P(PREDICT returns class6=cA)

C.2. CertiÔ¨Åcation
The certiÔ¨Åcation task is: given some input xand a randomized smoothing classiÔ¨Åer described by (f;), return both (1) the
predictiong(x)and (2) a radius Rin which this prediction is certiÔ¨Åed robust. This task requires identifying the class cA
with maximal weight in f(x+"), estimating a lower bound pAonpA:=P(f(x+") =cA)and estimating an upper bound
pBonpB:= maxc6=cAP(f(x+") =c)(Figure 1).
Suppose for simplicity that we already knew cAand needed to obtain pA. We could collect nsamples off(x+"), count
how many times f(x+") =cA, and use a Binomial conÔ¨Ådence interval to obtain a lower bound on pAthat holds with
probability at least 1 
over thensamples.
However, estimating pAandpBwhile simultaneously identifying the top class cAis a little bit tricky, statistically speaking.
We propose a simple two-step procedure. First, use n0samples from f(x+")to take a guess ^cAat the identity of the top
classcA. In practice we observed that f(x+")tends to put most of its weight on the top class, so n0can be set very small.
Second, use nsamples from f(x+")to obtain some pAandpBfor whichpApAandpBpBwith probability at least
1 
. We observed that it is much more typical for the mass of f(x+")not allocated to cAto be allocated entirely to one
runner-up class than to be allocated uniformly over all remaining classes. Therefore, the quantity 1 pAis a reasonably
tight upper bound on pB. Hence, we simply set pB= 1 pA, so our bound becomes
R=
2( 1(pA)  1(1 pA))
=
2( 1(pA) +  1(pA))
= 1(pA)
The full procedure is described in pseudocode as CERTIFY . IfpA<1
2, we abstain from making a certiÔ¨Åcation; this can
occur especially if ^cA6=g(x), i.e. if we misidentify the top class using the Ô¨Årst n0samples off(x+").
Proposition 2 (restated). With probability at least 1 
over the randomness in CERTIFY , ifCERTIFY returns a class ^cA
and a radius R(i.e. does not abstain), then we have the robustness guarantee
g(x+) = ^cAwheneverkk2<R
Proof. From the contract of LOWER CONFBOUND , we know that with probability at least 1 
over the sampling of
"1:::"n, we havepAP[f(x+") = ^cA]. Notice that CERTIFY returns a class and radius only if pA>1
2(otherwise it
abstains). If pAP[f(x+") = ^cA]and1
2<pA, then we can invoke Theorem 1 with pB= 1 pAto obtain the desired
guarantee.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
D. Estimating the certiÔ¨Åed test-set accuracy
In this appendix, we show how to convert the ‚Äúapproximate certiÔ¨Åed test accuracy‚Äù considered in the main paper into a
lower bound on the true certiÔ¨Åed test accuracy that holds with high probability over the randomness in C ERTIFY .
Consider a classiÔ¨Åer g, a test setS=f(x1;c1):::(xm;cm)g, and a radius r. For each example i2[m], letziindicate
whetherg‚Äôs prediction at xiis both correct and robust at radius r, i.e.
zi=1[g(xi+) =ci8kk2<r]
The certiÔ¨Åed test set accuracy of gat radiusris deÔ¨Åned as1
mPm
i=1zi. Ifgis a randomized smoothing classiÔ¨Åer, we cannot
compute this quantity exactly, but we can estimate a lower bound that holds with arbitrarily high probability over the
randomness in CERTIFY . In particular, suppose that we run CERTIFY with failure rate 
on each example xiin the test set.
Let the Bernoulli random variable Yidenote the event that on example i,CERTIFY returns the correct label cA=ciand a
certiÔ¨Åed radius Rwhich is greater than r. LetY=Pm
i=1Yi. In the main paper, we referred to Y=m as the ‚Äúapproximate
certiÔ¨Åed accuracy.‚Äù It is ‚Äúapproximate‚Äù because Yi= 1does not mean that zi= 1. Rather, from Proposition 2, we know
the following: if zi= 0, thenP(Yi= 1)
. We now show how to exploit this fact to construct a one-sided conÔ¨Ådence
interval for the unobserved quantity1
mPm
i=1ziusing the observed quantities Yandm.
Theorem 6. For any>0, with probability at least 1 over the randomness in CERTIFY ,
1
mmX
i=1zi1
1 
 
Y
m 
 r
2
(1 
) log(1=)
m log(1=)
3m!
(16)
Proof. Letmgood=Pm
i=1ziandmbad=Pm
i=1(1 zi)be the number of test examples on which zi= 1 orzi= 0,
respectively. We model YiBernoulli (pi), wherepiis in general unknown. Let Ygood=P
i:zi=1YiandYbad=P
i:zi=0Yi.
The quantity of interest, the certiÔ¨Åed accuracy1
mPm
i=1zi, is equal tomgood=m. However, we only observe Y=Ygood+Ybad.
Note that ifzi= 0, thenpi
, so we have E[Yi] =pi
and assuming 
1
2, we have Var[Yi] =pi(1 pi)
(1 
).
SinceYbadis a sum ofmbadindependent random variables each bounded between zero and one, with E[Ybad]
m badand
Var(Ybad)mbad
(1 
), Bernstein‚Äôs inequality (Blanchard, 2007) guarantees that with probability at least 1 over the
randomness in C ERTIFY ,
Ybad
m bad+p
2mbad
(1 
) log(1=) +log(1=)
3
From now on, we manipulate this inequality ‚Äî remember that it holds with probability at least 1 .
SinceY=Ygood+Ybad, may write
YgoodY 
m bad p
2mbad
(1 
) log(1=) log(1=)
3
SincemgoodYgood, we may write
mgoodY 
m bad p
2mbad
(1 
) log(1=) log(1=)
3
Sincemgood+mbad=m, we may write
mgood1
1 

Y 
m p
2mbad
(1 
) log(1=) log(1=)
3
Finally, in order to make this conÔ¨Ådence interval depend only on observables, we use mbadmto write
mgood1
1 

Y 
m p
2m
(1 
) log(1=) log(1=)
3
Dividing both sides of the inequality by mrecovers the theorem statement.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
E. ImageNet and CIFAR-10 Results
E.1. CertiÔ¨Åcation
Tables 2 and 3 show the approximate certiÔ¨Åed top-1 test set accuracy of randomized smoothing on ImageNet and CIFAR-10
with various noise levels . By ‚Äúapproximate certiÔ¨Åed accuracy,‚Äù we mean that we ran CERTIFY on a subsample of the
test set, and for each rwe report the fraction of examples on which CERTIFY (a) did not abstain, (b) returned the correct
class, and (c) returned a radius Rgreater than r. There is some probability (at most 
) that any example‚Äôs certiÔ¨Åcation is
inaccurate. We used 
= 0:001andn= 100000 . On CIFAR-10 our base classiÔ¨Åer was a 110-layer residual network and
we certiÔ¨Åed the full test set; on ImageNet our base classiÔ¨Åer was a ResNet-50 and we certiÔ¨Åed a subsample of 500 points.
Note that the certiÔ¨Åed accuracy at r= 0is just the standard accuracy of the smoothed classiÔ¨Åer. See Appendix J for more
experimental details.
r= 0:0r= 0:5r= 1:0r= 1:5r= 2:0r= 2:5r= 3:0
= 0:25 0.67 0.49 0.00 0.00 0.00 0.00 0.00
= 0:50 0.57 0.46 0.37 0.29 0.00 0.00 0.00
= 1:00 0.44 0.38 0.33 0.26 0.19 0.15 0.12
Table 2. Approximate certiÔ¨Åed test accuracy on ImageNet. Each row is a setting of the hyperparameter , each column is an `2radius.
The entry of the best for each radius is bolded. For comparison, random guessing would attain 0.001 accuracy.
r= 0:0r= 0:25r= 0:5r= 0:75r= 1:0r= 1:25r= 1:5
= 0:12 0.83 0.60 0.00 0.00 0.00 0.00 0.00
= 0:25 0.77 0.61 0.42 0.25 0.00 0.00 0.00
= 0:50 0.66 0.55 0.43 0.32 0.22 0.14 0.08
= 1:00 0.47 0.41 0.34 0.28 0.22 0.17 0.14
Table 3. Approximate certiÔ¨Åed test accuracy on CIFAR-10. Each row is a setting of the hyperparameter , each column is an `2radius.
The entry of the best for each radius is bolded. For comparison, random guessing would attain 0.1 accuracy.
E.2. Prediction
Table 4 shows the performance of PREDICT as the number of Monte Carlo samples nis varied between 100 and 10,000.
Suppose that for some test example (x;c),PREDICT returns the label ^cA. We say that this prediction was correct if^cA=c
and we say that this prediction was accurate if^cA=g(x). For example, a prediction could be correct but inaccurate if gis
wrong atx, yet PREDICT accidentally returns the correct class. Ideally, we‚Äôd like PREDICT to be both correct and accurate.
Withn=100 Monte Carlo samples and a failure rate of 
= 0:001,PREDICT is cheap to evaluate (0.15 seconds on our
hardware) yet it attains relatively high top-1 accuracy of 65% on the ImageNet test set, and only abstains 12% of the time.
When we use n=10,000 Monte Carlo samples, PREDICT takes longer to evaluate (15 seconds), yet only abstains 4% of the
time. Interestingly, we observe from Table 4 that most of the abstentions when n= 100 were for examples on which gwas
wrong, so in practice we would lose little accuracy by taking nto be as small as 100.
CORRECT ,ACCURATE CORRECT ,INACCURATE INCORRECT ,ACCURATE INCORRECT ,INACCURATE ABSTAIN
N
100 0.65 0.00 0.23 0.00 0.12
1000 0.68 0.00 0.28 0.00 0.04
10000 0.69 0.00 0.30 0.00 0.01
Table 4. Performance of PRECICT asnis varied. The dataset was ImageNet and = 0:25,
= 0:001. Each column shows the fraction
of test examples which ended up in one of Ô¨Åve categories; the prediction at xis ‚Äúcorrect‚Äù if PREDICT returned the true label, while the
prediction is ‚Äúaccurate‚Äù if PREDICT returnedg(x). Computing g(x)exactly is not possible, so in order to determine whether PREDICT
was accurate, we took the gold standard to be the top class over n=100,000 Monte Carlo samples.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
F. Training with Noise
As mentioned in section 3.3, in the experiments for this paper, we followed Lecuyer et al. (2019) and trained the base
classiÔ¨Åer by minimizing the cross-entropy loss with Gaussian data augmentation. We now provide some justiÔ¨Åcation for this
idea.
Letf(x1;c1);:::; (xn;cn)gbe a training dataset. We assume that the base classiÔ¨Åer takes the form f(x) =
arg maxc2Yfc(x), where each fcis the scoring function for class c.
Suppose that our goal is to maximize the sum of of the log-probabilities that fwill classify each xi+"asci:
nX
i=1logP"(f(xi+") =ci) =nX
i=1logE"1
arg max
cfc(xi+") =ci
(17)
Recall that the softmax function can be interpreted as a continuous, differentiable approximation to arg max :
1
arg max
cfc(xi+") =ci
exp(fci(xi+"))P
c2Yexp(fc(xi+"))
Therefore, our objective is approximately equal to:
nX
i=1logE"exp(fci(xi+"))P
c2Yexp(fc(xi+"))
(18)
By Jensen‚Äôs inequality and the concavity of log, this quantity is lower-bounded by:
nX
i=1E"
logexp(fci(xi+"))P
c2Yexp(fc(xi+"))
which is the negative of the cross-entropy loss under Gaussian data augmentation.
Therefore, minimizing the cross-entropy loss under Gaussian data augmentation will maximize (18), which will approxi-
mately maximize (17).CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
G. Noise Level can Scale with Input Resolution
Since our robustness guarantee (3)in Theorem 1 does not explicitly depend on the data dimension d, one might worry that
randomized smoothing is less effective for images in high resolution ‚Äî certifying a Ô¨Åxed `2radius is ‚Äúless impressive‚Äù for,
say,224224image than for a 5656image. However, it turns out that in high resolution, images can be corrupted
with larger levels of isotropic Gaussian noise while still preserving their content. This fact is made clear by Figure 12,
which shows an image at high and low resolution corrupted by Gaussian noise with the same variance.full The class
(‚Äúhummingbird‚Äù) is easy to discern from the high-resolution noisy image, but not from the low-resolution noisy image. As a
consequence, in high resolution one can take to be larger while still being able to obtain a base classiÔ¨Åer that classiÔ¨Åes
noisy images accurately. Since our Theorem 1 robustness guarantee scales linearly with , this means that in high resolution
one can certify larger radii.
Figure 12. Top: An ImageNet image from class ‚Äúhummingbird‚Äù in resolutions 56x56 (left) and 224x224 (right). Bottom : the same
images corrupted by isotropic Gaussian noise at = 0:5. On noiseless images the class is easy to distinguish no matter the resolution, but
on noisy data the class is much easier to distinguish when the resolution is high.
The argument above can be made rigorous, though we Ô¨Årst need to decide what it means for two images to be high- and
low-resolution versions of each other. Here we present one solution:
LetXdenote the space of ‚Äúhigh-resolution‚Äù images in dimension 2k2k3, and letX0denote the space of ‚Äúlow-resolution‚Äù
images in dimension kk3. Let AVGPOOL:X!X0be the function which takes as input an image xin dimension
2k2k3, averages together every 2x2 square of pixels, and outputs an image in dimension kk3.
Equipped with these deÔ¨Ånitions, we can say that (x;x0)2XX0are a high/low resolution image pair if x0=AVGPOOL(x).
Proposition 7. Given any smoothing classiÔ¨Åer g0:X0!Y , one can construct a smoothing classiÔ¨Åer g:X!Y with
the following property: for any x2X andx0=AVGPOOL(x),gpredicts the same class at xthatg0predicts atx0, but is
certiÔ¨Åably robust at twice the radius.
Proof. Given some smoothing classiÔ¨Åer g0= (f0;0)fromX0toY, deÔ¨Ånegto be the smoothing classiÔ¨Åer (f;)fromX
toYwith noise level = 20and base classiÔ¨Åer f(x) =f0(AVGPOOL(x)). Note that the average of four independent
copies ofN(0;(2)2)is distributed asN(0;2). Therefore, for any high/low-resolution image pair x0=AVGPOOL(x),
the random variable AVGPOOL(x+"), where"N(0;(2)2I2k2k3), is equal in distribution to the random variable
x0+"0, where"0N(0;2Ikk3). Hence,f(x+") =f0(AVGPOOL(x+"))has the same distribution as f0(x0+"0).
By the deÔ¨Ånition of g, this means that g(x) =g0(x0), Additionally, by Theorem 1, since = 20, this means that g‚Äôs
prediction at xis certiÔ¨Åably robust at twice the radius as g0‚Äôs prediction at x0.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
H. Additional Experiments
H.1. Comparisons to baselines
Figure 13 compares the certiÔ¨Åed accuracy of a smoothed 20-layer resnet to that of the released models from two recent works
on certiÔ¨Åed`2robustness: the Lipschitz approach from Tsuzuku et al. (2018) and the approach from Zhang et al. (2018).
Note that in these experiments, the base classiÔ¨Åer for smoothing was larger than the networks of competing approaches. The
comparison to Zhang et al. (2018) is on CIFAR-10, while the comparison to Tsuzuku et al. (2018) is on SVHN. Note that
for each comparison, we preprocessed the dataset to follow the preprocessing used when the baseline was trained; therefore,
the radii reported for CIFAR-10 here are not comparable to the radii reported elsewhere in this paper. Full experimental
details are in Appendix J.
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
radius0.00.20.40.60.81.0certified accuracyours
(Tsuzuku et al)
(a) Tsuzuku et al. (2018)
0.0 0.1 0.2 0.3 0.4 0.5
radius0.00.20.40.60.81.0certified accuracyours
(Zhang et al) (b) Zhang et al. (2018)
Figure 13. Randomized smoothing with a 20-layer resnet base classiÔ¨Åer attains higher certiÔ¨Åed accuracy than the released models from
two recent works on certiÔ¨Åed `2robustness.
H.2. High-probability guarantees
Appendix D details how to use CERTIFY to obtain a lower bound on the certiÔ¨Åed test accuracy at radius rof a randomized
smoothing classiÔ¨Åer that holds with high probability over the randomness in CERTIFY . In the main paper, we declined to do
this and simply reported the approximate certiÔ¨Åed test accuracy, deÔ¨Åned as the fraction of test examples for which CERTIFY
gives the correct prediction and certiÔ¨Åes it at radius r. Of course, with some probability (guaranteed to be less than 
), each
of these certiÔ¨Åcations is wrong.
However, we now demonstrate empirically that there is a negligible difference between a proper high-probability lower
bound on the certiÔ¨Åed accuracy and the approximate version that we reported in the paper. We created a randomized
smoothing classiÔ¨Åer gon ImageNet with a ResNet-50 base classiÔ¨Åer and noise level = 0:25. We used CERTIFY with

= 0:001to certify a subsample of 500 examples from the ImageNet test set. From this we computed the approximate
certiÔ¨Åed test accuracy at each radius r. Then we used the correction from Appendix D with = 0:001to obtain a lower
bound on the certiÔ¨Åed test accuracy at rthat holds pointwise with probability at least 1 over the randomness in CERTIFY .
Figure 14 plots both quantities as a function of r. Observe that the difference is so negligible that the lines almost overlap.
H.3. How much noise to use when training the base classiÔ¨Åer?
In the main paper, whenever we created a randomized smoothing classiÔ¨Åer gat noise level , we always trained the
corresponding base classiÔ¨Åer fwith Gaussian data augmentation at noise level . In Figure 15, we show the effects of
training the base classiÔ¨Åer with a different level of Gaussian noise. Observe that ghas a lower certiÔ¨Åed accuracy if fwas
trained using a different noise level. It seems to be worse to train with noise < than to train with noise >.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
radius0.00.20.40.60.81.0certified accuracyApproximate
High-Prob
Figure 14. The difference between the approximate certiÔ¨Åed accuracy, and a high-probability lower bound on the certiÔ¨Åed accuracy, is
negligible.
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
radius0.00.20.40.60.81.0certified accuracytrain =0.25
train =0.50
train =1.00
(a) CIFAR-10
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
radius0.00.20.40.60.81.0certified accuracytrain =0.25
train =0.50
train =1.00
 (b) ImageNet
Figure 15. Vary training noise while holding prediction noise Ô¨Åxed at = 0:50.
I. Derivation of Prior Randomized Smoothing Guarantees
In this appendix, we derive the randomized smoothing guarantees of Lecuyer et al. (2019) and Li et al. (2018) using the
notation of our paper. Both guarantees take same general form as ours, except with a different expression for R:
Theorem (generic guarantee): Letf:Rd!Y be any deterministic or random function, and let "N(0;2I). Letg
be deÔ¨Åned as in (1). Suppose cA2Y andpA;pB2[0;1]satisfy:
P(f(x+") =cA)pApBmax
c6=cAP(f(x+") =c) (19)
Theng(x+) =cAfor allkk2<R.
For convenience, deÔ¨Åne the notation XN(x;2I)andYN(x+;2I).
I.1. Lecuyer et al. (2019)
Lecuyer et al. (2019) proved a version of the generic robustness guarantee in which
R= sup
0<min
1;1
2logpA
pBr
2 log
1:25(1+exp())
pA exp(2)pB
Proof. In order to avoid notation that conÔ¨Çicts with the rest of this paper, we use and
where Lecuyer et al. (2019) used 
and.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
Suppose that we have some 0<1and
 >0such that
2=kk2
22 log1:25

(20)
The ‚ÄúGaussian mechanism‚Äù from differential privacy guarantees that:
P(f(X) =cA)exp()P(f(Y) =cA) +
 (21)
and, symmetrically,
P(f(Y) =cB)exp()P(f(X) =cB) +
 (22)
See Lecuyer et al. (2019), Lemma 2 for how to obtain this form from the standard form of the (;
)DP deÔ¨Ånition.
Fix a perturbation . To guarantee that g(x+) =cA, we need to show that P(f(Y) =cA)>P(f(Y) =cB)for each
cB6=cA.
Together, (21) and (22) imply that to guarantee P(f(Y) =cA)>P(f(Y) =cB)for anycB, it sufÔ¨Åces to show that:
P(f(X) =cA)>exp(2)P(f(X) =cB) +
(1 + exp()) (23)
Therefore, in order to guarantee that P(f(Y) =cA)>P(f(Y) =cB)for eachcB6=cA, by (19) it sufÔ¨Åces to show:
pA>exp(2)pB+
(1 + exp()) (24)
Now, inverting (20), we obtain:

= 1:25 exp
 22
2kk2
(25)
Plugging (25) into (24), we see that to guarantee P(f(Y) =cA)P(f(Y) =cB)it sufÔ¨Åces to show that:
pA>exp(2)pB+ 1:25 exp
 22
2kk2
(1 + exp()) (26)
which rearranges to:
pA exp(2)pB
1:25(1 + exp( ))>exp
 22
2kk2
(27)
Since the RHS is always positive, and the denominator on the LHS is always positive, this condition can only possibly hold
if the numerator on the LHS is positive. Therefore, we need to restrict to
0<min
1;1
2logpA
pB
(28)
The condition (27) is equivalent to:
kk2log1:25(1 + exp( ))
pA exp(2)pB<22
2(29)
SincepA1andpB0, the denominator in the LHS is 1which is in turnthe numerator on the LHS. Therefore, the
term inside the log in the LHS is greater than 1, so the log term on the LHS is greater than zero. Therefore, we may divide
both sides of the inequality by the log term on the LHS to obtain:
kk2<22
2 log
1:25(1+exp())
pA exp(2)pB (30)
Finally, we take the square root and maximize the bound over all valid (28) to yield:
kk< sup
0<min
1;1
2logpA
pBr
2 log
1:25(1+exp())
pA exp(2)pB(31)CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
Figure 16a plots this bound at varying settings of the tuning parameter , while Figure 16c plots how the bound varies with
for a Ô¨ÅxedpAandpB.
I.2. Li et al. (2018)
Li et al. (2018) proved a version of the generic robustness guarantee in which
R= sup

>0s
 2

log
1 pA pB+ 21
2(pA1 
+pB1 
)1 

Proof. A generalization of KL divergence, the 
-Renyi divergence is an information theoretic measure of distance between
two distributions. It is parameterized by some 
>0. The
-Renyi divergence between two discrete distributions PandQ
is deÔ¨Åned as:
D
(PjjQ) :=1

 1log kX
i=1p

i
q
 1
i!
(32)
In the continuous case, this sum is replaced with an integral. The divergence is undeÔ¨Åned when 
= 1since a division by
zero occurs, but the limit of D
(PjjQ)as
!1is the KL divergence between PandQ.
Li et al. (2018) prove that if Pis a discrete distribution for which the highest probability class has probability pAand all
other classes have probability pB, then for any other discrete distribution Qfor which
D
(PjjQ)< log
1 pA pB+ 21
2(pA1 
+pB1 
)1 

(33)
the highest-probability class in Qis guaranteed to be the same as the highest-probability class in P.
We now apply this result to the discrete distributions P=f(X)andQ=f(Y). IfD
(f(X)jjf(Y))satisÔ¨Åes (33), then it
is guaranteed that g(x) =g(x+).
The data processing inequality states that applying a function to two random variables can only decrease the 
-Renyi
divergence between them. In particular,
D
(f(X)jjf(Y))D
(XjjY) (34)
There is a closed-form expression for the 
-Renyi divergence between two Gaussians:
D
(XjjY) =
kk2
22(35)
Therefore, we can guarantee that g(x+) =cAso long as

kk2
22< log
1 pA pB+ 21
2(pA1 
+pB1 
)1 

(36)
which simpliÔ¨Åes to
kk<s
 2

log
1 pA pB+ 21
2(pA1 
+pB1 
)1 

(37)
Finally, since this result holds for any 
>0, we may maximize over 
to obtain the largest possible certiÔ¨Åed radius:
kk<sup

>0s
 2

log
1 pA pB+ 21
2(pA1 
+pB1 
)1 

(38)
Figure 16b plots this bound at varying settings of the tuning parameter 
, while Ô¨Ågure 16d plots how the bound varies with

for a Ô¨ÅxedpAandpB.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
(a) The Lecuyer et al. (2019) bound over several settings of . The
brown line is the pointwise supremum over all eligible , computed
numerically.
(b) The Li et al. (2018) bound over several settings of 
. The
purple line is the pointwise supremum over all eligible 
, computed
numerically.
(c) Tuning the Lecuyer et al. (2019) bound wrt whenpA=
0:8;pB= 0:2
(d) Tuning the Li et al. (2018) bound wrt 
whenpA= 0:999;pB=
0:0001
J. Experiment Details
J.1. Comparison to baselines
We compared randomized smoothing against three recent approaches for `2-robust classiÔ¨Åcation (Tsuzuku et al., 2018;
Wong et al., 2018; Zhang et al., 2018). Tsuzuku et al. (2018) and Wong et al. (2018) propose both a robust training method
and a complementary certiÔ¨Åcation mechanism, while Zhang et al. (2018) propose a method to certify generically trained
networks. In all cases we compared against networks provided by the authors. We compared against Wong et al. (2018) and
Zhang et al. (2018) on CIFAR-10, and we compared against Tsuzuku et al. (2018) on SVHN.
In image classiÔ¨Åcation it is common practice to preprocess a dataset by subtracting from each channel the mean over the
dataset, and dividing each channel by the standard deviation over the dataset. However, we wanted to report certiÔ¨Åed radii
in the original image coordinates rather than in the standardized coordinates. Therefore, throughout most of this work we
Ô¨Årstadded the Gaussian noise, and then standardized the channels, before feeding the image to the base classiÔ¨Åer. (In the
practical PyTorch implementation, the Ô¨Årst layer of the base classiÔ¨Åer was a layer that standardized the input.) However, all
of the baselines we compared against provided pre-trained networks which assumed that the dataset was Ô¨Årst preprocessed
in a speciÔ¨Åc way. Therefore, when comparing against the baselines we also preprocessed the datasets Ô¨Årst, so that we could
report certiÔ¨Åed radii that were directly comparable to the radii reported by the baseline methods.
Comparison to Wong et al. (2018) Following Wong et al. (2018), the CIFAR-10 dataset was preprocessed by subtracting
(0:485;0:456;0:406) and dividing by (0:225;0:225;0:225) .
While the body of the Wong et al. (2018) paper focuses on `1certiÔ¨Åed robustness, their algorithm naturally extends to
`2certiÔ¨Åed robustness, as developed in the appendix of the paper. We used three `2-trained residual networks publicly
released by the authors, each trained with a different setting of their hyperparameter 2f0:157;0:628;2:51g. We used code
publicly released by the authors at https://github.com/locuslab/convex_adversarial/blob/master/CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
examples/cifar_evaluate.py to compute the robustness radius of test images. The code accepts a radius and
returns TRUE (robust) or FALSE (not robust); we incorporated this subroutine into a binary search procedure to Ô¨Ånd the
largest radius for which the code returned TRUE.
For randomized smoothing we used = 0:6and a 20-layer residual network base classiÔ¨Åer. We ran CERTIFY withn0= 100 ,
n=100,000 and 
= 0:001.
For both methods, we certiÔ¨Åed the full CIFAR-10 test set.
Comparison to Tsuzuku et al. (2018) Following Tsuzuku et al. (2018), the SVHN dataset was not preprocessed except
that pixels were divided by 255 so as to lie within [0, 1].
We compared against a pretrained network provided to us by the authors in which the hyperparameter of their method was
set toc= 0:1. The network was a wide residual network with 16 layers and a width factor of 4. We used the authors‚Äô code
athttps://github.com/ytsmiling/lmt to compute the robustness radius of test images.
For randomized smoothing we used = 0:1and a 20-layer residual network base classiÔ¨Åer. We ran CERTIFY withn0= 100 ,
n=100,000 and 
= 0:001.
For both methods, we certiÔ¨Åed the whole SVHN test set.
Comparison to Zhang et al. (2018) Following Zhang et al. (2018), the CIFAR-10 dataset was preprocessed by subtracting
0.5 from each pixel.
We compared against the cifar 71024 vanilla network released by the authors, which is a 7-layer MLP. We used the
authors‚Äô code at https://github.com/IBM/CROWN-Robustness-Certification to compute the robustness
radius of test images.
For randomized smoothing we used = 1:2and a 20-layer residual network base classiÔ¨Åer. We ran CERTIFY withn0= 100 ,
n=100,000 and 
= 0:001.
For randomized smoothing, we certiÔ¨Åed the whole CIFAR-10 test set. For Zhang et al. (2018), we certiÔ¨Åed every fourth
image in the CIFAR-10 test set.
J.2. ImageNet and CIFAR-10 Experiments
Our code is available at http://github.com/locuslab/smoothing .
In order to report certiÔ¨Åed radii in the original coordinates, we Ô¨Årstadded Gaussian noise, and then standardized the data.
SpeciÔ¨Åcally, in our PyTorch implementation, the Ô¨Årst layer of the base classiÔ¨Åer was a normalization layer that performed
a channel-wise standardization of its input. For CIFAR-10 we subtracted the dataset mean (0:4914;0:4822;0:4465)
and divided by the dataset standard deviation (0:2023;0:1994;0:2010) . For ImageNet we subtracted the dataset mean
(0:485;0:456;0:406) and divided by the standard deviation (0:229;0:224;0:225) .
For both ImageNet and CIFAR-10, we trained the base classiÔ¨Åer with random horizontal Ô¨Çips and random crops (in addition
to the Gaussian data augmentation discussed explicitly in the paper). On ImageNet we trained with synchronous SGD on
four NVIDIA RTX 2080 Ti GPUs; training took approximately three days.
On ImageNet our base classiÔ¨Åer used the ResNet-50 architecture provided in torchvision . On CIFAR-10 we used a
110-layer residual network from https://github.com/bearpaw/pytorch-classification .
On ImageNet we certiÔ¨Åed every 100-th image in the validation set, for 500 images total. On CIFAR-10 we certiÔ¨Åed the
whole test set.
In Figure 8 ( middle ) we Ô¨Åxed= 0:25and
= 0:001while varying the number of samples n. We did not actually vary
the number of samples nthat we simulated: we kept this number Ô¨Åxed at 100,000 but varied the number that we fed the
Clopper-Pearson conÔ¨Ådence interval.
In Figure 8 ( right ), we Ô¨Åxed= 0:25andn=100,000 while varying 
.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
J.3. Adversarial Attacks
As discussed in Section 4, we subjected smoothed classiÔ¨Åers to a projected gradient descent-style adversarial attack. We
now describe the details of this attack.
Letfbe the base classiÔ¨Åer and let be the noise level. Following Li et al. (2018), given an example (x;c)2RdY and a
radiusr, we used a projected gradient descent style adversarial attack to optimize the objective:
arg max
:kk2<rE"N(0;2I)[`(f(x++");c)] (39)
where`is the softmax loss function. (Breaking notation with the rest of the paper in which freturns a class, the function f
here refers to the function that maps an image in Rdto a vector of classwise scores.)
At each iteration of the attack, we drew ksamples of noise, "1:::"kN(0;2I), and followed the stochastic gradient
gt=Pk
i=1rt`(f(x+t+"k);c).
As is typical (Kolter & Madry, 2018), we used a ‚Äústeepest ascent‚Äù update rule, which, for the `2norm, means that we
normalized the gradient before applying the update. The overall PGD update is: t+1=projr
t+gt
kgtk
where the
function projrthat projects its input onto the ball fz:kzk2rgis given by projr(z) =rz
max(r;kzk2). We used a constant
step sizeand a Ô¨Åxed number Tof PGD iterations.
In practice, our step size was = 0:1, we usedT= 20 steps of PGD, and we computed the stochastic gradient using
k= 1000 Monte Carlo samples.
Unfortunately, the objective we optimize (39) is not actually the attack objective of interest. To force a misclassiÔ¨Åcation, an
attacker needs to Ô¨Ånd some perturbation withkk2<rand some class cBfor which
P"N(0;2I)(f(x++") =cB)P"N(0;2I)(f(x++") =c)
Effective adversarial attacks against randomized smoothing are outside the scope of this paper.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
K. Examples of Noisy Images
We now show examples of CIFAR-10 and ImageNet images corrupted with varying levels of noise.
= 0:00
 = 0:25
 = 0:50
 = 1:00
Figure 17. CIFAR-10 images additively corrupted by varying levels of Gaussian noise N(0;2I). Pixel values greater than 1.0 (=255) or
less than 0.0 (=0) were clipped to 1.0 or 0.0.CertiÔ¨Åed Adversarial Robustness via Randomized Smoothing
= 0:00
 = 0:25
 = 0:50
 = 1:00
Figure 18. ImageNet images additively corrupted by varying levels of Gaussian noise N(0;2I). Pixel values greater than 1.0 (=255) or
less than 0.0 (=0) were clipped to 1.0 or 0.0.